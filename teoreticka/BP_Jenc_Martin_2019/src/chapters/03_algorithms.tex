\chapter{Algorithms}

\newcommand{\paramhat}[1]{\boldsymbol{\hat{\textbf{#1}}}}
\newcommand{\param}[1]{\textbf{\text{#1}}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
% vector x = x1,x2,...,xn
\newcommand{\showVec}[3]{\vec{#1} = (\lowercase{#2}_1, \lowercase{#2}_2,\ldots,\lowercase{#2}_#3)}
% matrix notation
\newcommand{\m}[1]{\boldsymbol{\uppercase{#1}}}
% \cite{rouss:2000}

% \usepackage{amssymb}
\let\vec\boldsymbol


% \textbf{X} - bold
% \pmb{X} - strong bold
% \boldmath{X} - quite normal
%  $\textbf{X}^T$ - TRANSPOSED MATRIX

% change arrowed vector to the bold vector
% mathbf give bold 
% bold-symbol gives bold cursive
% **********************************************************************************
% ************************* FAST - LTS *********************************************
% **********************************************************************************
\section{FAST-LTS}
In this section we will introduce FAST-LTS algorithm\cite{rouss:2000}. 
It is, as well as in other cases, iterative algorithm. We will discuss all main components
of the algorithm starting with its core idea called concentration step which 
authors simply call C-step.

% $\showVec{\hat{w}}{w}{n}$
% $\boldsymbol{Y}$ - bold cursive

\subsection{C-step}
We will show that from existing LTS estimate $\boldsymbol{\hat{w}_{old}}$ we 
can construct new LTS estimate $\boldsymbol{\hat{w}_{new}}$ which objective 
function is less or equal to the old one. Based on this property we will be able 
to create sequence of LTS estimates which will lead to better results.

% \usepackage{etoolbox}

\newcommand{\what}[1]{
	\vec{\hat{w}_#1}
}
\newcommand{\whatn}{
	\vec{\hat{w}}
}


%******************************** C-STEP theorem  ***************************************************%

\begin{theorem}
Consider dataset consisting of
$\vec{x_1}, \vec{x_2} \ldots,\vec{x_n}$ explanatory variables where 
$\vec{x_i}\in\mathbb{R}^p\,, \forall \vec{x_i} = (x^i_1, x^i_2,\ldots,x^i_p)$ where $x^i_1 = 1$
and its corresponding $y_1, y_2,\ldots,y_n$ response variables. 
Let's also have $\vec{\hat{w}_0}\in\mathbb{R}^p$ any p-dimensional vector and 
$H_0 = \{{h_i ; h_i \in\mathbb{Z}\,, 1 \leq h_i \leq n\}}\,, |H_0| = h$. 
Let's now mark $RSS(\what{0}) = \sum_{i\in H_0} (r_0(i))^2$ where 
$r_0(i) = y_i - (w_1^0x^i_1 + w_2^0x^i_2 +\ldots+ w_p^0x^i_p$).
%Let's now mark  $H_1 = \{{h_i ;  1 \leq h_i \leq n\   \}}$ 
%uch that 
Let's take $\hat{n} = \{{1,2,\ldots,n\}}$ and mark
$\pi: \hat{n} \rightarrow \hat{n}$ permutation of $\hat{n}$ such that $|r_0({\pi(1)})| \leq |r_0({\pi(2)})| \leq \ldots \leq |r_0({\pi(n)})|$
and mark $H_1 = \{{\pi(1)\,, \pi(2)\,,... \pi(h)\}}$ set of $h$ indexes corresponding to $h$ smallest absolute residuals $r_0(i)$.
Finally take $\vec{\hat{w}^{OLS(H_1)}_1 }$ ordinary least squares fit on $H_1$ subset of observations
and its corresponding $RSS(\what{1}) = \sum_{i\in H_1} (r_1(i))^2$ sum of least squares. Then
\[ 
	RSS(\what{1}) \leq RSS(\what{0}) \numberthis
\]
\end{theorem}

\begin{proof}
	Because we take $h$ observations with smallest absolute residuals $r_0$, then for sure $\sum_{i\in H_1} (r_0(i))^2 \leq \sum_{i\in H_0} (r_0(i))^2 =  RSS(\what{0})$.
	When we take into account that Ordinary least squares fit $OLS_{H_1}$ minimize objective function of 
	$H_1$ subset of observations, then for sure  $RSS(\what{1}) =  \sum_{i\in H_1} (r_1(i))^2 \leq \sum_{i\in H_1} (r_0(i))^2$.
	Together we get $$RSS(\what{1})=\sum_{i\in H_1}(r_1(i))^2\leq\sum_{i\in H_1}(r_0(i))^2\leq\sum_{i\in H_0}(r_0(i))^2=RSS(\what{0})$$
\end{proof}

%******************************** C-STEP algorithm **********************************************%

\begin{corollary} 
	Based on previous theorem, using some $\vec{\hat{w}^{OLS(H_{old})}}$  on $H_{old}$ subset of observations we can
	construct $H_new$ subset with corresponding $\vec{\hat{w}^{OLS(H_{new})}}$ such that $RSS(\vec{\hat{w}^{OLS(H_{new})}}) \leq RSS(\vec{\hat{w}^{OLS(H_{old})}})$. 
	With this we can apply above theorem again on $\vec{\hat{w}^{OLS(H_{new})}}$ with $H_{new}$. This will lead to the iterative sequence of
	$RSS(\what{{old}}) \leq RSS(\what{{new}}) \leq \ldots$. One step of this process is described by following pseudocode. Note that for C-step we actually need only $\vec{\hat{w}}$ 
	 without need of passing  $H$.
\end{corollary}

\begin{algorithm}[H]
	\label{alg:Cstep}
    % \SetKwInOut{Input}{input}
    % \SetKwInOut{Output}{output}
    \KwIn{dataset consiting of $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ and $\boldsymbol{y} \in \mathbb{R}^{n \times 1}$,  $\what{{old}} \in \mathbb{R}^{p \times 1}$}
    \KwOut{ $\what{{new}}$, $H_{new}$ }
	\caption{C-step}
	
	$R \gets \emptyset$\;
	\For{$i \gets 1$ \textbf{to} $n$}{  
		$R \gets R \cup \{ |y_i - \what{{old}} \vec{x_i}^T |\}$\;
	}
	$H_{new} \gets $ select set of $h$ smallest absolute residuals from $R$\;
	$\what{{new}} \gets OLS(H_{new})$\;
	\Return{ $\what{{new}}$\,, $H_{new}$ }\;
\end{algorithm}

%******************************** C-STEP alg. time complexity ******************************************%
\begin{observation} 
	Time complexity of algorithm C-step \ref{alg:Cstep} is the same as time complexity as OLS. Thus $O(p^2n)$
	$\boldsymbol{{TODO}}$
\end{observation} 

\begin{lemma}
	Time complexity of OLS  on $\m{X}^{n \times p}$ and $\m{Y}^{n \times 1}$ is $O(p^2n)$.
\end{lemma}

\begin{proof}
	Normal equation of OLS is $\vec{\hat{w}} = (\m{X^T}\m{X})^{-1}\m{X^T}\m{Y}$.
	Time complexity  of matrix multiplication $\m{A}^{m \times n}$ and  $\m{B}^{n \times p}$ is $\sim \mathcal{O}(mnp)$.
	Time complexity of matrix $\m{C}^{m \times m}$ is $\sim \mathcal{O}(m^3)$
	So we need to compute 
	$\m{A} = \m{X^T}\m{X} \sim \mathcal{O}(p^2n)$ and
	$\m{B} = \m{X^T}\m{Y} \sim \mathcal{O}(pn)$ and
	$\m{C} = \m{A}^{-1} \sim \mathcal{O}(p^3)$ and finally 
	$\m{C}\m{B} \sim \mathcal{O}(p^2)$. 
	That gives us $\mathcal{O}(p^2n + pn + p^3 + p^2)$. Because  $\mathcal{O}(p^2n)$ and 
	$\mathcal{O}(p^3)$ asymptotically dominates over $\mathcal{O}(p^2)$ and $\mathcal{O}(pn)$ we can
	write $\mathcal{O}(p^2n + p^3)$.

	$\boldsymbol{{TODO}}$ CO zo toho je vic? Neni casove narocnejsi vynasobeni $\m{X^T}\m{X}$ nez inverze, kdyz bereme v uvahu $n >> p$ ???
\end{proof}

\begin{proof}
	In C-step we must compute $n$ absolute residuals. Computation of one absolute residual consists of
	matrix multiplication of shapes $1 \times p$ and $p \times 1$ that gives us $\mathcal{O}(p)$. Rest is in constant time.
	So time of computation $n$ residuals is $\mathcal{O}(np)$.
	Next we must select set of $h$ smallest residuals which can be done in $\mathcal{O}(n)$ using modification 
	of algorithm QuickSelect. reference: $\boldsymbol{{TODO}}$
	Finally we must compute $\hat{w}$ OLS estimate on $h$ subset of data.
	Because $h$ is linearly dependent on $n$, we can say that it is $\mathcal{O}(p^2n + p^3)$ which 
	is asymptotically dominant against previous steps which are $\mathcal{O}(np + n)$.
\end{proof}

As we stated above, repeating algorithm C-step will lead to sequence of $\what{1}, \what{2} \ldots$ 
on subsets $H_1, H_2 \ldots$ with corresponding residual sum of squares
$RSS(\what{{1}}) \geq RSS(\what{{2}}) \geq \ldots$. One could ask if this sequence will converge, so that
$RSS(\what{{i}}) == RSS(\what{{i+1}})$. 
Answer to this question will be presented by the following theorem.


%******************************** C-STEP alg. will converge ********************************************%
\begin{theorem}
	Sequence of C-step will converge to $\what{{m}}$ after maximum of $m = {n \choose h}$
	so that $RSS(\what{{m}}) == RSS(\what{{n}})\,, \forall n\geq m$ where $n$ is number of data samples 
	and $h$ is size of subset $H_i$.
\end{theorem}

\begin{proof}
	Since  $RSS(\what{{i}})$ is non-negative and $RSS(\what{{i}}) \leq RSS(\what{{i+i}})$ the 
	sequence will converge. $\what{{i}}$  is computed out of subset 
	$H_i \subset \{{1,2,\ldots,n\}}$. When there is finite number of subsets of size $h$ out of $n$ samples, namely ${n \choose h}$, the sequence will converge at the latest after this number of steps.
\end{proof}

%******************************** ITERATE-C-STEP algorithm **********************************************%
Above theorem gives us clue to create algorithm described by following pseudocode.



\begin{algorithm}[H]
	\label{alg:RepeatCstep}
	\KwIn{dataset consiting of $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ 
	and $\boldsymbol{y} \in \mathbb{R}^{n \times 1}$,  $\what{{old}} \in \mathbb{R}^{p \times 1}\,, H_0 $}
    \KwOut{ $\what{{final}}$, $H_{final}$ }
	\caption{Repeat-C-step}
	\SetKw{Break}{break}
	$\what{{new}} \gets \emptyset$\;
	$H_{new} \gets \emptyset$\;
	$RSS_{new} \gets \infty $\;

	\While{$True$}{
		$RSS_{old} \gets RSS(\what{{old}})$\;
		$\what{{new}}$\,, $H_{new} \gets \boldsymbol{X}\,, \boldsymbol{y}\,, \what{{old}}$\;
		$RSS_{new} \gets RSS(\what{{new}})$\;
		\If{$RSS_{old} == RSS_{new}$}{
			\Break
		  }
		$\what{{old}} \gets \what{{new}}$
	}

	\Return{ $\what{{new}}$, $H_{new}$ }\;
\end{algorithm}

It is important to note, that although maximum number of steps of this algorithm is ${n \choose h}$ in practice it is very low, most often under $20$ steps. $\boldsymbol{TODO}$ nejaky hezky grafik ktery to ukazuje....
That is not enough for the algorithm $Repeat-C-step$ to converge to global minimum, but it is necessary condition. That gives us an idea how to create the final algorithm. \cite{rouss:2000}

Choose a lot of initial subsets $H_1$ and on each of them apply algorithm Repeat-C-step. From all converged subsets with corresponding $\hat{w}$ estimates choose that which has lowest $RSS(\hat{w})$. 


Before we can construct final algorithm we must decide how to choose initial subset $H_1$ and how many of them mean ``\emph{a lot of}''. First let's focus on how to choose initial subset $H_1$.

\renewcommand{\O}[1]{$\mathcal{O}(#1)$}


\subsection{Choosing initial $H_1$subset}

We have couple of different options.

\begin{description}
	\item[Random] 
	\item[Not random] 
\end{description}



%********************************************************************************************************%
%******************************************** END *******************************************************%
%********************************************************************************************************%

\begin{algorithm}
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	\KwIn{A finite set $A=\{a_1, a_2, \ldots, a_n\}$ of integers}
	\KwOut{The largest element in the set}
	$max \gets a_1$\;
	\For{$i \gets 2$ \textbf{to} $n$} {
	  \If{$a_i > max$} {
		$max \gets a_i$\;
	  }
	}
	\Return{$max$}\;
	\label{algo:max}
	\end{algorithm}
	
	Algorithm~\ref{algo:change} is a greedy change-making algorithm (Slide 19 in Class Slides).
	
	\begin{algorithm}
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead 
	\KwIn{A set $C = \{c_1, c_2, \ldots, c_r\}$ of denominations of coins, where $c_i > c_2 > \ldots > c_r$ and a positive number $n$}
	\KwOut{A list of coins $d_1,d_2,\ldots,d_k$, such that $\sum_{i=1}^k d_i = n$ and $k$ is minimized}
	$C \gets \emptyset$\;
	\For{$i \gets 1$ \textbf{to} $r$}{
	  \While{$n \geq c_i$} {
		$C \gets C \cup \{c_i\}$\;
		$n \gets n - c_i$\;
	  }
	}
	\Return{$C$}\;
	\label{algo:change}
	\end{algorithm}
	
	Algorithm~\ref{algo:duplicate} and Algorithm~\ref{algo:duplicate2} will
	find the first duplicate element in a sequence of integers.
	
	\begin{algorithm}
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	\KwIn{A sequence of integers $\langle a_1, a_2, \ldots, a_n \rangle$}
	\KwOut{The index of first location witht he same value as in a previous location in the sequence}
	$location \gets 0$\;
	$i \gets 2$\;
	\While{$i \leq n$ \textbf{and} $location = 0$}{
	  $j \gets 1$\;
	  \While{$j < i$ \textbf{and} $location = 0$}{
		% The "u" before the "If" makes it so there is no "end" after the statement, so the else will then follow
		\uIf{$a_i = a_j$}{
		  $location \gets i$\;
		}
		\Else{
		  $j \gets j + 1$\;
		}
	  }
	  $i \gets i + 1$\;
	}
	\Return{location}\;
	\label{algo:duplicate}
	\end{algorithm}
	
	\begin{algorithm}
	\DontPrintSemicolon
	\KwIn{A sequence of integers $\langle a_1, a_2, \ldots, a_n \rangle$}
	\KwOut{The index of first location witht he same value as in a previous location in the sequence}
	$location \gets 0$\;
	$i \gets 2$\;
	\While{$i \leq n \land location = 0$}{
	  $j \gets 1$\;
	  \While{$j < i \land location = 0$}{
		% The "l" before the If makes it so it does not expand to a second line
		\lIf{$a_i = a_j$}{
		  $location \gets i$\;
		}
		\lElse{
		  $j \gets j + 1$\;
		}
	  }
	  $i \gets i + 1$\;
	}
	\Return{location}\;
	\label{algo:duplicate2}
	\end{algorithm}
% algorithm - float wrapper == similar to table or figure (stay on one page)
% type settings environments
% algorithmic, algorithmic-x, algorithm2e
% alg-pseudocode - layout for algorithmic-x
% algorithmic-x + alg-pseudocode  >> algorithmic
% algorithmic-x + alg-pseudocode similar to algorithm2e
% but algorithm2e  clearer syntax ! and suitable for latex2e

%$   F(x):=\frac{a_0}{2} + \sum_{k=1}^\infty a_k \cos\frac{k\pi x}{T} + b_k \sin\frac{k\pi x}{T}\,, \quad \forall x\in\mathbb{R}\,,$
% \begin{veta}\label{vet:Fourier_exp}

% \begin{enumerate}[(i)]
% 	\item 
% \end{enumerate}


	
   
In this section we will describe FAST-LTS algorithm and its main properties. The main idea of this algorithm is based on the fact that from one approximation of the algorithm we can compute another which can have lower objective function.
 TA DAAAAAAAAAAAAAAAAA
Theorem 1: \cite{rybicka}
Let w0 ... wp be the LTS estimate.
for each data sample we can compute |y-wx|


% \begin{description}
% 	\item[BP] 
% 	\item[DP] 
% \end{description}


% **********************************************************************************
% ************************* EXACT POLYNOMIAL ALGORITHM *****************************
% **********************************************************************************
\section{Exact algorithm}
\section{Feasible solution}
\section{MMEA}
\section{Branch and bound}
\section{Adding row}
