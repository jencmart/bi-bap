\select@language {english}
\select@language {english}
\addvspace {10pt}
\addvspace {10pt}
\contentsline {figure}{\numberline {1.1}{\ignorespaces Change of the regression hyperplane given by coefficients estimated with OLS method when one of the four observations (highlighted with red color) starts to deviate from the linear pattern.}}{9}{figure.1.1}
\addvspace {10pt}
\contentsline {figure}{\numberline {2.1}{\ignorespaces Illustration of the C-step algorithm. $(1)$ represents the value of $\of ^{(OLS,\boldsymbol {M_1}\boldsymbol {X}, \boldsymbol {M_1}\boldsymbol {y} )} (\boldsymbol {\mathaccentV {hat}05E{w}_1})$ (which is equal to the value $\oflts (\boldsymbol {m_1})$ ). $(2)$~represents the value of $\of ^{(OLS,\boldsymbol {M_2}\boldsymbol {X}, \boldsymbol {M_2}\boldsymbol {y} )} (\boldsymbol {\mathaccentV {hat}05E{w}_1})$ and $(3)$ represents the value of $\of ^{(OLS,\boldsymbol {M_2}\boldsymbol {X}, \boldsymbol {M_2}\boldsymbol {y} )} (\boldsymbol {\mathaccentV {hat}05E{w}_2})$ (which is equal to the value $\oflts (\boldsymbol {m_2})$). }}{24}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Value of the residual sum of squares (normalized) based on the number of the step of C-step algorithm for 100 different starting subsets. Dataset D3 was used with configuration $n=500, p=20$ and $30\%$ of the the outliers (see Section~\ref {chapterexperiments} for more details about the dataset). }}{26}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Tree consisting of all element subsets for $n=4$ and $h=3$. Leaves with blue border color represents $h$-element subsets.}}{46}{figure.2.3}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Step in the algorithms when due to the sorted siblings node $j_k$ can be trimmed}}{48}{figure.2.4}
\addvspace {10pt}
\addvspace {10pt}
\addvspace {10pt}
\addvspace {10pt}
