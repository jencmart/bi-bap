\chapter{Algorithms}

% mathbb - double -> R,N,Z ...
% mathcal - curved -> N, O
% sigma tilde hat 
In previous chapter we've covered necessary theory needed to implement algorithms that are in this chapter. Let's quickly recap most important fact that we know so far.

With robust linear regression problem we assume that our model with intercept
\begin{equation}
		y_i = \vec{w}^T\vec{x_i} + \varepsilon_i
\end{equation}
where $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$ is $i.i.d.$ random variable has some displacements in upmost half of the explanatory $\vec{x_i}$ or dependent $y_i$ variables. Thus only some subset $\vec{\tilde{X}} = \vec{M}\vec{X}$ with corresponding $\vec{\tilde{y}} = \vec{}\vec{y}$ where $\vec{M} = diag(\vec{m})$ and $\vec{m} \in Q^{(n,h)}$ can be perceived so that 

\begin{equation}
	\tilde{y}_i \sim \mathcal{N}(\vec{w}^T\vec{\tilde{x}_i}, \sigma^2).
\end{equation}
We'll denote this subset simply as $h subset$ of data in order to simplify following text.
We've also learnt that finding solution of LTS we need to find correct $h$ subset and then calculate estimate of regression coefficients $\tilde{w}$. 
So to find exact solution of LTS we need to go through all h subsets, find the correct one and calculate ordinary least squares fit to get the regression coefficients. There are not much of other options because objective function is non-differentiable and non-convex with lots of local minima.  

We also know that exhaustive approach will fail due  exponential size of $Q^{(n,h)}$. So what are the possibilities? First attempts were based on iterative removal data samples whose residuum had the highest value based on OLS fit on whole dataset. Such attempts were proven to be completely wrong because initial OLS fit is usually already  heavily affected by outliers and we my end up removing data samples which represents original model.

Then there are algorithms based on purely on random approach. 

On such algorithm  is Random solution algorithm \cite{bai2003random} which is basically randomly selects $L$ $h$ subsets and subsequently compute OLS fit on each of them and selecting fit with smallest $RSS$ and consider it a approximate solution. Such approach is very simple, but in general probability of selecting at least one such subset from $L$ subsets which don't contain outliers thus has a chance of producing good result goes to zero for increasing number of data samples $n$ as we'll describe in detail in \ref{hrandomsamples}.

Another very similar algorithm called Resampling algorithm introduced in \cite{rousseeuw1987robust}
have basically just a little difference and that it select vectors from $Q^{(n,p+1)}$ instead of 
$Q^{(n, h)}$. This minor tweak has not only higher chance to succeed because number of vectors in this set is significantly lower than in $Q^{(n, h)}$ (at least if $h$ conservatively chosen thus $h = [(n/2] + [(p+1)/2]$) but also because probability of selecting $L$ subsets of size which is independent of $n$ gives nonzero probability of selecting at least one subset such that it don't include outliers see \ref{prandomsamples} for more details.

Generating all possible h subsets is computationally hard and relying on selecting random subsets don't produce sufficiently good results. So what are our options? In \cite{hawkins1999improved} two criterions called \defterm{necessary conditions} are introduced. They talk about necessary properties which some $h$ subset must satisfy so it could be set which leads to global optima of LTS. Let's introduce those two necessary conditions. For that it's convenient not to only label $h$ subset of used observations but also complementary subset of not used observations. We'll refer to this complementary subset as $g$ subset.

\begin{theorem} Strong necessary condition.
The criterion cannot be improved by exchanging any of the observations from $g$ subset 
for any of the currently used observations in $h$ subset. Thus $\vec{m} \in Q^{(n, h)}$ meets the criterion if  $J(\vec{m}) \leq J(\vec{m_{swap}})$ where $\vec{m_{swap}}$ is any vector from $Q^{(n, h)}$ such that it has same values except one swapped.
\end{theorem}

\begin{proof}
	Trivial. LTS uses subset of $h$ observations that minimize it's objective function. To be this true none of the swaps between observations from $h$ subset and $g$ subset  must not improve (reduce) it's objective function. 
\end{proof}
Based on this idea algorithm can be created. We'll discuss it in detail in \ref{section_feasible_solution}.

Second necessary condition named \defterm{weak necessary condition} 
\begin{theorem}
	$\vec{m} \in Q^{(n, h)}$ meets the criterion if for each observation from $h$ subset has smaller (or equal) squared residual than any observation from $g$ subset.
\end{theorem}	
Again, based on this criterion an algorithm can be crated. Corollary of this criteria together with proof can be found in \ref{section_fast_lts}. 

Very interesting consequence which we'll use later gives us following lemma.

\begin{lemma} \label{lemma_conditions}
	Strong necessary condition is not satisfied unless weak necessary condition in satisfied. Thus if strong condition is satisfied then weak is also. 
\end{lemma}

\begin{proof}
	We'll make proof by contradiction. Let's assume that we have $\vec{m} \in Q^{(n, h)}$ and $J(\vec{m})$ for which strong necessary condition is satisfied but weak necessary condition is not. That means there exists $\vec{x_i}$ with $y_i$ from $h$ subset and $\vec{x_j}$ and $y_j$ from $g$ subset such that $r_j^2 < r_i^2$. Thus 

	\begin{equation} 
		\oflts(\vec{m}) > \oflts(\vec{m}) + r_j^2 - r_i^2  
	\end{equation}

	Now we just need to show that $\vec{m_{swap}}$ vector that is created by swapping that $j$th observation from $g$ subset with $i$th observation from $h$ subset leads to 

	\begin{equation} 
	  \oflts(\vec{m}) + r_j^2 - r_i^2  \geq \oflts(\vec{m_{swap}}).
	\end{equation}
	That's indeed trivial because $\oflts(\vec{m_{swap}})$ is in fact just OLS hat minimize objective function on given subset of observations. Thats of course contradiction with our assumption which says that strong necessary condition is already satisfied. 
\end{proof}
When we'll discuss algorithms based on this conditions we'll show that algorithm based on weak necessary condition is much faster than algorithm based on strong necessary condition which will lead us to another algorithm where we'll use \ref{lemma_conditions}.

Now we've covered all necessary theoretical background and it's time to introduce currently popular algorithms of computing LTS estimate.

\section{Computing OLS}  \label{ols:computing}
In this section we'll describe what algorithms of computing OLS exists. We'll see that beside computing OLS directly from the objective function there are better ways. Let's now start with this straightforward  approach.

\begin{lemma}
	Time complexity of OLS  on $\m{X}^{n \times p}$ and $\m{Y}^{n \times 1}$ is $O(p^2n)$.
\end{lemma}

\begin{proof}
	Normal equation of OLS is $\vec{\hat{w}} = (\m{X^T}\m{X})^{-1}\m{X^T}\m{Y}$.
	Time complexity  of matrix multiplication $\m{A}^{m \times n}$ and  $\m{B}^{n \times p}$ is $\sim \mathcal{O}(mnp)$.
	Time complexity of matrix $\m{C}^{m \times m}$ is $\sim \mathcal{O}(m^3)$
	So we need to compute 
	$\m{A} = \m{X^T}\m{X} \sim \mathcal{O}(p^2n)$ and
	$\m{B} = \m{X^T}\m{Y} \sim \mathcal{O}(pn)$ and
	$\m{C} = \m{A}^{-1} \sim \mathcal{O}(p^3)$ and finally 
	$\m{C}\m{B} \sim \mathcal{O}(p^2)$. 
	That gives us $\mathcal{O}(p^2n + pn + p^3 + p^2)$. Because  $\mathcal{O}(p^2n)$ and 
	$\mathcal{O}(p^3)$ asymptotically dominates over $\mathcal{O}(p^2)$ and $\mathcal{O}(pn)$ we can
	write $\mathcal{O}(p^2n + p^3)$.

	\todo{ CO zo toho je vic? Neni casove narocnejsi vynasobeni $\m{X^T}\m{X}$ nez inverze, kdyz bereme v uvahu $n >> p$ ???}
	
\end{proof}
\todo{complete this section with describing computation of OLS using matrix decomposition}

\section{Computing LTS}

\begin{note}
	When discussing following algorithms, we'll refer to given $\vec{X}$ and $y$ as to \textbf{\textit{data set}} and to $y_i$ with corresponding $\vec{x_i}$ as to \textbf{\textit{data sample}} or \textbf{\textit{observation}}. Sometimes it's also useful to refer to multiple observations as to subset of observations. When we want to mark subset of observations  
	$y_i\\, \vec{x_i}\\, i \in H\,, H \subset \{{1,2,\ldots , n\}}$ we can simply refer to it as to subset of observations $H$. Sometimes it's also useful to mark matrix $\vec{X}$ with only some subset of observations which we'll do by $\vec{X_H}$.  
\end{note}

% \textbf{X} - bold
% \pmb{X} - strong bolda
% \boldmath{X} - quite normal
%  $\textbf{X}^T$ - TRANSPOSED MATRIX

% change arrowed vector to the bold vector
% mathbf give bold 
% bold-symbol gives bold cursive
% **********************************************************************************
% *************% ************* ************ FAST - LTS *********************************************
% ************* ************ FAST - LTS *********************************************
% ************* ************ FAST - LTS *********************************************
% ************* ************ FAST - LTS *********************************************
% ************ FAST - LTS *********************************************
% **********************************************************************************
\section{FAST-LTS} \label{section_fast_lts}
In this section we will introduce FAST-LTS algorithm\cite{rouss:2000}. 
It is, as well as in other cases, iterative algorithm. We will discuss all main components
of the algorithm starting with its core idea called concentration step which '
authors simply call C-step.

% $\showVec{\hat{w}}{w}{n}$
% $\boldsymbol{Y}$ - bold cursive

\subsection{C-step}
We will show that from existing LTS estimate $\boldsymbol{\hat{w}_{old}}$ we 
can construct new LTS estimate $\boldsymbol{\hat{w}_{new}}$ which objective 
function is less or equal to the old one. Based on this property we will be able 
to create sequence of LTS estimates which will lead to better results.

% \usepackage{etoolbox}


%******************************** C-STEP theorem  ***************************************************%

\begin{theorem}
Consider dataset consisting of
$\vec{x_1}, \vec{x_2} \ldots,\vec{x_n}$ explanatory variables where 
$\vec{x_i}\in\mathbb{R}^p\,, \forall \vec{x_i} = (fx^i_1, x^i_2,\ldots,x^i_p)$ where $x^i_1 = 1$
and its corresponding $y_1, y_2,\ldots,y_n$ response variables. 
Let's also have $\vec{\hat{w}_0}\in\mathbb{R}^p$ any p-dimensional vector and 
$H_0 = \{{h_i ; h_i \in\mathbb{Z}\,, 1 \leq h_i \leq n\}}\,, |H_0| = h$. 
Let's now mark $RSS(\what{0}) = \sum_{i\in H_0} (r_0(i))^2$ where 
$r_0(i) = y_i - (w_1^0x^i_1 + w_2^0x^i_2 +\ldots+ w_p^0x^i_p$).
%Let's now mark  $H_1 = \{{h_i ;  1 \leq h_i \leq n\   \}}$ 
%uch that 
Let's take $\hat{n} = \{{1,2,\ldots,n\}}$ and mark
$\pi: \hat{n} \rightarrow \hat{n}$ permutation of $\hat{n}$ such that $|r_0({\pi(1)})| \leq |r_0({\pi(2)})| \leq \ldots \leq |r_0({\pi(n)})|$
and mark $H_1 = \{{\pi(1)\,, \pi(2)\,,... \pi(h)\}}$ set of $h$ indexes corresponding to $h$ smallest absolute residuals $r_0(i)$.
Finally take $\vec{\hat{w}^{OLS(H_1)}_1 }$ ordinary least squares fit on $H_1$ subset of observations
and its corresponding $RSS(\what{1}) = \sum_{i\in H_1} (r_1(i))^2$ sum of least squares. Then
\[ 
	RSS(\what{1}) \leq RSS(\what{0}) \numberthis
\]
\end{theorem}

\begin{proof}
	Because we take $h$ observations with smallest absolute residuals $r_0$, then for sure $\sum_{i\in H_1} (r_0(i))^2 \leq \sum_{i\in H_0} (r_0(i))^2 =  RSS(\what{0})$.
	When we take into account that Ordinary least squares fit $OLS_{H_1}$ minimize objective function of 
	$H_1$ subset of observations, then for sure  $RSS(\what{1}) =  \sum_{i\in H_1} (r_1(i))^2 \leq \sum_{i\in H_1} (r_0(i))^2$.
	Together we get $$RSS(\what{1})=\sum_{i\in H_1}(r_1(i))^2\leq\sum_{i\in H_1}(r_0(i))^2\leq\sum_{i\in H_0}(r_0(i))^2=RSS(\what{0})$$
\end{proof}

%******************************** C-STEP algorithm ***************************************************************************%

\begin{corollary} 
	Based on previous theorem, using some $\vec{\hat{w}^{OLS(H_{old})}}$  on $H_{old}$ subset of observations we can
	construct $H_new$ subset with corresponding $\vec{\hat{w}^{OLS(H_{new})}}$ such that $RSS(\vec{\hat{w}^{OLS(H_{new})}}) \leq RSS(\vec{\hat{w}^{OLS(H_{old})}})$. 
	With this we can apply above theorem again on $\vec{\hat{w}^{OLS(H_{new})}}$ with $H_{new}$. This will lead to the iterative sequence of
	$RSS(\what{{old}}) \leq RSS(\what{{new}}) \leq \ldots$. One step of this process is described by following pseudocode. Note that for C-step we actually need only $\vec{\hat{w}}$ 
	 without need of passing  $H$.
\end{corollary}
\todo{include and reference nice image showing one c-step }

\begin{algorithm}[H]
	\label{alg:Cstep}
    % \SetKwInOut{Input}{input}
    % \SetKwInOut{Output}{output}
    \KwIn{dataset consiting of $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ and $\boldsymbol{y} \in \mathbb{R}^{n \times 1}$,  $\what{{old}} \in \mathbb{R}^{p \times 1}$}
    \KwOut{ $\what{{new}}$, $H_{new}$ }
	\caption{C-step}
	
	$R \gets \emptyset$\;
	\For{$i \gets 1$ \textbf{to} $n$}{  
		$R \gets R \cup \{ |y_i - \what{{old}} \vec{x_i}^T |\}$\;
	}
	$H_{new} \gets $ select set of $h$ smallest absolute residuals from $R$\;
	$\what{{new}} \gets OLS(H_{new})$\;
	\Return{ $\what{{new}}$\,, $H_{new}$ }\;
\end{algorithm}

%******************************** C-STEP alg. time complexity ******************************************%
\begin{observation} 
	Time complexity of algorithm C-step \ref{alg:Cstep} is the same as time complexity as OLS. Thus $O(p^2n)$
	\todo{create better proof. And take into account both versions - directly vs. using decomposition}
\end{observation} 


\begin{proof}
	In C-step we must compute $n$ absolute residuals. Computation of one absolute residual consists of
	matrix multiplication of shapes $1 \times p$ and $p \times 1$ that gives us $\mathcal{O}(p)$. Rest is in constant time.
	So time of computation $n$ residuals is $\mathcal{O}(np)$.
	Next we must select set of $h$ smallest residuals which can be done in $\mathcal{O}(n)$ using modification of algorithm QuickSelect. \todo{reference or define quick select} 
	Finally we must compute $\hat{w}$ OLS estimate on $h$ subset of data.
	Because $h$ is linearly dependent on $n$, we can say that it is $\mathcal{O}(p^2n + p^3)$ which 
	is asymptotically dominant against previous steps which are $\mathcal{O}(np + n)$.
\end{proof}

As we stated above, repeating algorithm C-step will lead to sequence of $\what{1}, \what{2} \ldots$ 
on subsets $H_1, H_2 \ldots$ with corresponding residual sum of squares
$RSS(\what{{1}}) \geq RSS(\what{{2}}) \geq \ldots$. One could ask if this sequence will converge, so that
$RSS(\what{{i}}) == RSS(\what{{i+1}})$. 
Answer to this question will be presented by the following theorem.


%******************************** C-STEP alg. will converge ********************************************%
\begin{theorem}
	Sequence of C-step will converge to $\what{{m}}$ after maximum of $m = {n \choose h}$
	so that $RSS(\what{{m}}) == RSS(\what{{n}})\,, \forall n\geq m$ where $n$ is number of data samples 
	and $h$ is size of subset $H_i$.
\end{theorem}

\begin{proof}
	Since  $RSS(\what{{i}})$ is non-negative and $RSS(\what{{i}}) \leq RSS(\what{{i+i}})$ the 
	sequence will converge. $\what{{i}}$  is computed out of subset 
	$H_i \subset \{{1,2,\ldots,n\}}$. When there is finite number of subsets of size $h$ out of $n$ samples, namely ${n \choose h}$, the sequence will converge at the latest after this number of steps.
\end{proof}

%******************************** ITERATE-C-STEP algorithm **********************************************%
Above theorem gives us clue to create algorithm described by following pseudocode.

\begin{algorithm}[H]
	\label{alg:RepeatCstep}
	\KwIn{dataset consiting of $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ 
	and $\boldsymbol{y} \in \mathbb{R}^{n \times 1}$,  $\what{{old}} \in \mathbb{R}^{p \times 1}\,, H_0 $}
    \KwOut{ $\what{{final}}$, $H_{final}$ }
	\caption{Repeat-C-step}
	\SetKw{Break}{break}
	$\what{{new}} \gets \emptyset$\;
	$H_{new} \gets \emptyset$\;
	$RSS_{new} \gets \infty $\;

	\While{$True$}{
		$RSS_{old} \gets RSS(\what{{old}})$\;
		$\what{{new}}$\,, $H_{new} \gets \boldsymbol{X}\,, \boldsymbol{y}\,, \what{{old}}$\;
		$RSS_{new} \gets RSS(\what{{new}})$\;
		\If{$RSS_{old} == RSS_{new}$}{
			\Break
		  }
		$\what{{old}} \gets \what{{new}}$
	}

	\Return{ $\what{{new}}$, $H_{new}$ }\;
\end{algorithm}

It is important to note, that although maximum number of steps of this algorithm is ${n \choose h}$ in practice it is very low, most often under $20$ steps.
\todo{include sime nice grap which show this. or table ?}
That is not enough for the algorithm $Repeat-C-step$ to converge to global minimum, but it is necessary condition. That gives us an idea how to create the final algorithm. \cite{rouss:2000}

Choose a lot of initial subsets $H_1$ and on each of them apply algorithm Repeat-C-step. From all converged subsets with corresponding $\hat{w}$ estimates choose that which has lowest $RSS(\hat{w})$. 

Before we can construct final algorithm we must decide how to choose initial subset $H_1$ and how many of them mean ``\emph{a lot of}''. First let's focus on how to choose initial subset $H_1$.

% \renewcommand{\O}[1]{$\mathcal{O}(#1)$}

% ************************************************	INITIAL H_1 SUBSET *************************************%
\subsection{Choosing initial $H_1$subset}

It is important to note, that when we choose $H_1$ subset such that it contains outliers, then iteration of  C-steps
usually won't converge to good results, so we should focus on methods with non zero probability of selecting $H_1$ such that it won't contain outliers.
There are a lot of possibilities how to create initial $hH_1$ subset. Lets start with most trivial one.


% ************************************************** RANDOM SELECTION **************************************%
\subsubsection{Random selection}
Most basic way of creating $H_1$ subset is simply to choose random $H_1 \subset \{{1,2,\ldots , n\}}$. Following observation will show that it not the best way.

\begin{observation} \label{hrandomsamples}
	With increasing number of data samples, thus with increasing $n$, the probability of choosing among $m$ random selections of $H_{1_1}, \ldots ,H_{1_m}$ the probability of selecting
	at least one $H_{1_i}$ such that its corresponding data samples does not contains outliers, goes to $0$.
\end{observation}

\begin{proof}
	Consider dataset of $n$ containing $\epsilon > 0$ relative amount of outliers. Let $h=(n+p+1)/2$ and $m$ is number of selections random $|H| = h$ subsets. Then
	\begin{align*}
		P(one~random~data~sample~not~outliers) &= (1-\epsilon) \\
		P(one~subset~without~outliers) &= (1-\epsilon)^h \\
		P(one~subset~with~at~least~one~outlier) &= 1-(1-\epsilon)^h \\
		P(m~subsets~with~at~least~one~outlier~in~each) &= (1-(1-\epsilon)^h)^m \\
		P(m~subsets~with~at~least~one~subset~without~outliers) &= 1-(1-(1-\epsilon)^h)^m \\
	\end{align*}

	Because $n \rightarrow \infty 	
	\Rightarrow (1-\epsilon)^h  \rightarrow 0 	
	\Rightarrow 1- (1-\epsilon)^h  \rightarrow 1
	\Rightarrow (1-(1-\epsilon)^h)^m  \rightarrow 1
	\Rightarrow 1- (1-(1-\epsilon)^h)^m  \rightarrow 0 $
\end{proof}

That means that we should consider other options of selecting $H_1$ subset. Actually if we would like to continue with selecting some random subsets, previous observation gives us clue, that we should choose it independent of $n$. Authors of algorithm came with such solution and it goes as follows.

% ***************************************************** P - SELECTION ***************************************%
\subsubsection{P-subset selection}
Let's choose subset $J \subset \{{1,2,\ldots,n\}}\,, |J| = p$. Next compute rank of matrix $\m{X}_{J:}$. If $rank(\m{X}_{J:}) < p$ add randomly selected rows to $\m{X}_{J:}$ without repetition until $rank(\m{X}_{J:}) = p$. Let's from now on suppose that $rank(\m{X}_{J:}) = p$. Next let us mark $\what{0} = OLS(J)$ and corresponding $(r_0(1)), (r_0(2)), \ldots ,(r_0(n))$ residuals.  Now mark $\hat{n} = \{{1,2,\ldots,n\}}$ and let
$\pi: \hat{n} \rightarrow \hat{n}$ be permutation of $\hat{n}$ such that $|r({\pi(1)})| \leq |r({\pi(2)})| \leq \ldots \leq |r({\pi(n)})|$. Finally put $H_1 = \{{\pi(1)\,, \pi(2)\,,... \pi(h)\}}$ set of $h$ indexes corresponding to $h$ smallest absolute residuals $r_0(i)$.

\begin{observation} \label{prandomsamples}
	With increasing number of data samples, thus with increasing $n$, the probability of choosing among $m$ random selections of $J_{1_1}, \ldots ,J_{1_m}$ the probability of selecting
	at least one $J_{1_i}$ such that its corresponding data samples does not contains outliers, goes toho
	$$ 1-(1-(1-\epsilon)^h)^m  > 0$$
\end{observation}

\begin{proof}
	Similarly as in previous observation.
\end{proof}

\begin{itshape}
Note that there are other possibilities of choosing $H_1$ subset other than these presented in \cite{rouss:2000}.
We'll properly discuss them in chapter \todo{write reference to section where this'll be}
\end{itshape}

Last missing piece of the algorithm is determining number of $m$ initial $H_1$ subsets, which will maximize probability to at least one of them will converge to good solution. Simply put, the more the better. So before we will answer this question properly, let's discuss some key observations about algorithm.

% ***************************************************** SPEED-UP ***************************************%

\subsection{Speed-up of the algorithm}
In this section we will describe important observations which will help us to formulate final algorithm. In two subsections we'll briefly describe how to optimize current algorithm. 

% *************************************************SELECTIVE ITERATION  ***************************************%
\subsubsection{Selective iteration}
The most computationally demanding part of one C-step is computation of OLS on $H_i$ subset and then 
calculation of $n$ absolute residuals. How we stated above, convergence is usually achieved under 20 steps. 
So for fast algorithm run we would like to repeat C-step as little as possible and in the same time didn't loose performance of algorithm. 

Due to that convergence of repeating C-step is very fast, it turns out, that we are able to distinguish between starts that will lead to good solutions and those who won't even after very little C-steps iterations. <based on empiric observation, we can distinguish good or bad solution already after two or three iterations of C-steps based on $RSS(\what{3})$ or $RSS(\what{4})$ respectively. 

So even though authors don't specify size of $m$ explicitly, they propose that after a few C-steps we can choose (say~10) best solutions among all $H_1$ starts and continue C-steps till convergence only on those best solutions.
This process is called Selective iteration.

\begin{itshape}
	We can choose $m$ with respect to observation \ref{prandomsamples}. In ideal case we would like to have probability of existence at least one initial $H_1$ subset close to $1$. As we see $m$ is exponentially dependent on $p$ and at the same time in practice we don't know percentage of outliers in dataset. So it is difficult to mention exact value. Specific values of $m$ in respect to data size is visible in table \todo{include some nice table}. So we can say that with $p < 10$ choosing $m = 500$ is usually safe starting point.
\end{itshape}

\subsubsection{Nested extension}
C-step computation is usually very fast for small $n$. Problem starts with very high $n$ say $n > 10^3$ because we need to compute OLS on $H_i$ subset of size $h$ which is dependent on $n$. And then calculate $n$ absolute residuals.

Authors came up with solution they call Nested extension. We will describe it briefly now.
\begin{itemize}
	\item If $n$ is greater than limit $l$, we'll create subset of data samples $L\,, |L| = l$ and divide this subset into $s$ disjunctive sets $P_1,P_2,\ldots,P_s\,, |P_i| = \frac{l}{s}\,, P_i\cap P_j  = \emptyset\,, \bigcup_{i=1}^{s} P_{i} = L$.
	\item For every $P_i$ we'll set number of starts $m_{P_i} = \frac{m}{l}$. 
	\item Next in every $P_i$ we'll create $m_{P_i}$ number of initial $H_{P_{i_1}}$ subsets and iterate C-steps for two iterations.
	\item Then we'll choose $10$ best results from each subsets and merge them together. We'll get family of sets
	$F_{merged}$ containing $10$ best $H_{P_{i_3}}$ subsets from each $P_i$.
	\item On each subset from  $F_{merged}$ family of subsets we'll again iterate $2$ C-steps and then choose $10$ best results.
	\item Finally we'll use these best $10$ subsets and use them to iterate C-steps till convergence.
	\item As a result we'll choose best of those $10$ converged results.
\end{itemize} 

\subsubsection{Putting all together}
We've described all major parts of the algorithm FAST-LTS. One last thing we need to mention is that even though C-steps iteration usually converge under $20$ steps it is appropriate to introduce two parameters $max_iteration$ and $threshold$ which will limit number of C-steps iterations in some rare cases when convergence is too slow. Parameter $max_iteration$ denotes maximum number of iterations in final C-step iteration till convergence. Parameter $threshold$ denotes stopping criterion such that $| RSS(\what{{i}}) - RSS(\what{{i+1}})| \leq threshold$ instead of 
$RSS_{i} == RSS_{i+1}$ . When we put all together, we'll get FAST-LTS algorithm which is described by following pseudocode.


\begin{algorithm}[H]
	\label{alg:FAST-LTS}
	\KwIn{$\boldsymbol{X} \in \in \mathbb{R}^{n \times p}, \boldsymbol{y} \in \mathbb{R}^{n \times 1}, m, l, s, max_iteration, threshold $}
    \KwOut{ $\vec{\hat{w_{final}}}$, $H_{final}$ }
	\caption{FAST-LTS}
	\SetKw{Break}{break}
	$\what{{final}} \gets \emptyset$\;
	$H_{final} \gets \emptyset$\;
	$F_{best} \gets \emptyset$\;

	\uIf{$n \geq l$}{
		$F_{merged} \gets \emptyset$\;
		% for each split
		\For{$i \gets 0$ \textbf{to} $s$}{
			% create xx number of starts
			$F_{selected}  \gets \emptyset$\;
			\For{$j \gets 0$ \textbf{to} $\frac{l}{s}$}{
			  $F_{initial} \gets Selective~iteration(\frac{m}{l})$\;
			  % on each starts iterate few c steps
			  \For{$H_i$ \textbf{in} $F_{initial}$}{
				$H_i \gets Iterate~C~step~few~times(H_i)$\;
				$F_{selected} \gets  F_{selected} \cup \{{ H_i \}} $\;
			  }
			}
			% among all starts select 10 best and add it to merged set
			$F_{merged} \gets F_{merged} \cup Select~10~best~subsets~from~F_{selected}$\;
		  }
		
		  % for each, say 50 best, iterate few and add it to best
		  \For{$H_i$ \textbf{in} $F_{merged}$}{
			$H_i \gets Iterate~C~step~few~times(H_i)$\;
			$F_{best} \gets F_{best} \cup \{{ H_i \}} $\;
		  }
		  $F_{best} \gets  Select~10~best~subsets~from~F_{best} $\;
	}
	\Else{
		$F_{initial} \gets Selective~iteration(m)$\;
		$F_{best} \gets  Select~10~best~subsets~from~F_{initial} $\;
	}

	% iterate till convergence on few best final results
	$F_{final} \gets \emptyset$\;
	$W_{final} \gets \emptyset$\;
	\For{$H_i$ \textbf{in} $F_{best}$}{
			$H_i, \what{i} \gets Iterate~C~step~till~convergence(H_i, max_iteration, threshold)$\;
			$F_{final} \gets F_{final} \cup \{{ H_i \}}$\;
			$W_{final} \gets W_{final} \cup \{{ \what{i} \}}$\;
	}

	% select one best result
	$\what{{final}}, H_{final} \gets select~what~with~best~RSS(F_{final}, W_{final})$\;

	\Return{ $\what{{final}}, H_{final}$  }\;
\end{algorithm}


% \begin{description}
% 	\item[BP] 
% 	\item[DP] 
% \end{description}



% ****************************************************************************************************
% ************************* FEASIBLE SOLUTION ******************************************************
% **************************************************************************************************
\section{Feasible solution} \label{section_feasible_solution}
In practical terms,
there is a decision to be made in considering the swaps â€“ should one
1. Accept the first swap that leads to an improvement in the criterion,
2. Search for the swap that leads to the greatest improvement, or
3. Search for a while, stopping at the first subset has been found that gives at least
some minimum threshold of improvement.
In complexity terms, there is no difference between these three approaches (since in
all of them to establish that a trial solution is feasible you need to evaluate all possible
swaps), but in practical terms 3 is a clear winner since it leads to many fewer inner
iterations than 1, and mostly much faster inner iterations than 2. However the overall
complexity of O(n2p) or O(n3p) remains, and means that the feasible solution algorithms
based on the case-swap necessary condition cannot be used for very large data sets.


% **********************************************************************************
% ************************* EXACT POLYNOMIAL ALGORITHM *****************************
% **********************************************************************************
\section{Exact algorithm}

\section{MMEA}
\section{Branch and bound}
\section{Adding row}
