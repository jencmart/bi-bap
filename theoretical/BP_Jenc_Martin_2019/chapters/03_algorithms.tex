\chapter{Algorithms}

In the previous chapter, we cover the theoretical background required to implement algorithms that are in this chapter. We introduced the discrete version of the LTS objective function which minimum is equivalent to the continuous one. Finding the minimum of this function requires to find the correct $h$ subset and then calculate the estimate of regression coefficients $\vec{w}$. To achieve this, we need to examine all $h$ subsets. The exhaustive approach fails due to the exponential size of $Q^{(n,h)}$. So what are the possibilities? 

\subsection{First attempts}
Initial algorithms were based on iterative removal data samples whose residuum had the highest value based on OLS fit on the whole dataset. Such attempts are known to be flawed \cite{hawkins:1994} because the initial OLS fit can be already profoundly affected by outliers and we may remove data samples which represent the actual model.

Other algorithms were based purely on a random approach. On such algorithm is Random solution algorithm \cite{bai2003random} which randomly selects $l$ of $h$ subsets and subsequently compute OLS fit on each of them and chooses fit with a minimum value of the objective function.  Such approach is straightforward, but the probability of selecting at least one subset from $l$ subsets which don't contain outliers thus has a chance of producing good result tends to zero for an increasing number of data samples $n$ as we'll describe in detail in section~\ref{section:random:h:samples}.

Another very similar algorithm called Resampling algorithm introduced in \cite{rousseeuw1987robust}. It selects vectors from $Q^{(n,p+1)}$ instead of $Q^{(n, h)}$. This minor tweak has a higher chance to succeed. Mainly because the probability of selecting $l$ subsets of size $p+1$ thus it is independent of $n$ gives the nonzero probability of selecting at least one subset such that it does not include outliers (see section~\ref{section:random:p:samples} for more details). Besides that the number of vectors in this set is significantly lower than in $Q^{(n, h)}$ (at least if $h$ is conservatively chosen so that $h = [(n/2] + [(p+1)/2]$).
 
\subsection{Strong and weak necessary conditions}
Generating all possible $h$ subsets is computationally exhaustive and relying on randomly generating  ``good'' $h$-element subsets does not lead to reliable results. So what are our options? In \cite{hawkins1999improved} two criteria called \defterm{weak necessary conditions} and \defterm{strong necessary conditions} are introduced. They state necessary properties which some $h$ subset must satisfy to be subset which leads to the global optimum of the LTS objective function. Let us introduce those two necessary conditions. For that, it is convenient not only to label a $h$ subset of \defterm{non-trimmed} observations but also the complementary subset of not used observations. We'll refer to this complementary subset as to \defterm{trimmed} subset.

%%%%%%%%%%% STRONG CONDITION %%%%%%%%%%%%%
\begin{definition} \label{strong_condition}
 A $h$-element subset corresponding to $\vec{m} \in Q^{(n, h)}$ satisfies  the \defterm{strong necessary condition} if for any vector $\vec{m_{swap}}$ which differs from $\vec{m}$ only by swapping an $1$ with one $0$, we get  $\oflts(\vec{m}) \leq \oflts(\vec{m_{swap}})$. Thus the value of discrete LTS objective function cannot be reduced by swapping one non-trimmed observation with one trimmed observation.
\end{definition}

Based on this fact an algorithm can be created. We'll discuss it in detail in section~\ref{section_feasible_solution}.

%%%%%%%%%%% WEAK CONDITION %%%%%%%%%%%%%
The second necessary condition is named \defterm{weak necessary condition} 

\begin{definition}
A $h$-element subset corresponding to $\vec{m} \in Q^{(n, h)}$ satisfies  the \defterm{strong necessary condition} if 
$r_i^2(\vec{\hat{w}^{(OLS, \vec{M}\vec{X, \vec{M}\vec{y})} } })$ for all trimmed observation is greater of equal to the greatest non-trimmed squared residuum $r_j^2(\vec{\hat{w}^{(OLS, \vec{M}\vec{X, \vec{M}\vec{y})} }})$.
\end{definition}    
Again, based on this criteria an algorithm can be created. The corollary of this criteria together with proof can be found in  section~\ref{section_fast_lts}. 

% %%%%%%    WEAK AND STRONG CONDITIONS LEMMA    %%%%%%%%%
The interesting consequence which we'll use later gives us the following lemma.

\begin{lemma} \label{lemma_conditions}
    The strong necessary condition is not satisfied unless the weak necessary condition is satisfied. Thus if a strong condition is satisfied then weak is also. 
\end{lemma}

\begin{proof}
		We make proof by contradiction. Let us assume that we have $\vec{\hat{w}^{(OLS, \vec{M}\vec{X, \vec{M}\vec{y})} }}$ where $\vec{m} \in Q^{(n, h)}$ for which strong necessary condition is satisfied, but the weak necessary condition is not. That means there exists $\vec{x_i}$ with $y_i$ from non-trimmed subset and $\vec{x_j}$ and $y_j$ from trimmed subset such that
		\begin{equation*} 
			r_j^2(\vec{\hat{w}^{(OLS, \vec{M}\vec{X, \vec{M}\vec{y})} }}) < r_i^2(\vec{\hat{w}^{(OLS, \vec{M}\vec{X, \vec{M}\vec{y})} }}).
	\end{equation*}
		 Thus 
    \begin{equation} 
        \oflts(\vec{m}) > \oflts(\vec{m}) + r_j^2 - r_i^2  
    \end{equation}
Now we need to show that $\vec{m_{swap}}$ vector that is created by swapping $j$th observation from trimmed subset with $i$th observation from non-trimmed subset leads to 
    \begin{equation} 
      \oflts(\vec{m}) + r_j^2 - r_i^2  \geq \oflts(\vec{m_{swap}}).
    \end{equation}
That is true because the value of $\oflts(\vec{m_{swap}})$  is minimum on the given subset of observations. That is, of course, contradiction with our assumption which says that strong necessary condition is already satisfied. 
\end{proof}

Now we have covered all the necessary theoretical background, and it is time to introduce currently known algorithms for computing the LTS estimate.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%   SECTION  COMPUTING THE OLS  (60 -- 300)   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Computing OLS}
In this section, we describe a few of many methods that can be used to obtain  $\vec{\hat{w}}^{(OLS,n)}$. Those methods are parts of the algorithms used to calculate the LTS estimate. 

In the following algorithms, we describe its time complexity. Because matrix multiplication is the fundamental part of all the following algorithms, it is, therefore, appropriate to mention a few facts about the time complexity of matrix multiplication. 

There are multiple algorithms for matrix multiplication, for example:
\begin{itemize}
  \item Naive algorithm; its time complexity is $\mathcal{O}(n^{3})$.
  \item Strassen algorithm; its time complexity is $\mathcal{O} (n^{2.8074})$ \cite{strassen1969gaussian}. 
  \item Coppersmith-Winograd; its time complexity is $\mathcal {O}(n^{2.375477}) \cite{coppersmith1990matrix}$
\end{itemize}

In practice, however, the naive algorithm is usually used. Even though some of those algorithms have been efficiently implemented and are known to be numerically stable (primarily variations of Strassen algorithm), their error bound is weaker than in case of the naive algorithm. For this reason, they are not used even when they might improve performance \cite{ballard2015improving}.

Moreover current implementations of the linear algebra namely LAPACK and more importantly BLAS level 3 routines implementing matrix-matrix multiplication which are widely used in such linear algebra software implement naive matrix-matrix multiplication \cite{laug}. 

For that reason we assume in this work that matrix multiplication is  $\mathcal{O}(n^{3})$. For not square matrices $ \vec{A} \in \mathbb{R}^{m \times n}, \vec{B} \in \mathbb{R}^{n\times p}$ it is then$ \mathcal{O}(mnp)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%  I N V E R S I O N    %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Computation using a matrix inversion}
Solving OLS using matrix inversion can be done as follows:

\begin{enumerate}
 \item Compute  $\vec{A} = \vec{X}^T\vec{X}$ and $\vec{B} = \vec{X}\vec{y}$.
  \item Find inversion of $\vec{A}$.
  \item Multiply $\vec{A}^{-1}\vec{B}$.
\end{enumerate}

\begin{observation} \label{time:complexity:ols:inversion}
    Time complexity of computing the OLS  estimate on $\vec{X} \in \mathbb{R}^{n \times p}$ and $\vec{y}  \in \mathbb{R}^{n \times 1}$ using matrix inversion is $O(p^2n)$ .
\end{observation}

\begin{proof}
First, we compute 
$\vec{A} = \vec{X^T}\vec{X}$  and
$\vec{B} = \vec{X^T}\vec{y}$. 
That gives us $\mathcal{O}(p^2n + pn)$.
Next, we compute inversion  $\m{C} = \m{A}^{-1}$ which gives us $\mathcal{O}(p^3)$.
Finally, we compute
$ \vec{\hat{w}}^{(OLS,n)} = \m{C}\m{B}$ which is $\mathcal{O}(p^2)$. 
Together we get $ \mathcal{O}(p^2n + pn + p^3 + p^2)$.
$p^2n$ and $p^3$ asymptotically dominates over the rest so 
\begin{equation}
\mathcal{O}(p^2n + pn + p^3 + p^2) \sim \mathcal{O}(p^2n + p^3).
\end{equation}
Moreover if we assume that $n \ge p$ and usually even $n \gg p$  we get $\mathcal{O}(p^2n + p^3) \sim \mathcal{O}(p^2n)$.
\end{proof}

Computing OLS estimate using matrix inversion is possible, however in most cases not used in practice because of the low numerical stability computing matrix inversion.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%  C H O L E S K Y    %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Computation using the Cholesky decomposition}

\begin{definition}
    Let $\vec{A} \in \mathbb{R}^{n \times n}$ be an symmetric positive definite matrix. Then it is possible to find lower triangular matrix $L$ so that 
    \begin{equation}
        \vec{A} = \vec{L}\vec{L^T}
    \end{equation}
    We call this decomposition as \defterm{Cholesky factorization} (sometimes also  \defterm{Cholesky decomposition})
\end{definition}
    
If we look at our problem of finding a solution to 
\begin{equation}
    \vec{X}^T\vec{X}\vec{w} = \vec{X}^T\vec{y},
\end{equation}
we can easily rewrite it as 
\begin{equation}
    \vec{Z}\vec{w} = \vec{b}
\end{equation}
where $\vec{Z} = \vec{X}^T\vec{X}$ is symmetric positive definite matrix and $\vec{b} = \vec{X}^T\vec{y} $. Because $\vec{Z}$ can be factorized as $\vec{L}\vec{L^T}$ the solution can be found easily by substitution

\begin{align}
    \vec{L}\vec{d} = \vec{b} \\
    \vec{L}^T\vec{w} = \vec{d},
\end{align}
where $\vec{d}$ is solved by forward substitution and $\vec{w}$ is then solved by backward substitution which ten represents $\vec{\hat{w}}^{(OLS,n)}$ estimate.

%%%%%%% C H O L E S K Y     A L G O R I T H M %%%%%%%%%%%%%%

So the algorithm for solving OLS using Cholesky factorization goes as follows:
\begin{enumerate}
  \item Compute $\vec{X}^T\vec{X}$ and   $\vec{X}^T\vec{y}$.
  \item Compute Cholesky factorization $\vec{Z} = \vec{L}\vec{L}^T$ where $\vec{Z} = \vec{X}^T\vec{X}$.
  \item Solve lower triangular system $\vec{L}\vec{d} = \vec{b}$  where $ \vec{b} = \vec{X}^T\vec{y}$  for $\vec{d}$ using forward substitution.
  \item Solve upper triangular system $ \vec{L}^T\vec{w} = \vec{d}$ for $\vec{w}$.
\end{enumerate}

\begin{observation} \label{time:complexity:ols:cholesky}
    The time complexity of solving OLS using Cholesky factorization is $\mathcal{O}(p^2n)$
\end{observation}

\begin{proof}
    First step requires two matrix multiplication $\vec{X}^T\vec{X}$ which is $\mathcal{O}(np^2)$ and  $\vec{X}^T\vec{y}$ which is $\mathcal{O}(np)$.
    Second step represents computing Cholesky factorization. This can be done in $\mathcal{O}(\frac{1}{3}p^3)$ \cite{krishnamoorthy2013matrix}
    In the third and fourth step, we are solving triangular systems both of them require  $\mathcal{O}(\frac{1}{2}p^2)$, so together $\mathcal{O}(p^2)$.

    Putting all steps together we get  

    \begin{equation} 
        \mathcal{O}(np^2 + np + \frac{1}{3}p^3 + p^2)
    \end{equation}
    and because we assume $n \geq p$ and most usually  $n \gg p$ then multiplication $\vec{X}^T\vec{X}$ asymptotically dominates over the rest so we get $\mathcal{O}(p^2n)$.
\end{proof}

% \begin{note}
%     In case we have $\vec{X}^T\vec{X}$ and  $\vec{X}^T\vec{y}$ computed in advance, then it is asymptotically dominated by Cholesky factorization 
%     \begin{equation} \label{time:complexity:ols:cholesky:we:have:xx:xy}
%         \mathcal{O}(\frac{1}{3}p^3) \sim \mathcal{O}(p^3)
%     \end{equation}
% \end{note}

Computation OLS estimate using the Cholesky factorization is more numerically stable than using matrix inversion and time complexity is asymptotically similar. This approach is however not usually used in practice as well. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%  Q R     D E C O M P O S I T I O N     %%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Computation using the QR decomposition}

Let us now look on a similar method of computing OLS estimate and which does not require multiplying $\vec{X}^T\vec{X}$. 

\begin{definition}
    Let $\vec{A} \in \mathbb{R}^{n \times n}$ be any square matrix. Then it is possible to find matrices $\vec{Q}$ and $\vec{R}$ so that 

    \begin{equation}
        \vec{A} = \vec{Q}\vec{R},
    \end{equation}
    where $\vec{Q} \in \mathbb{R}^{n \times n}$ is orthogonal matrix and $\vec{R} \in \mathbb{R}^{n \times n}$ is upper triangular matrix. 
This decomposition is known as \defterm{QR decomposition}.
\end{definition}
If matrix $\vec{A} \in \mathbb{R}^{m \times n}$ thus is not square, then decomposition can be found as

    \begin{equation}
        \vec{A} = \vec{Q}\vec{R} = \vec{Q} \begin{bmatrix}
            \vec{R_1} \\
            \vec{0}
        \end{bmatrix}
        =  \begin{bmatrix}
            \vec{Q_1} & \vec{Q_2}
        \end{bmatrix}  \begin{bmatrix}
            \vec{R_1} \\
            \vec{0}
        \end{bmatrix} 
        = \vec{Q_1} \vec{R_1}
    \end{equation}
    Where $\vec{Q} \in \mathbb{R}^{m \times m}$ is orthogonal matrix, $\vec{R_1} \in \mathbb{R}^{n \times n} $ is upper triangular matrix, $\vec{0} \in \mathbb{R}^{ (m-n) \times n} $  is zero matrix,  $\vec{Q_1} \in \mathbb{R}^{n \times n}$ is matrix with orthogonal columns and  $\vec{Q_2} \in \mathbb{R}^{(m-n) \times n}$ is also matrix with orthogonal columns.
    
That means that  $\vec{X}$ can be factorized as
\begin{equation}
    \vec{X} = \vec{Q}\vec{R}.
\end{equation}
Then
\begin{equation}
    \vec{X}^T\vec{X} = (\vec{Q}\vec{R})^T \vec{Q}\vec{R} = \vec{R}^T\vec{Q}^T\vec{Q}\vec{R}
\end{equation}
and because $\vec{Q}$ is orthogonal, then $\vec{Q}^T\vec{Q} = \vec{I}$ so that
\begin{equation}
    \vec{X}^T\vec{X} = \vec{R}^T\vec{R}.
\end{equation}
Because $\vec{X} \in  \mathbb{R}^{n \times p}$ is not square matrix then as we shown $\vec{R} = \begin{bmatrix}
    \vec{R_1} \\
    \vec{0}
\end{bmatrix}$
so that 
\begin{equation}
    \vec{X}^T\vec{X} = \vec{R_1}^T\vec{R_1}.
\end{equation}
 Where $\vec{R_1^T}$ is lower triangular matrix. 

Solution to
\begin{equation}
    \vec{X}^T\vec{X}\vec{w} = \vec{X}^T\vec{y}
\end{equation}
is then given by 
\begin{equation}
    \vec{R_1}^T\vec{R_1}\vec{w} = \vec{R_1}^T\vec{Q_1}^T \vec{y}
\end{equation}
and because we assume $\vec{X}$ have full column rank, thus is invertible, we can simplify it to
\begin{equation}
    \vec{R_1}\vec{w} = \vec{b_1}
\end{equation}
where $ \vec{b_1} = \vec{Q_1}^T \vec{y}$. Because $\vec{R_1}$ is upper triangular matrix, solution of $\vec{w}$ is trivial using backward substitution.  $\vec{w}$ then represents our OLS estimate $\vec{\hat{w}}^{(OLS,n)}$.

\begin{note}
        We can see that Cholesky factorization  $\vec{X}^T\vec{X} = \vec{L}\vec{L}^T$ is closely connected to the QR decomposition $\vec{X} = \vec{Q_1}\vec{R_1}$ so that  
        \begin{equation} \label{qrcholesky}
                \vec{R_1^T} = \vec{L}
            \end{equation}
\end{note}

%%%%%%      Q R    A L G O R I T H M      %%%%%%%%%%%%

So the algorithm for solving OLS using QR decomposition can go as follows:
\begin{enumerate}
  \item Calculate QR decomposition $\vec{X} = \vec{Q}\vec{R}^T = \vec{Q_1}\vec{R_1}^T$.
  \item Calculate $\vec{b_1} = \vec{Q_1}^T \vec{y}$.
  \item Solve upper triangular system $\vec{R_1}\vec{w} = \vec{b_1}$ for $\vec{w}$.
\end{enumerate}

QR factorization can be calculated in multiple ways. The most basic method is applying the Gram–Schmidt process
to columns of matrix $\vec{X}$. This approach is not numerically stable so in practice is not used as much as two other methods. Those are \defterm{Householder transformations} and \defterm{Givens rotations}.
The time complexity of both algorithms is similar. QR decomposition of matrix $\vec{X} \in \mathbb{R}^{n \times p}$ is 

\begin{equation}
    \mathcal{O}(2p^2n - \frac{2}{3}p^3).
\end{equation}
\todo{find citation}

On the other hand, using Givens rotation for QR decomposition of the same matrix is 
\begin{equation} \label{givenstimenoproof}
    \mathcal{O}(3np^2 - p^3)
\end{equation}
so Householder transformations are about $50\%$ faster, but Givens rotations are more numerically stable. 
Moreover, Givens rotations and are suitable for sparse matrices.  This property of Givens rotations we use in section \todo{ref}.
That is why we describe Givens rotation in detail in \ref{givensrotation}. We also make proof of \eqref{givenstimenoproof} time complexity there.

For now, let us look at time complexity of solving OLS using Householder transformations. 
\begin{observation}
    The time complexity of solving OLS estimate using QR decomposition by Householder transformations is $\mathcal{O}(2p^2n - \frac{2}{3}p^3)$
\end{observation}

\begin{proof}
    In the first step, we calculate QR decomposition. Householder transformations can be done in $\mathcal{O}(2np^2 - \frac{2}{3}p^3)$.
    Second step consist of matrix multiplication $\vec{Q_1}^T \vec{y}$ which is $\mathcal{O}(np)$.  In the last step we are solving upper triangular systems which is $\mathcal{O}(\frac{1}{2}p^2)$.
    Putting all steps together we get  

    \begin{equation} \label{time:complexity:ols:qr:householder}
        \mathcal{O}(2np^2 - \frac{2}{3}p^3 + np + \frac{1}{2}p^2)
    \end{equation}
    We can see that $p^2n$ asymptotically dominates.
\end{proof}

The QR decomposition is considered as a standard way of computing OLS estimate because of its high numerical stability. Let us mention that a little slower, but a more stable method of computing OLS estimate exists, and that is the singular value decomposition (SVD). On the other hand, the QR decomposition is sufficiently stable for most cases so describing SVD decomposition is out of the scope of this work.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%        FAST    LTS       %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{FAST-LTS} \label{section_fast_lts}
In this section, we introduce the FAST-LTS algorithm\cite{rouss:2000}. 
It is, as well as other algorithms we introduce, iterative algorithm. We discuss all main components of the algorithm starting with its core idea called a concentration step which authors call a \defterm{C-step}.

\subsection{C-step}
We show that from an existing LTS estimate $\boldsymbol{\hat{w}}$ we can construct a new LTS estimate $\boldsymbol{\hat{w}_{new}}$ so that value the objective function at $\boldsymbol{\hat{w}_{new}}$ is less or equal to the value at $\boldsymbol{\hat{w}}$. Based on this property, the algorithm creates a sequence of LTS estimates which leading to better results.


%%%%%%%%%     C-STEP theorem       %%%%%%%%%%%%

\begin{theorem}
Consider 
$\vec{X} \in \mathbb{R}^{n \times p}$ and 
$\vec{y} \in \mathbb{R}^{n \times 1}$.  
Let us also have $\vec{w_0}\in\mathbb{R}^p$ and $\vec{m_0} \in Q^{(n,h)}$. 
Let us put $L_0 = \sum\limits_{i=1}^n m^{(0)}_i r_{i}^2(\vec{w_0})$ where $r_{i}^2(\vec{w_0}) = y_i - (w_1^0x^i_1 + w_2^0x^i_2 +\ldots+ w_p^0x^i_p$).
Let us take $\hat{n} = \{{1,2,\ldots,n\}}$ and mark
$\pi: \hat{n} \rightarrow \hat{n}$ permutation of $\hat{n}$ such that $|r_0({\pi(1)})| \leq |r_0({\pi(2)})| \leq \ldots \leq |r_0({\pi(n)})|$
and mark   $\vec{m_1} \in Q^{(n,h)}$  such that $m^1_i = 1$ for $i \in \{{\pi(1)\,, \pi(2)\,,... \pi(h)\}}$ and  $m^1_i = 0$  otherwise. 
This means that  $\vec{m_1}$ corresponds to $h$ subset with smallest absolute residuals $r_{i}^2(\vec{w_0})$.

Finally, take
$\of^{(OLS,\vec{M_1}\vec{X},  \vec{M_1}\vec{y} )} (\vec{w})$ least squares fit on $m_1$ subset of observations and its corresponding 
$L_1 = \sum\limits_{i=1}^n m^{(1)}_i r_{i}^2(\vec{w_1})$. Then
\begin{equation} 
    L_1  \leq L_0
\end{equation}
\end{theorem}

\begin{proof}
    Because we take $h$ observations with smallest absolute residuals $r_{i}^2(\vec{w_0})$, then for sure
$\sum\limits_{i=1}^n m^{(1)}_i r_{i}^2(\vec{w_0})
\leq
\sum\limits_{i=1}^n m^{(0)}_i r_{i}^2(\vec{w_0}) =  L_0
$.
When we take into account that the OLS estimate minimizes the objective function of $\vec{m_1}$ subset of observations, then for sure  
$L_1 =  \sum\limits_{i=1}^n m^{(1)}_i r_{i}^2(\vec{w_1})
 \leq 
\sum\limits_{i=1}^n m^{(1)}_i r_{i}^2(\vec{w_0})$.

Together we get 
$
L_1 = \sum\limits_{i=1}^n m^{(1)}_i r_{i}^2(\vec{w_1})
\leq
\sum\limits_{i=1}^n m^{(1)}_i r_{i}^2(\vec{w_0})
\leq
\sum\limits_{i=1}^n m^{(0)}_i r_{i}^2(\vec{w_0}) =  L_0
$
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%    C-STEP algorithm    %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{corollary} 
    Based on the previous theorem, using some $\vec{\hat{w}_{old}^{OLS(h, n)}}$  of $\vec{m_{old}}$ subset of observations we can construct $\vec{m_{new}}$ subset with corresponding $\vec{\hat{w}_{new}^{OLS(H_{new})}}$ such that $L_{new} \leq L_{old}$. 
    Applying the theorem repetitively leads to the iterative sequence of
    $L_{1} \leq L_{2} \leq \ldots$. One step, called \defterm{C-step} of this process is described by the following pseudocode. Note that for C-step we  need only $\vec{\hat{w}}$ 
     without the need of passing  $\vec{m}$.
\end{corollary}

\begin{figure}[h]
\centering
\missingfigure{Image visualizing  one C-step}
\caption{todo caption}
\label{figure:one:c:step}
\end{figure}

\begin{algorithm}[H]
    \label{alg:Cstep}
    % \SetKwInOut{Input}{input}
    % \SetKwInOut{Output}{output}
    \KwIn{dataset consiting of $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ and $\boldsymbol{y} \in \mathbb{R}^{n \times 1}$,  $\what{{old}} \in \mathbb{R}^{p \times 1}$}
    \KwOut{ $\what{{new}}$, $m_{new}$ }
    \caption{C-step}
    
    $R \gets \emptyset$\;
    \For{$i \gets 1$ \textbf{to} $n$}{  
        $R \gets R \cup \{ |y_i - \what{{old}} \vec{x_i}^T |\}$\;
    }
    $m_{new} \gets $ select set of $h$ smallest absolute residuals from $R$\;
    $\vec{\hat{w_{new}}} \gets$ OLS fit on $m_{new}$ subset;
    \Return{ $\what{{new}}\,, m_{new}$ }\;
\end{algorithm}

One step of algorithm C-step is visualized on figure~\ref{figure:one:c:step}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%    C-STEP time complexity    %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{observation} 
    The time complexity of algorithm C-step~\ref{alg:Cstep} is asymptotically similar to the time complexity as OLS fit. Thus $O(p^2n)$.
\end{observation} 

\begin{proof}
    In C-step we must compute $n$ absolute residuals. Computation of one absolute residual consists of
    matrix multiplication of shapes $1 \times p$ and $p \times 1$ that gives us $\mathcal{O}(p)$. So time of computation $n$ residuals is $\mathcal{O}(np)$.
    Next, we must select a set of $h$ smallest residuals which can be done in $\mathcal{O}(n)$ using a modification of algorithm QuickSelect \cite{hoare1961algorithm}.
    Finally, we must compute $\hat{w}$ OLS estimate on a $h$ subset of data. Because $h$ is linearly dependent on $n$, we can say that it is $\mathcal{O}(p^2n + p^3)$ which is asymptotically dominant against previous steps which are $\mathcal{O}(np + n)$. Because we assume$n \ge p$ then $\mathcal{O}(p^2n + p^3) \sim \mathcal{O}(p^2n) $
\end{proof}

As we stated above, repeating C-step leads to sequence of $\what{1}, \what{2} \ldots$ 
on subsets $\vec{m_1}, \vec{m_2} \ldots$ with corresponding 
$L_1 \geq L_2\geq \ldots$. One could ask if this sequence converges, so that $L_i == L_{i+1}$. 
Answer to this question is presented by the following theorem.



%%%%%%%%       C-STEP  converges        %%%%%%%%%%%%%%%

\begin{theorem}
    Sequence of C-step converges to $\what{{k}}$ after maximum of $k = {n \choose h}$ so that
\begin{equation}
    L_k = L_i \,, \forall i \geq k.
\end{equation}
\end{theorem}

\begin{proof}
    Since  $\oflts(\what{{i}})$ is non-negative and $\oflts(\what{{i}}) \leq \oflts(\what{{i+i}})$ the sequence  converges. $\what{{i}}$  is computed out of subset 
    $\vec{m_i} \in Q^{(n, h)}$. Since $Q^{(n, h)}$ is finite,  namely its size is ${n \choose h}$, the sequence converges at the latest after this number of steps.
\end{proof}


%%%%%%%%       C-STEP  iteration        %%%%%%%%%%%%%%%%

The theorem gives us a clue to create algorithm described by the following pseudocode.

\begin{algorithm}[H]
    \label{alg:RepeatCstep}
    \KwIn{dataset consiting of $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ 
    and $\boldsymbol{y} \in \mathbb{R}^{n \times 1}$,  $\what{{old}} \in \mathbb{R}^{p \times 1}\,, \vec{m_0}$}
    \KwOut{ $\what{{final}}$, $\vec{m_{final}}$ }
    \caption{Repeat-C-step}
    \SetKw{Break}{break}
    $\what{{new}} \gets \emptyset$\;
    $\vec{m_{new}} \gets \emptyset$\;
    $L_{new} \gets \infty $\;

    \While{$True$}{
        $L_{old} \gets \oflts(\what{{old}})$\;
        $\what{{new}}$\,, $H_{new} \gets $ C-step$(\boldsymbol{X}\,, \boldsymbol{y}\,, \what{{old}})$\;
        $L_{new} \gets \oflts(\what{{new}})$\;
        \If{$L{old} == L{new}$}{
            \Break
          }
        $\what{{old}} \gets \what{{new}}$
    }

    \Return{ $\what{{new}}$, $\vec{m_{new}}$ }\;
\end{algorithm}

It is important to note, that although the maximum number of steps of this algorithm is ${n \choose h}$ in practice, it is shallow, most often under $20$ steps as can be seen on figure~\ref{figure:repeat:c:steps:cnt:converge}.

\begin{figure}[h]
\centering
\missingfigure{Statistic graph visualizing  number of steps required to convergence of algorithm }
\caption{todo caption}
\label{figure:repeat:c:steps:cnt:converge}
\end{figure}

That is not enough for the algorithm Repeat-C-step to converge to the global minimum. That gives us an idea of how to create the final algorithm. \cite{rouss:2000}

Choose a lot of initial subsets $\vec{m_1}$ and on each of them apply algorithm Repeat-C-step. From all converged subsets with corresponding $\vec{\hat{w}}$ estimates choose the one with the lowest value of $\oflts(\hat{w})$. 

Before we can construct final the algorithm, we must decide how to choose initial subset $\vec{m_1}$ and how many of them mean ``\emph{a lot}''. First, let us focus on how to choose an initial subset $\vec{m_1}$.



%%%%%%%%      INITIAL M_1 SUBSET      %%%%%%%%%%%%%

\subsection{Choosing initial $\vec{m_1}$subset}

It is important to note, that when we choose $\vec{m_1}$ subset such that it contains outliers, then iteration of  C-steps usually does not converge to good results, so we should focus on methods with non zero probability of selecting $\vec{m_1}$ such that it does not contain outliers.
There are many possibilities of how to create an initial $\vec{m_1}$ subset. Let us start with the most trivial one.


%%%%%%%%      RANDOM SELECTION         %%%%%%%%%%%%%
\subsubsection*{Random selection} \label{section:random:h:samples}
Most basic way of creating $\vec{m_1}$ subset is simply to choose random $\vec{m_1} \in Q^{(n, h)}$. The following observation shows that it is not the best way.

\begin{observation} \label{hrandomsamples}
    With an increasing number of data samples, thus with increasing $n$, the probability of choosing among $k$ random selections of $\vec{m_{1_1}}, \ldots ,\vec{m_{1_k}}$ the probability of selecting at least one $\vec{m_{1_i}}$ such that its corresponding data samples does not contains outliers, goes to $0$.
\end{observation}

\begin{proof}
    Consider dataset of $n$ containing $\epsilon > 0$ relative amount of outliers. Let $h$ be chosen conservatively so that $h = [(n/2] + [(p+1)/2]$ and $k$ is the number of selections random $h$ subsets. Then
    \begin{align*}
        P(one~random~data~sample~not~outliers) &= (1-\epsilon) \\
        P(one~subset~without~outliers) &= (1-\epsilon)^h \\
        P(one~subset~with~at~least~one~outlier) &= 1-(1-\epsilon)^h \\
        P(m~subsets~with~at~least~one~outlier~in~each) &= (1-(1-\epsilon)^h)^k \\
        P(m~subsets~with~at~least~one~subset~without~outliers) &= 1-(1-(1-\epsilon)^h)^k \\
    \end{align*}

    Because
$n \rightarrow \infty$, then    
$ (1-\epsilon)^h  \rightarrow 0 $, then   
$ 1- (1-\epsilon)^h  \rightarrow 1$, then
$ (1-(1-\epsilon)^h)^m  \rightarrow 1$, then 
$1- (1-(1-\epsilon)^h)^m  \rightarrow 0 $
\end{proof}

That means that we should consider other options for selecting $\vec{m_1}$ subset. If we would like to continue with selecting some random subsets, previous observation gives us a clue, that we should choose it independent of $n$. Authors of the algorithm came with such a solution, and it goes as follows.


%%%%%%%%%%%     P - SELECTION %%%%%%%%%%%

\subsubsection*{P-subset selection} \label{section:random:p:samples}

Let us choose vector $\vec{c} \in Q^{(n, p)}$. 
Next we compute rank of matrix $\vec{X}_{C} = \vec{C}\vec{X}$, where $C = diag(c)$. If $rank(\vec{X}_{C}) < p$ add randomly selected rows to $\vec{X}_{C}$ without repetition until $rank(\vec{X}_{C}) = p$. Let us from now on suppose that $rank(\vec{X}_{C}) = p$. Next let us mark $\what{0} = \oflts(c)$ and corresponding $r_1(\what{0}), r_2(\what{0}), \ldots ,r_n(\what{0})$ residuals.  Now mark $\hat{n} = \{{1,2,\ldots,n\}}$ and let
$\pi: \hat{n} \rightarrow \hat{n}$ be permutation of $\hat{n}$ such that $|r({\pi(1)})| \leq |r({\pi(2)})| \leq \ldots \leq |r({\pi(n)})|$. 
 Finally, mark $\vec{m_1} \in Q^{(n,h)}$  such that $m^1_i = 1$ for $i \in \{{\pi(1)\,, \pi(2)\,,... \pi(h)\}}$ and  $m^1_i = 0$  otherwise.

\begin{observation} \label{prandomsamples}
    With the increasing number of data samples, thus with increasing $n$, the probability of choosing among $k$ random selections of $\vec{c_{1_1}}, \ldots, \vec{c_{1_k}}$ the probability of selecting at least one $\vec{c_{1_i}}$ such that its corresponding data samples do not contains outliers, goes to
\begin{equation}
    1-(1-(1-\epsilon)^h)^k  > 0.
\end{equation}
\end{observation}

\begin{proof}
    Similarly as in previous observation.
\end{proof}

Last missing piece of the algorithm is determining the number of $k$ initial $\vec{m_1}$ subsets, which maximize the probability to at least one of them converges to correct solution. Simply put, the more, the better. So before we answer this question accurately, let us discuss some key observations about the algorithm.

%%%%%%%%%%%     SPEED-UP      %%%%%%%%%%%

\subsection{Speed-up of the algorithm}
In this section, we describe essential observations which help us to formulate the final algorithm. In two subsections we describe how to optimize the current algorithm. 

%%%%%%%%%%%     SELECTIVE ITERATION      %%%%%%%%%%%
\subsubsection*{Selective iteration}
The most computationally demanding part of one C-step is computation of OLS fit on $\vec{m_i}$ subset and then the calculation of $n$ absolute residuals. How we stated above, convergence is usually achieved under 20 steps. So for fast algorithm run, we would like to repeat C-step as little as possible and at the same time do not lose the performance of the algorithm. 

Due to that convergence of repeating C-step is very fast, it turns out, that we can distinguish between starts that leads to good solutions and those which does not, even after a small amount of the C-steps iterations. Based on empiric observation, we can distinguish good or bad solution already after two or three iterations of C-steps based on $\oflts(\what{3})$ or $oflts(\what{4})$ respectively. 

So even though authors do not specify the size of $k$ explicitly, they propose that after a few C-steps we can choose (say~10) best solutions among all $\vec{m_1}$ starting subsets and continue iteration of the C-steps till convergence only on those best solutions.
Authors refer to this process as to \defterm{selective iteration}.

\subsubsection*{Nested extension}
C-step computation is usually very fast for small $n$. Problem starts with very high $n$ say $n > 10^3$ because we need to compute OLS on $\vec{m_i}$ subset of size $h$ which is dependent on $n$. And then calculate $n$ absolute residuals.

Authors came up with a solution they call \defterm{nested extension}. We describe it briefly now.
\begin{itemize}
    \item If $n$ is greater than limit $l$, we create subset of data samples $L\,, |L| = l$ and divide this subset into $s$ disjunctive sets $P_1,P_2,\ldots,P_s\,, |P_i| = \frac{l}{s}\,, P_i\cap P_j  = \emptyset\,, \bigcup_{i=1}^{s} P_{i} = L$.
    \item For every $P_i$ we set number of starts $m_{P_i} = \frac{m}{l}$. 
    \item Next in every $P_i$ we create $m_{P_i}$ number of initial $H_{P_{i_1}}$ subsets and iterate C-steps for two iterations.
    \item Then we choose $10$ best results from each subsets and merge them together. We get family of sets
    $F_{merged}$ containing $10$ best $H_{P_{i_3}}$ subsets from each $P_i$.
    \item On each subset from  $F_{merged}$ family of subsets we again iterate $2$ C-steps and then choose $10$ best results.
    \item Finally we use these best $10$ subsets and use them to iterate C-steps till convergence.
    \item As a result we choose best of those $10$ converged results.
\end{itemize} 

%%%%%%%%    PUTTING ALL TOGETHER    %%%%%%%%%%%


\subsection{Putting all together}
We described all major parts of the algorithm FAST-LTS. One last thing we need to mention is that even though C-steps iteration usually converges under $20$ steps, it is appropriate to introduce two parameters 
$i_{max}$ and $t$ which limits the number of C-steps iterations in some rare cases when convergence is too slow. Parameter $i_{max}$ denotes the maximum number of iterations in final C-step iteration till convergence. Parameter $t$ denotes threshold stopping criterion such that $| \oflts(\what{{i}}) - \oflts(\what{{i+1}})| \leq t$ instead of 
$\oflts(\what{{i}}) = \oflts(\what{{i+1}})$ . When we put all together, we get \defterm{FAST-LTS} algorithm which is described by the following pseudocode.

\begin{algorithm}[H]
    \label{alg:FAST-LTS}
    \KwIn{$\boldsymbol{X} \in \mathbb{R}^{n \times p}, \boldsymbol{y} \in \mathbb{R}^{n \times 1}, m, l, s, i_{max}, t $}
    \KwOut{ $\vec{\hat{w}_{final}}$, $\vec{m_{final}}$ }
    \caption{FAST-LTS}
    \SetKw{Break}{break}
    $\what{{final}} \gets \emptyset$\;
    $\vec{m_{final}} \gets \emptyset$\;
    $F_{best} \gets \emptyset$\;

    \uIf{$n \geq l$}{
        $F_{merged} \gets \emptyset$\;
        % for each split
        \For{$i \gets 0$ \textbf{to} $s$}{
            % create xx number of starts
            $F_{selected}  \gets \emptyset$\;
            \For{$j \gets 0$ \textbf{to} $\frac{l}{s}$}{
              $F_{initial} \gets Selective~iteration(\frac{m}{l})$\;
              % on each starts iterate few c steps
              \For{$\vec{m_i}$ \textbf{in} $F_{initial}$}{
                $\vec{m_i} \gets Iterate~C~step~few~times(\vec{m_i})$\;
                $F_{selected} \gets  F_{selected} \cup \{{ \vec{m_i} \}} $\;
              }
            }
            % among all starts select 10 best and add it to merged set
            $F_{merged} \gets F_{merged} \cup Select~10~best~subsets~from~F_{selected}$\;
          }
        
          % for each, say 50 best, iterate few and add it to best
          \For{$\vec{m_i}$ \textbf{in} $F_{merged}$}{
            $\vec{m_i} \gets Iterate~C~step~few~times(\vec{m_i})$\;
            $F_{best} \gets F_{best} \cup \{{ \vec{m_i} \}} $\;
          }
          $F_{best} \gets  Select~10~best~subsets~from~F_{best} $\;
    }
    \Else{
        $F_{initial} \gets Selective~iteration(m)$\;
        $F_{best} \gets  Select~10~best~subsets~from~F_{initial} $\;
    }

    % iterate till convergence on few best final results
    $F_{final} \gets \emptyset$\;
    $W_{final} \gets \emptyset$\;
    \For{$\vec{m_i}$ \textbf{in} $F_{best}$}{
            $\vec{m_i}, \what{i} \gets Iterate~C~step~till~convergence(\vec{m_i}, i_{max}, t)$\;
            $F_{final} \gets F_{final} \cup \{{ \vec{m_i} \}}$\;
            $W_{final} \gets W_{final} \cup \{{ \what{i} \}}$\;
    }

    % select one best result
    $\what{{final}}, \vec{m_{final}} \gets select~\vec{\hat{w_i}}~and~\vec{m_{i}}~with~smallest~\oflts(\vec{\hat{w_i}})~from~F_{final}$\;

    \Return{ $\what{{final}}, \vec{m_{final}}$  }\;
\end{algorithm}







% 645 - 800
% ****************************************************************************************************
% ************************* FEASIBLE SOLUTION ******************************************************
% **************************************************************************************************
\section{Feasible solution} \label{section_feasible_solution}
In this section we'll introduce feasible solution algorithm from \cite{hawkins:1994}.
It is based on strong necessary condition we've described at \ref{strong_condition}. 
The basic idea can be described as follows.

Let's consider that we have some $\vec{m} \in Q^{(n,h)}$. It'll be convenient when we'll mark in the following text $O_m = \{{i \in  \set{ 1,2,\ldots , n } ;w_i = 1\}}$ and $Z_m = \{{j \in  \set{ 1,2,\ldots , n } ;w_j = 0\}}$ thus sets of indexes of positions where is $0$ respectively $1$ in vector $\vec{m}$. We can think about it as indexes of observations in $h$ set and $g$ set respectively. Then we can mark $\vec{m}^{(i,j)}$ as a vector which is constructed by swap of its ith and jth element where $i \in O$ and $j \in Z$. Such vector correspond to vector $\vec{m_{swap}}$ which we marked at \ref{strong_condition}.

With this in mind we can mark let's mark 
\begin{equation}
	\Delta S^{(\vec{m})}_{i,j} = \oflts(\vec{m}^{(i,j)}) - \oflts(\vec{m})
\end{equation}
thus change of the LTS objective function by swapping one observation from $h$ subset with another from $g$ subset. To calculate this we can obviously first calculate  $\oflts(\vec{m})$ and $ \oflts(\vec{m}^{(i,j)})$ and finally subtract both results. Although it is a option, it it computationally hard. So the question is if there is easier way of calculating $\Delta S^{(\vec{m})}_{i,j}$ and the answer is positive. 

Let's mark  
$M = diag(\vec{m}$ and 
$M^(i,j) = diag(\vec{m}^{(i,j)})$ and also
$\vec{H} = (\vec{X}^T\vec{M}^T\vec{X})$
For now let's assume that we already calculated
$\vec{H}^{-1}$ and also $\vec{\hat{w}} = \vec{H}^{-1}\vec{X}^T\vec{M}\vec{y}$
we now want to calculate $\Delta S^{(\vec{m})}_{i,j}$
Let's mark vector of residuals 
$\vec{r}^{(m)} = \vec{Y} - \vec{X} \vec{\hat{w}} $
and also $d_{r,s} = \vec{x_r} \vec{H}^{-1}  \vec{x_s} $
then by equation introduced in \cite{atkinson1991simulated} we get

\begin{equation} \label{hawkins:rovnice}
	\Delta S^{(\vec{m})}_{i,j} = 
	\frac{({r}^{(m)}_{j})^2(1-d_{i,i})- ({r}^{(m)}_{i})^2(1+d_{j,j}) + 2{r}^{(m)}_{i}{r}^{(m)}_{j}d_{j,j}}
	{(1-d_{i,i})(1+d_{j,j}) + d_{i,j}^2}.
\end{equation}

Let's now describe core of the algorithm. It's a  similar to the FAST-LTS algorithm in the sense of iterative refinement of $h$ subset. So let's assume that we have some vector  $\vec{m} \in Q^{(n,h)}$. No we'll compute  $\Delta S^{(\vec{m})}_{i,j}$ for all $i \in O$ and $j \in Z$. This may lead to several different outcomes

\begin{enumerate}
	\item all $S^{(\vec{m})}_{i,j}$ are non-negative
	\item one $S^{(\vec{m})}_{i,j}$ is positive
	\item multiple $S^{(\vec{m})}_{i,j}$ are positive
\end{enumerate}

In the first case, all $\oflts(\vec{m}^{(i,j)})$ are higher than $\oflts(\vec{m})$ so none swap will lead to an improvement. That also means that strong necessary condition is satisfied and the algorithm ends.

In the second and third case strong necessary condition is not satisfied and we make the swap. In second case it's easy which one to choose, because we have only one. in the third case we have couple of options again. 
\begin{enumerate}
	\item use the first swap that leads to the improvement (so don't event try to find different swap)
	\item from all possible swaps choose one that has highest improvement value  $\oflts(\vec{m}^{(i,j)})$
	\item use the first swap that has improvement higher than some given threshold
\end{enumerate}
In the terms of complexity all three options give the same results \todo{O(n2p) or O(n3p) remains}, because to find feasible solution you need to evaluate all pair swaps. In practice third options is winner because it'll lead to least amount of iterations. On the other hand as we said this won't improve complexity of the algorithm, so from now on let's assume that we'll use case number two.
So if there positive  $S^{(\vec{m})}_{i,j}$ we'll make the swap and repeat the process again. 
Algorithm ends when there is no possible improvement i.e. when all $S^{(\vec{m})}_{i,j}$ are non-negative.
Number of iterations needed till algorithm stops is usually quite low, but for practical usage it's still convenient to use some parameter $max\_iteration$ after which algorithm will stop without finding $h$ subset satisfying strong necessary condition. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\todo{put sem part of the pseudocode describing core iteration?}
\todo{and its time compleixty}
% \begin{observation} 
% 	Time complexity of algorithm feasible solution \ref{alg:Cstep} is the same as time complexity as OLS. Thus $O(p^2n)$
% 	\todo{create better proof. And take into account both versions - directly vs. using decomposition}
% \end{observation} 


% \begin{proof}
% 	In C-step we must compute $n$ absolute residuals. Computation of one absolute residual consists of
% 	matrix multiplication of shapes $1 \times p$ and $p \times 1$ that gives us $\mathcal{O}(p)$. Rest is in constant time.
% 	So time of computation $n$ residuals is $\mathcal{O}(np)$.
% 	Next we must select set of $h$ smallest residuals which can be done in $\mathcal{O}(n)$ using modification of algorithm QuickSelect. \todo{reference or define quick select} 
% 	Finally we must compute $\hat{w}$ OLS estimate on $h$ subset of data.
% 	Because $h$ is linearly dependent on $n$, we can say that it is $\mathcal{O}(p^2n + p^3)$ which 
% 	is asymptotically dominant against previous steps which are $\mathcal{O}(np + n)$.
% \end{proof}

% As we stated above, repeating algorithm C-step will lead to sequence of $\what{1}, \what{2} \ldots$ 
% on subsets $H_1, H_2 \ldots$ with corresponding residual sum of squares
% $RSS(\what{{1}}) \geq RSS(\what{{2}}) \geq \ldots$. One could ask if this sequence will converge, so that
% $RSS(\what{{i}}) == RSS(\what{{i+1}})$. 
% Answer to this question will be presented by the following theorem.


% %******************************** C-STEP alg. will converge ***********************%
% \begin{theorem}
% 	Sequence of C-step will converge to $\what{{m}}$ after maximum of $m = {n \choose h}$
% 	so that $RSS(\what{{m}}) == RSS(\what{{n}})\,, \forall n\geq m$ where $n$ is number of data samples 
% 	and $h$ is size of subset $H_i$.
% \end{theorem}

% \begin{proof}
% 	Since  $RSS(\what{{i}})$ is non-negative and $RSS(\what{{i}}) \leq RSS(\what{{i+i}})$ the 
% 	sequence will converge. $\what{{i}}$  is computed out of subset 
% 	$H_i \subset \{{1,2,\ldots,n\}}$. When there is finite number of subsets of size $h$ out of $n$ samples, namely ${n \choose h}$, the sequence will converge at the latest after this number of steps.
% \end{proof}


% \todo{REWRITE UP FROM THIS TO MATCH FEASIBLE SOLUTION qwerty}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
One run of this iteration process will lead to some local optima i.e. set satisfying strong necessary condition. In \cite{hawkins:1994} they refer to to this set as to \defterm{feasible set}.
This because it is not global optima the algorithm needs to be run multiple times say $N$ times. As a final solution is taken such $h$  subset with the smallest residual sum of squares. 

We didn't yet mention how to create initial $h$ subset respectively initial $\vec{m}$. We already had this discussion when describing FAST-LTS algorithm. The \cite{hawkins:1994} describes only random starting $h$ subsets, but using $p$ subsets instead may lead to improvement \todo{experiment with this and refer here}. More importantly as we already suggested $h$ subset satisfying weak necessary condition don't need to satisfy strong necessary condition so passing such a $h$ subset as input to this algorithm is another option and we'll discuss it in detail later. For that reasons we'll now describe feasible algorithm with pseudocode and we'll assume that we already have some function that generates for us $h$ subsets e.g. random one. 

\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\begin{algorithm}[H]
	\label{alg:feasible_solution}
		\KwIn{$\boldsymbol{X} \in \mathbb{R}^{n \times p}, \boldsymbol{y} \in \mathbb{R}^{n \times 1},  max\_iteration, N $}
		\KwOut{ $\vec{\hat{w}_{final}}$, $H_{final}$ }
		
	\caption{Feasible solution}
	\SetKw{Break}{break}
	$\vec{\hat{w}_{final}} \gets \emptyset$\;
	$H_{final} \gets \emptyset$\;
	$Results \gets \emptyset$\;

	\For{$k \gets 0$ \textbf{to} $N$}{
		$\vec{m} \gets generate\_intial\_subset()$  \tcp*{e.g. random $\vec{m} \in Q^{(n,h)}$}
		\While{$True$}{ 
			$best_i \gets 0$ \;
			$best_j \gets 0$ \;
			$best_{delta} \gets 0$ \;
			\For{$ i \in O_m$}{ 
				\For{$j \in Z_m$}{ 
					$delta \gets calculate \Delta S^{(\vec{m})}_{i,j}$\;
					\If{$delta >  best_{delta}$}{
						$best_{delta} \gets delta$\;
						$best_i \gets i$ \;
						$best_j \gets j$ \;
					}
				}
			}
			\If{$best_{delta} > 0$}{
				$\vec{m} \gets \vec{m}^{(i,j)}$\;
			}
			\Else {
				$H \gets h\ subset\ corresponding\ to\ \vec{m}$\;
				$\vec{M} \gets diag(\vec{m})$\;
				${\hat{w}} \gets \of^{(OLS,\vec{M}\vec{X},  \vec{M}\vec{y} )}$\;
				$Results \gets Results \cup \{ H, {\hat{w}} \}$\;
				\Break\;
			}
		}		
	}
	$H_{final}, \vec{\hat{w}_{final}}  \gets $ select best from $Results$ based on smallest $RSS$\;
	\Return{ $\what{{final}}$\,, $H_{final}$ }\;
\end{algorithm}

\todo{time complexity}

\section{Combined algorithms}
\todo{ todo this section and move it somewhere else qwerty}
Here we'll describe what we've indicated above and that is combination of both previous algorithms.
Two options. 
z
\begin{enumerate}
	\item Let fast-lts converge and then run FSA and let it converge
	\item Let fast-lts converge make one step of FSA and try fast-LTS again and iterate between this
\end{enumerate}

Second option would be faster ? Let's think about it.. etc etc..
























%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%% IMPROVED FSA %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Improved FSA} % two exchanging algorithms from Agulló, 2000
So far as we've described FSA algorithm we assumed that after each cycle of the algorithm (after each best swap) wee need to recalculate inversion $(\vec{X}^T\vec{X})$ together with $\vec{\hat{w}}$. In this section we'll introduce different approaches described in \cite{agullo2001new} which will improve it so that we'll be able to update it instead of recalculate. Moreover these ideas will lead us to bounding condition of FSA which will improve the speed and we'll also be able to construct another algorithms based on this idea. 

We've introduced additive formula \ref{hawkins:rovnice}. Let's now try to obtain similar formula but let's try to focus on how individual elements in our current algorithm will change not only on the swap of two rows but how those elements will be affected after adding one row and also removing one row. Moreover during this derivation we'll be able to obtain two different approaches of calculating one thing. Both are important and we'll recapitulate them after.


Let $\vec{A} = (\vec{X}, \vec{y})$ be a matrix $\vec{X}$ expended by one column of corresponding dependent variables and $\vec{Z} = \vec{X}^T\vec{X}$ and $\vec{\tilde{Z}} = \vec{A}^T\vec{A}$. Then

\[  \numberthis \label{matrixZ}
	\vec{\tilde{Z}} = \begin{bmatrix}
		\vec{Z} & \vec{X}^T\vec{y} \\
    \vec{y}^T\vec{X} & \vec{y}^T\vec{y}
  \end{bmatrix} .
\]
Notice that $Z$ is symmetric square matrix $\in \mathbb{R}^ {p \times p}$ moreover we suppose that $Z$ is regular. $\vec{X}^T\vec{y}$ is column vector $\in \mathbb{R}^ {1 \times p}$, $\vec{y}^T\vec{X}$ is vector $\in \mathbb{R}^ {p \times 1}$ and $\vec{y}^T\vec{y}$ is scalar.
OLS estimate $\vec{\hat{w}}$ is then
\begin{equation}
	\vec{\hat{w}^{(OLS, n)}} = \vec{Z}^{-1} \vec{X}^T\vec{y}
\end{equation}
and residual sum of squares 

\begin{equation}
	RSS = \vec{y}^T\vec{y} - \vec{y}^T\vec{X}\vec{\hat{w}}
\end{equation}

Also note that all necessary multiplications are already in the matrix $\vec{\tilde{Z}}$.
Let's now realize that $RSS$ can  be expressed as ratio of determinants $\det(\vec{\tilde{Z}})$ and $\det(\vec{Z})$ (using determinant rule for block matrices) so that 

\begin{align*} \numberthis \label{rssdeterminant} 
	RSS &= \frac{\det(\vec{\tilde{Z}})}{\det(\vec{Z})} \\ &= \frac{
    \det\begin{pmatrix}
			\vec{Z} & \vec{X}^T\vec{y} \\
			\vec{y}^T\vec{X} & \vec{y}^T\vec{y}
		\end{pmatrix}
		}{\det\left(\vec{M}\right)}\\
		 &=\frac{ \det(\vec{Z}) \det\left(\vec{y}^T\vec{y} -  \vec{y}^T\vec{X} \vec{Z}^{-1} \vec{y}\right) }
		{\det\left(\vec{Z}\right)} \\ 
		&= \vec{y}^T\vec{y} - \vec{y}^T\vec{X}\vec{\hat{w}}.
\end{align*}

If we assume $RSS > 0$ then $\vec{\tilde{Z}}^{-1}$ can be expressed as 

\[ \numberthis
	\vec{\tilde{Z}}^{-1} = 
	\begin{bmatrix}
		\vec{Z}^{-1}+\dfrac{\vec{\hat{w}} \vec{\hat{w}}^{T}}{RSS} & - \dfrac{\vec{\hat{w}}^T}{RSS} \\[6pt]
		- \dfrac{\vec{\hat{w}}}{RSS} & \dfrac{1}{RSS}
	\end{bmatrix}.
\]
For following equations it's important to notice that for any two observations $z_i = (x_i, y_i)$  and $\vec{z_j} = (\vec{x_j}, y_j)\, \vec{z_i}, z_i \in \mathbb{R}^{p+1 \times 1}$ is 

\begin{equation}
	\vec{z_i} \vec{\tilde{Z}}^{-1} \vec{z_i^T} = \dfrac{ ( y_i - \vec{x_i}\vec{\hat{w}} )^2 }{RSS}  + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}
\end{equation}
and
\begin{equation}
	\vec{z_j} \vec{\tilde{Z}}^{-1} \vec{z^T} = \dfrac{ ( y_j - \vec{x_j}\vec{\hat{w}} ) ( y_i - \vec{x_i}\vec{\hat{w}} ) }{RSS}  + \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}
\end{equation}
Using above let's express how the determinant $\det(\vec{Z})$ and inverse of $vec{Z}^{-1}$ will change when we'll add one observation $\vec{z_i} = (\vec{x_i}, y_i) \in \mathbb{R}^{p+1 \times 1} $ to the matrix $\vec{A}$. First lets notice that if we add this row to $\vec{A}$, then  $\vec{Z}$ will change 

\[ \numberthis \label{addedrow}
\begin{bmatrix}
	x_{11} & x_{12} & \dots  & x_{1n} & x_{1i}  \\
	x_{21} & x_{22} & \dots  & x_{2n} & x_{2i} \\
	\vdots & \vdots & \vdots & \ddots & \vdots \\
	x_{p1} & x_{p2} & \dots  & x_{pn} & x_{pi} 	
\end{bmatrix}
\begin{bmatrix}
	x_{11} & x_{12}  & \dots  & x_{1p} \\
	x_{21} & x_{22}  & \dots  & x_{2p} \\
	\vdots  & \vdots & \ddots & \vdots \\
	x_{n1} & x_{n2}  & \dots  & x_{np} \\
	x_{i1} & x_{i2}  & \dots  & x_{ip}
\end{bmatrix}
 = \vec{X}^T\vec{X} + \vec{x_i^T}\vec{x_i} = \vec{Z} + \vec{x_i^T}\vec{x_i},
\]
so determinant with appended row will be
\begin{equation} \label{udpateddeterminant}
	\det(\vec{Z} + \vec{x_i^T}\vec{x_i}) = det(\vec{Z})(1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T})
\end{equation}
and inversion can be obtained using Sherman-Morrison formula \cite{bartlett1951inverse} so that 
\begin{equation} \label{shermanmorris}
	(\vec{Z} + \vec{x_i^T}\vec{x_i})^{-1} = \vec{Z}^{-1} - \dfrac{\vec{Z}^{-1}\vec{x_i^T}\vec{x_i}\vec{Z}^{-1}}{1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}}
\end{equation}
For following equations it'll be convenient for us to mark 
\begin{equation}
	b = \dfrac{-1}{(1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T})},  b \in \mathbb{R},
\end{equation}
and 
\begin{equation} \label{agullo_u}
	\vec{u} = \vec{Z}^{-1}\vec{x_i^T},  	\vec{u} \in \mathbb{R}^{p \times 1},
\end{equation}
so that \ref{shermanmorris} can be written as
\begin{equation} \label{inversionplus}
	(\vec{Z} + \vec{x_i^T}\vec{x_i})^{-1} = \vec{Z}^{-1} + b\vec{u}\vec{u^T}.
\end{equation}
Given that last piece that we're missing to express updated $\vec{\hat{w}}$ is appended $\vec{X}^T\vec{y}$ by one row, which can be simply expressed by same idea as \ref{addedrow} so that updated $\vec{\hat{w}}$ which we'll denote as $\vec{\overline{\hat{w}}}$ is 
\begin{equation}
	\vec{\overline{\hat{w}}} = (\vec{Z}^{-1} + b\vec{u}\vec{u^T})(\vec{X^T}\vec{y} + y_i\vec{x_i^T}).
\end{equation}
This can be simplified so that we get
\todo{mistake in original paper where is w + (y - xw)bu}
\begin{equation} \label{thetaplus}
	\vec{\overline{\hat{w}}} = \vec{\hat{w}} - (y_i - \vec{x_i}\vec{\hat{w}})b\vec{u}.
\end{equation}
Last but not least we want to express updated $RSS$ which we denote as $\overline{RSS}$. This can be done easily from \ref{rssdeterminant} and \ref{udpateddeterminant} as 

\begin{equation}
	\overline{RSS} =  RSS + \dfrac{(y_i - \vec{x_i}\vec{\hat{w})^2}}{(1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T})}
\end{equation}
and again, it's convenient to mark 

\begin{equation}
	\gamma^{+}(\vec{z_i}) = \dfrac{(y_i - \vec{x_i}\vec{\hat{w})^2}}{(1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T})},
\end{equation}
so that 
\begin{equation}
	\overline{RSS} =  RSS + \gamma^{+}(\vec{z_i})
\end{equation}
We can see that $\gamma^{+}(\vec{z_i})$ measures how $RSS$ increase after we append our dataset with observation $\vec{z_i}$, thus we can see that $\gamma^{+}(\vec{z_i}) \geq 0$

Because we want to express both increment and decrement change in our dataset, let's now focus how $RSS$, $\vec{Z^{-1}}$ and $\vec{\hat{w}}$ will change after we exclude one observation. 

Consider that we've already included one observation $z_i$ in our dataset and mark $\vec{\overline{Z}} = \vec{Z} + \vec{x_i} \vec{x_i^T} $ If we exclude one observation $\vec{z_j} = (\vec{x_j}, y_j) \in \mathbb{R}^{p+1 \times 1} $ from already updated matrix $\vec{A}$ then determinant $\det(\vec{\overline{Z}})$  will change as

\begin{equation} 
	\det(\vec{\overline{Z}} - \vec{x_j^T}\vec{x_j}) = det(\vec{\overline{Z}})(1 - \vec{x_j}\vec{\overline{Z}}^{-1}\vec{x_j^T})
\end{equation}
and inversion will change (again according to Sherman-Morrison formula) as 
\begin{equation}  \label{updatedinversion2}
	(\vec{\overline{Z}} - \vec{x_j^T}\vec{x_j})^{-1} = \vec{\overline{Z}}^{-1} + \dfrac{\vec{\overline{Z}}^{-1}\vec{x_j^T}\vec{x_j}\vec{\overline{Z}}^{-1}}{1 - \vec{x_j}\vec{\overline{Z}}^{-1}\vec{x_j^T}}.
\end{equation}
Once again, it's convenient to denote
\begin{equation}
	\overline{b} = \dfrac{-1}{(1  \vec{x_j}\vec{\overline{Z}}^{-1}\vec{x_j^T})},  \overline{v} \in \mathbb{R},
\end{equation}
and 
\begin{equation}
	\vec{\overline{u}} = \vec{\overline{Z}}^{-1}\vec{x_j^T},  	\vec{\overline{u}} \in \mathbb{R}^{p \times 1},
\end{equation}
so that we can write
\begin{equation}
	(\vec{\overline{Z}} + \vec{x_j^T}\vec{x_j})^{-1} = \vec{Z}^{-1} - \overline{b}\vec{\overline{u}}\vec{\overline{u}^T}.
\end{equation}
Using same approach as before we can denote express downdated estimate which we denote as $\vec{\overline{\overline{\hat{w}}}}$

\begin{equation} \label{thetaminus}
	\vec{\overline{\overline{\hat{w}}}} =  +\vec{\overline{\hat{w}}} (y_j - \vec{x_j}\vec{\overline{\hat{w}}}) \overline{b} \vec{\overline{u}}.
\end{equation}

Finally lets also express updated $\overline{RSS}$ which we denote as $\overline{\overline{RSS}}$. This can be done easily from \ref{rssdeterminant} and \ref{udpateddeterminant} and \ref{updatedinversion2} as 

\begin{equation}
	\overline{\overline{RSS}} =  \overline{RSS} - \dfrac{(y_j - \vec{x_j}\vec{\overline{\hat{w}})^2}}{(1 - \vec{x_j}\vec{\overline{{Z}}}^{-1}\vec{x_j^T})}
\end{equation}
and let's also mark

\begin{equation}
	\gamma^{-}(\vec{z_j}) = \dfrac{(y_j - \vec{x_j}\vec{\overline{\hat{w}})^2}}{(1 - \vec{x_j}\vec{\overline{{Z}}}^{-1}\vec{x_j^T})},
\end{equation}
so that 
\begin{equation}
	\overline{\overline{RSS}} =  \overline{RSS} - \gamma^{-}(\vec{z_j})
\end{equation}

Lets now express equation for including and removing observation at once. First let's notice that from \ref{updatedinversion2} we can express 

\begin{equation} \label{xjoverlinez}
	\vec{x_j}\vec{\overline{{Z}}}^{-1}\vec{x_j^T}) =  \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}) - 
	\dfrac{( \vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2}{1 +  \vec{x_i}\vec{Z}^{-1}\vec{x_i^T})},
\end{equation}
 so that 

 \begin{gather}
 \begin{align*} \numberthis
	&\det(\vec{Z} + \vec{x_i^T}\vec{x_i} - \vec{x_j^T}\vec{x_j}) = \\
	\det(\vec{Z})(1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T} -& \vec{x_j}\vec{Z}^{-1}\vec{x_j^T} +  ( \vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2 - \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}\vec{x_j}\vec{Z}^{-1}\vec{x_j^T} )
\end{align*}
\end{gather}

Finally we can express $\overline{\overline{RSS}} $ as

\begin{equation} \label{updaterss}
	\overline{\overline{RSS}}  = RSS\rho(z_i, z_j),
\end{equation}
where
\begin{equation} \label{agullo:rovnice}
	\rho(z_i, z_j) =
	 \dfrac
	 {(1+\vec{x_i}\vec{Z}^{-1}\vec{x_i^T} + \dfrac{e_j^2}{RSS})
		(1 - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T} - \dfrac{e_i^2}{RSS} )+
		(\vec{x_i}\vec{Z}^{-1}\vec{x_j^T} + \dfrac{e_i e_j}{RSS} )^2}
	{1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}  - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}  + ( \vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2 -   \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}\vec{x_j}\vec{Z}^{-1}\vec{x_j^T} },
\end{equation}
where $e_i = y_i - x_i\vec{\hat{w}}$ and $e_j = y_j - x_j\vec{\hat{w}}$.

We can see that this formula is similar to the one \ref{hawkins:rovnice} but here the $\rho(z_i, z_j)$ represents multiplicative increment. Moreover $0 < \rho(z_i, z_j) $ and if $0 < \rho(z_i, z_j)< 1 $ then the swap leads to improvement in terms of feasible solution.

With all this we are now able to change FSA so that we don't need to recompute $\vec{\hat{w}}$ and inversion $(\vec{X}^T \vec{X})^{-1}$ but we can update it. That means after we find $z_i, z_j$ that improve the current RSS the most (that means we find smallest $\rho(z_i, z_j)$ by \ref{agullo:rovnice}). We can then update $RSS$ by \ref{updaterss}, update $(\vec{X}^T \vec{X})^{-1}$ by \ref{inversionplus} and \ref{updatedinversion2} and finally update $\vec{\hat{w}}$ by \ref{thetaplus} and \ref{thetaminus}.

\todo{pseudokod}


Let's now talk about time complexity of such solution. We can clearly see that core iteration takes the most time. So let's first describe time complexity of finding optimal swap and updating result with such pair. 
We need to go through all pairs between $z_i, z_j$ where $z_i$ is from $h$-subset and $z_j$ is from $g$-subset. That is 
\begin{equation}
	h(n-h) \approx (\frac{n^2 - p^2}{4})^2 \sim \mathcal{O}(n^2 - p^2)
\end{equation}
For each of this pair we need to calculate \ref{hawkins:rovnice}. Note that $e_i$ and $e_j$ can be calculated before this loop. The same goes for $\vec{x_i}\vec{Z}^{-1}\vec{x_i^T}$ and $\vec{x_j}\vec{Z}^{-1}\vec{x_j^T} $. So inside the loop there is only one vector-matrix-vector multiplication that is $\vec{x_i}\vec{Z}^{-1}\vec{x_j^T}$ which is $\mathcal{p^2}$. Rest are scalars. The whole loop is then 
\begin{equation}
	\mathcal{O}( (n^2 - p^2) p^2) 
\end{equation}
Before we'll move to the updating we must not to forget for quantities which we said we can calculate in advance. That is $\\mathcal{O}(2np^2)$ for all $\vec{x_i}\vec{Z}^{-1}\vec{x_i^T}$ and $\vec{x_j}\vec{Z}^{-1}\vec{x_j^T}$ and $\mathcal{O}(2np)$ for all $e_j$ and $e_j$.

Finally we need to update $RSS$ which is trivial. Next inversion by \ref{inversionplus} which is $\mathcal{O}(4p^2)$ and \ref{updatedinversion2} which is also $\mathcal{O}(4p^2)$. Finally we need to update $\vec{\hat{w}}$ by \ref{thetaplus} and \ref{thetaminus} which are both $\mathcal{O}(p^2 +p)$. Putting everything together we get

\begin{equation}
	\mathcal{O}( (n^2 - p^2) p^2 + 2np^2 + 2np + 8p^2 + 2p^2 + p) \sim \mathcal{O}(n^2p^2 - p^4) 
\end{equation}

\todo{nejak zminit kolikrat ten loop vetsinou projede atd.. }

Note that right now it doesn't matter if we use \ref{hawkins:rovnice} with stopping criterion $	\Delta S^{(\vec{m})}_{i,j} \geq 0$  or \ref{agullo:rovnice} and use $\rho(z_i, z_j) \geq 1 $ stopping criterion. With both results we can update $RSS$. But the advantage of \ref{agullo:rovnice} is that we can use following bounding condition to improve performance.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% BOUNDING CONDITION SPEEDUP %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Bounding condition improvement}
$\rho(z_i, z_j)$ is expressed as ratio. We can see that in numerator we have

\begin{equation}
	(1+\vec{x_i}\vec{Z}^{-1}\vec{x_i^T} + \dfrac{e_j^2}{RSS})
		(1 - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T} - \dfrac{e_i^2}{RSS} )+
		(\vec{x_i}\vec{Z}^{-1}\vec{x_j^T} + \dfrac{e_i e_j}{RSS} )^2
\end{equation}

and because $ \dfrac{e_i e_j}{RSS} )^2 \geq 0$ then whole numerator is greater or equal to

\begin{equation}
	(1+\vec{x_i}\vec{Z}^{-1}\vec{x_i^T} + \dfrac{e_j^2}{RSS})
(1 - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T} - \dfrac{e_i^2}{RSS} ).
\end{equation}

On the other hand we can see that denominator is

\begin{equation}
	1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}  - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}  + (\vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2 -   \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}\vec{x_j}\vec{Z}^{-1}\vec{x_j^T} 
\end{equation}

and because $(\vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2$ and $\vec{x_i}\vec{Z}^{-1}\vec{x_i^T}\vec{x_j}\vec{Z}^{-1}\vec{x_j^T} $ are actually inner products of $x_i$ and $x_j$  ($\vec{Z}^{-1}$ is positive definite) so 
\begin{equation}
	(\vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2 \leq \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}\vec{x_j}
\end{equation}
using Cauchy-Schwarz inequality. That means that denominator is less or equal to
\begin{equation}
	1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}  - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}.
\end{equation}
Given that we can denote $\rho_b(z_i, z_j)$ so that


\begin{equation} \label{boundingcondition}
	\rho_b(z_i, z_j) = \dfrac{(1+\vec{x_i}\vec{Z}^{-1}\vec{x_i^T} + \dfrac{e_j^2}{RSS})
	(1 - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T} - \dfrac{e_i^2}{RSS} )}{1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}  - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}} \quad \leq \rho(z_i, z_j)
\end{equation}

So the actual speed improvement is given by we don't need to compute $\vec{x_i}\vec{Z}^{-1}\vec{x_j^T}$ in each of $h(n-h)$ pairs swap comparison. Every time that algorithm starts to compare all pairs we set $\rho_{min} :=1$. Then every time we only compute $\rho_b(z_i, z_j)$ and if it is greater of equal to $\rho_{min}$ we can continue to next pair without computing $\rho(z_i, z_j)$. On the other hand if $\rho_b(z_i, z_j)$ is less that  $\rho_{min}$ we compute $\rho(z_i, z_j)$ and set $\rho_{min} := \rho(z_i, z_j)$ This of course won't improve $\mathcal{n^2}$ time required to compare all $z_i, z_j$ pairs.
Finally lets note that \cite{agullo2001new} call FSA  which compute $\rho(z_i, z_j)$  \ref{agullo:rovnice} as \defterm{Optimal exchange algorithm (OEA)} and OEA using bounding condition \ref{boundingcondition} as  \defterm{Modified optimal exchange algorithm (MOEA)}.


\todo{write the pseudocode}


\subsubsection*{Different method of computing Improved FSA}
\todo{zmenit to oznaceni Improved FSA na neco jineho. myslim ze to pouzivam nekonzistentne}

We've described way how to modify FSA so that we can update $\vec{hat{w}}$ and inversion $(\vec{X}^T \vec{X})^{-1}$. This requires to compute inversion at the start of the algorithm (this is also the case for the FSA). In practice, however, this is not usually the way how OLS estimate $\vec{\hat{w}}$ is compute. That is primarily due to low numerical stability. In practice we usually use QR decomposition. In this subsection we describe how we can modify FSA to use QR decomposition. Let us start by describing Givens rotations algorithm in detail, because it is critical part of the modified algorithm. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%    H E R E   W E R E   O L S   M E T H O D S %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% QR DECOMPOSITION - GIVENS ROTATION  %%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Givens Rotation} \label{givensrotation}
In this section, we describe Givens rotations in detail. Moreover, we also show how to update QR decomposition in terms of including and excluding row from factorized matrix $\vec{X}$ which help us in our algorithm.

We compute the QR factorization of $\vec{A} \in \mathbb{R}^{m \times n}$ which has full column rank so that we apply orthogonal transformation by a matrix $\vec{Q}^T$ so that \cite{hammarling2008updatingqr}

\begin{equation}
    \vec{Q}^T\vec{A} = \vec{R}
\end{equation}
 
where $\vec{Q}$ is product of orthogonal matrices. One such example of orthogonal matrix can be:

\begin{equation} \label{givensmatrix}
\vec{Q} = 
\begin{bmatrix} 
\cos(\varphi) & \sin(\varphi) \\
- \sin(\varphi) & \cos(\varphi)
\end{bmatrix}
\end{equation}
This matrix is indeed orthogonal since 
\begin{align*} 
\vec{Q}\vec{Q}^T = \vec{Q}^T\vec{Q}    
\end{align*}
 \begin{equation*}
    = \begin{bmatrix} 
        \cos(\varphi)^2 + \sin(\varphi)^2  & \sin(\varphi)\cos(\varphi) - \cos(\varphi)\sin(\varphi) \\
        \cos(\varphi)\sin(\varphi) - \sin(\varphi)\cos(\varphi) & \cos(\varphi)^2 + \sin(\varphi)^2
    \end{bmatrix} \\ = \vec{I}
    \numberthis
\end{equation*}
Moreover if we multiply this orthogonal with some column vector $\vec{x} \in \mathbb{R}^{2 \times 1}$ thus $\vec{Q}\vec{x}$ as a result we get vector of same length but is rotated clockwise by  $\varphi$ radians. When we say that vector have same length we mean than L-2 norm of such vector stays the same.This is important property of all orthogonal matrices. We can simply verify correctness of this claim. Let us have any orthogonal matrix $\vec{Q} \in \mathbb{R}^{m \times m}$ and any column vector $\vec{x} \in  \mathbb{R}^{m \times 1}$ then

\begin{equation}
    \norm{\vec{Q}\vec{x}}^2 = (\vec{Q}\vec{x})^T (\vec{Q}\vec{x}) = \vec{x}^T\vec{Q}^T\vec{Q}\vec{x} = 
    \vec{x}^T\vec{I}\vec{x} = \vec{x}^T\vec{x} = \norm{\vec{x}}^2
\end{equation}

So we would like to create a series of orthogonal matrices which gradually rotates column vectors of $\vec{A}$, so we obtain zeros under diagonal. One such method - \defterm{Givens rotation} uses \defterm{Givens matricies} that zero one element under diagonal at a time.
The example of \ref{givensmatrix} is not random. Let us look at this orthogonal matrix one more time and let us multiply this matrix with some vector.

\begin{equation}
    \begin{bmatrix} 
        \cos(\varphi) & \sin(\varphi) \\
        - \sin(\varphi) & \cos(\varphi)
        \end{bmatrix}
        \begin{bmatrix} 
            a \\
            b
            \end{bmatrix} = 
            \begin{bmatrix} 
                r \\
                z
                \end{bmatrix}
\end{equation}

To introduce this zeroing effect, we need to rotate this vector so that it is parallel to $
\begin{bmatrix} 
    1 \\
    0
\end{bmatrix}
$. Now it is only up to find  $\cos(\varphi) a + \sin(\varphi) = r $.
As we know rotation with $\vec{Q}$ preserve L-2 norm. That means if we want to $z = 0$  then $r$ must be equal to L-2 norm of the vector  
$
\begin{bmatrix} 
    a \\
    b
\end{bmatrix}
$  thus $r = \sqrt{a^2 + b^2}$. 
Then the solution seems trivial. We do not even need to calculate $\varphi$ because if we put
\begin{equation} \label{givens_cos}
    \cos(\varphi) = \frac{a}{\sqrt{a^2 + b^2}} 
\end{equation}
and   
\begin{equation} \label{givens_sin}
    \sin(\varphi) = \frac{b}{\sqrt{a^2 + b^2}} 
\end{equation}
then 
\begin{align}
    r &= \frac{a^2}{\sqrt{a^2 + b^2}} + \frac{b^2}{\sqrt{a^2 + b^2}} = \sqrt{a^2 + b^2} \\
    z &= \frac{-ab}{\sqrt{a^2 + b^2}} +  \frac{ab}{\sqrt{a^2 + b^2}} = 0.
\end{align}

Practically used algorithm of computing $\cos{\varphi}$ and  $\sin{\varphi}$ is slightly different because we want to prevent overflow. This algorithm is described by the following pseudocode:


\begin{algorithm}[H]
    \label{alg:givens}
    % \SetKwInOut{Input}{input}
    % \SetKwInOut{Output}{output}
    \KwIn{$a, b$}
    \KwOut{$cos, sin$ }
    \caption{Rotate}
    $sin \gets \emptyset$\;
    $cos \gets \emptyset$\;
    \uIf{$b == 0$}{
        $sin \gets 0$\;
        $cos \gets 1$\;
    }
    \uElseIf{$abs(b) \geq abs(a)$}{
        $cotg \gets \frac{a}{b}$\;
        $sin \gets \frac{1}{\sqrt{1 + (cotg)^2}}$\;
        $cos \gets sin\ cotg$\;
    }
    \Else{
        $tan \gets \frac{b}{b} $\;
        $cos \gets \frac{1}{\sqrt{1 + (tan)^2}}$\;
        $sin \gets cos\ tan$\;
    }
    \Return{ $cos$\,, $sin$ }\;
\end{algorithm}

We can scale this same idea to higher dimensions. Let us denote matrix $\vec{Q}(i, j) \in \mathbb{R}^{m \times m}$ defined as 
\renewcommand{\kbldelim}{[}% Left delimiter
\renewcommand{\kbrdelim}{]}% Right delimiter
\[
  \vec{Q}(i, j) = \kbordermatrix{
    &   &       & i &       & j &      &  \\
    & 1 & \dots & 0 & \dots & 0 &\dots & 0 \\
    & \vdots & \ddots & \vdots & & \vdots & & \vdots \\
   i  & 0 & \dots & c & \dots & s & \dots & 0 \\
     & \vdots &  & \vdots & \ddots & \vdots & & \vdots \\
     j  & 0 & \dots & -s & \dots & c & \dots & 0 \\
     & \vdots &  & \vdots &  & \vdots & \ddots & \vdots \\
     & 0 & \dots & 0 & \dots & 0 &\dots & 1 \\
  }
\]

where $c =     \cos(\varphi) $ and $ s = \sin(\varphi)$ for some $\varphi$. That means this matrix is orthogonal. Now if we have some column vector $\vec{x} \in \mathbb{R}^{m \times 1}$ and calculate $c$ and $s$ by means of \ref{givens_cos} and \ref{givens_sin} for $a := x_i $ and $b := x_j$. 
Finally if we multiply $\vec{Q}(i, j) \vec{x} = \vec{p}$ we can see that $\vec{p}$ is same as vector $\vec{x}$ except $p_i$ and $p_j$ so that:

\[
    \vec{Q}(i, j) \vec{x} = \vec{Q}(i,j) 
    \kbordermatrix{
    &   \\
    & x_1 \\
    & \vdots \\
   i  & x_i  \\
     & \vdots \\
     j  & x_j  \\
     & \vdots \\
     & x_m \\
  }
    =  \vec{p} = \kbordermatrix{
    &   \\
    & x_1 \\
    & \vdots \\
   i  & r  \\
     & \vdots \\
     j  & 0  \\
     & \vdots \\
     & x_m \\
  }
\]

where $r = \sqrt{x_i^2 + x_j^2}$. If we have matrix $\vec{A} \in \mathbb{R}^{n \times p}$ instead of only one column $\vec{x}$ it works the same way. So if we have such matrix we need to create matrix $    \vec{Q}(i, j)$ for each $a_{ij}$ under the diagonal in order to create upper triangular matrix. Usually we are zeroing columns so that we start with $a_{12}$ then $a_{13} \ldots a_{1_n}$.
Then we start with second column $a_{23}$ and so on. That means we need to create in total exactly
$e = \frac{p^2 - p}{2} + np - p^2$ matrices $\vec{Q}(i, j)$. We can denote this sequence of matrices as $\vec{Q_1},  \vec{Q_2}, \ldots \vec{Q_e}$.
The QR decomposition then looks like

\begin{equation}
    \vec{Q_e}\ldots\vec{Q_2}\vec{Q_1}\vec{A} = \vec{R}
\end{equation}
where $\vec{Q_e}\ldots\vec{Q_2}\vec{Q_1}$ is actually $\vec{Q}^T$. So $\vec{Q}$ is obtained by 

\begin{equation}
 \vec{Q} = \vec{Q_1^T}\vec{Q_2^T}\ldots\vec{Q_e^T}.
\end{equation}

We can see that for each row of the matrix we need one additional matrix $\vec{Q_i}$ and with such matrix, we are multiplying only two rows. Moreover, the rows are getting shorter after each finished column. So the time we see that time complexity is equal to
\todo{todo write the pseudocode}
\begin{equation}
    \sum\limits_{i=1}^n  \sum\limits_{j=i+1}^p 6(n-i+1) \approx 6np^2 - 3np^2 - 3p^3 + 2p^3 = 3np^2 - p^3
\end{equation}
\todo{move all this about rotations somewhere else}

So we are now in a situation when we have QR decomposition of $\vec{X}$, and we need to exchange $i$th row for $j$th row. We can simulate this by first adding $j$th row and consequently removing $i$th row.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%    QR INSERT     %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Updating QR decomposition}
First, let us discuss how to update QR decomposition when a row is added. 
If we add a row to $\vec{A}$ our decomposition looks like
\begin{equation}
    \vec{R^{+}} = \vec{Q_e}\ldots\vec{Q_2}\vec{Q_1}\vec{A_{new}}  = 
    \begin{bmatrix}
        \times & \cdots & \cdots & \cdots & \times \\
        0 &\times & \cdots & \cdots & \times \\
        \vdots& 0&\ddots & \cdots & \vdots \\
        \vdots& \vdots&0 & \ddots &  \vdots \\
        \vdots& \vdots& \vdots& 0& \times \\
        \vdots& \vdots& \vdots& \vdots& 0  \\
        \vdots& \vdots& \vdots& \vdots& \vdots \\
        \times & \cdots & \cdots & \cdots & \times \\
    \end{bmatrix}
\end{equation}

So all we need to do is create matrices $\vec{Q_{e+1}}\ldots\vec{Q_{e+p}}$ to zero newly added row.
So the $R$ is then equal to $\vec{R_{new}} = \vec{R}\vec{Q_{e+1}}\vec{Q_e}\ldots\vec{Q_2}\vec{Q_1}\vec{A}$. \todo{better notation}
$\vec{Q}$ is updated in the same way thus $\vec{Q_{new}} = \vec{Q}\vec{Q_e^T}\vec{Q_{e+1}^T}\ldots\vec{Q_{e+p}^T}$.

\todo{todo pseudokod? moc se mi nechce :-D}
\begin{algorithm}[H]
    \label{addingrowqr}
    % \SetKwInOut{Input}{input}
    % \SetKwInOut{Output}{output}
    \KwIn{$\vec{Q}, \vec{R}, \vec{x_i}$}
        \KwOut{$\vec{Q^{+}}, \vec{R^{+}}$ }
      \caption{QR insert}

    \Return{ $\vec{Q^{+}}$\,, $\vec{R^{+}}$ }\;
\end{algorithm}

Computing $\vec{R^{+}}$ is $\mathcal{O}(p^2)$ and computing $\vec{Q^{+}}$ is $\mathcal{O}(np)$.

\begin{note} \label{qnotrequired}
    For adding rows, the matrix $\vec{Q}$ is not needed so that we can update $\vec{R}$ to $\vec{R^{+}}$. Later we show that this is useful. \todo{say where}
\end{note}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%    QR DELETE     %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

When we are deleting the row $\vec{x_i}$ from matrix $\vec{A}$ we can use following trick 
\cite{hammarling2008updatingqr}. First, we move such row as the first row of the matrix $\vec{A}$ so ve create permutation matrix $\vec{P}$ so that 
\begin{equation}
    \vec{P}\vec{A} = \begin{bmatrix}
        \vec{x_i} \\
        \vec{A(1:i-1 , 1:p)} \\
        \vec{A(1:i+1 , 1:p)} 
    \end{bmatrix}
    = 
    \begin{bmatrix}
        \vec{x_i} \\
        \vec{A^{-}}
    \end{bmatrix}
    = \vec{P}\vec{Q}\vec{R}
\end{equation} 
where $\vec{A(a:b , 1:p)} $ means rows of matrix $\vec{A}$ from $a$ to $b$.
No we can se that we only need to zero first row $\vec{q_1}$ of matrix $\vec{Q}$. We can do this by $n-1$ matrixes matrixes $\vec{Q}(i,j) \in \mathbb{R}^{n \times n}$ so that

\begin{equation}
    \vec{Q}(n-1,n) \ldots \vec{Q}(1,2)\vec{q_1^T} =     \begin{bmatrix}
        1 \\
        0\\
        \vdots \\
    \end{bmatrix}
\end{equation}

To propagate the change into $\vec{R}$ we then consequently need to update $\vec{R}$ so that
\begin{equation}
    \vec{Q}(n-1,n) \ldots \vec{Q}(1,2)\vec{q_1^T} \vec{R} =     \begin{bmatrix}
        \times \\
        \vec{R^{-}}\\
        \vdots \\
    \end{bmatrix}.
\end{equation}
The result is
\begin{equation}
    \vec{P}\vec{A} = (\vec{P}\vec{Q}  \vec{Q^T}(1,2) \ldots   \vec{Q^T}(n-1,n)) (\vec{Q}(n-1,n) \ldots \vec{Q}(1,2)\vec{q_1^T} \vec{R} ) \\
    = 
    \begin{bmatrix}
        1 & 0 \\
        0 & \vec{Q^{-}}
    \end{bmatrix}
    \begin{bmatrix}
        \times \\
        \vec{R^{-}}
    \end{bmatrix},
\end{equation}
so that 
\begin{equation}
    \vec{A^{-}} = \vec{Q^{-}}\vec{R^{-}}
\end{equation}

\todo{napsat pseudokod. opet se mi moc necche :-D }
\begin{algorithm}[H]
    \label{removingrow}
    % \SetKwInOut{Input}{input}
    % \SetKwInOut{Output}{output}
    \KwIn{$\vec{Q}, \vec{R}, i$}
        \KwOut{$\vec{Q^{-}}, \vec{R^{-}}$ }
      \caption{QR delete}

    \Return{ $\vec{Q^{-}}$\,, $\vec{R^{-}}$ }\;
\end{algorithm}

Computing $\vec{R^{-}}$ is $\mathcal{O}(p^2)$ and computing $\vec{Q^{-}}$ is $\mathcal{O}(np)$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%% IMPROVED FSA - QR DECOMPOSITION  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Calculation of imporved FSA using QR}
So let's focus on our problem. As we said using decomposition is better than calculating inversion primarily due to higher numerical stability. \todo{nevim jestli zminovat ze muzu delat i SVD dekompozici ktera je nejstabilnejsi ze vsech...}
We'll show that both FSA as well as improved FSA can be calculated using QR decomposition.

Let's start with \ref{agullo:rovnice}. Here inversion is need to calculate 
$\vec{x_i}\vec{Z}^{-1}\vec{x_i^T}$, $\vec{x_j}\vec{Z}^{-1}\vec{x_j^T}$ and $\vec{x_i}\vec{Z}^{-1}\vec{x_j^T}$. This can also be done without inversion, only using the decomposition. We can write this equation
\begin{equation} \label{solveimi}
	\vec{x_i}\vec{Z}^{-1}\vec{x_i^T} = \vec{v}^T\vec{v} 	\iff \vec{x_i}(\vec{R}^T\vec{R})^{-1}\vec{x_i^T} = \vec{v}^T\vec{v}
\end{equation} 
where $\vec{v}$ can be obtained by solving lower triangular system
\begin{equation} \label{vsolution}
	\vec{R}^T\vec{v} = \vec{x_i}^T.
\end{equation} 
The same can be done with  $\vec{x_j}\vec{Z}^{-1}\vec{x_j^T}$. Last but not least we need to solve 
$\vec{x_i}\vec{Z}^{-1}\vec{x_j^T}$. We can write this 
\begin{equation}
	\vec{x_i}\vec{Z}^{-1}\vec{x_j^T} = \vec{x_j}\vec{u} 	\iff \vec{x_i}(\vec{R}^T\vec{R})^{-1}\vec{x_j^T} = \vec{x_j}^T\vec{u}
\end{equation} 
Where column vector $\vec{u}$ is defined by \ref{agullo_u}. We can see that 

\begin{equation}
	\vec{u} = \vec{Z}^{-1}\vec{x_i^T} 	\iff (\vec{R}^T\vec{R})^{-1}\vec{x_i^T} = \vec{u}
\end{equation}
so that $\vec{u}$ can be obtained ba solving upper triangular system 

\begin{equation} \label{solve_u_qr}
	\vec{R}\vec{u} = \vec{v}
\end{equation}
where $\vec{v}$ is solution of \ref{vsolution}. Other quantities of \ref{agullo:rovnice} does not require $\vec{Z}^{-1}$.

When optimal exchange $\vec{z_i}, \vec{z_j}$ is found then we need to update $RSS$ which can be done same way by \ref{updaterss}. Updating $\vec{\hat{w}}$ to  $\vec{\overline{\hat{w}}}$ by \ref{thetaplus} requires
$\vec{u}$  which in this case we calculate by \ref{solve_u_qr} and $b$ which requires  $\vec{x_j}\vec{Z}^{-1}\vec{x_j^T}$ but that we've also already covered by idea \ref{solveimi}. Analogous operations can be used to update $\vec{\overline{\hat{w}}}$ to $\vec{\overline{\overline{\hat{w}}}}$  by means of \ref{thetaminus}. Only thing wee need to realize that we can express \ref{xjoverlinez} as $\vec{x_j}\vec{\overline{{Z}}}^{-1}\vec{x_j^T} = \vec{x_i}\vec{{{Z}}}^{-1}\vec{x_i^T} + b(\vec{u}\vec{x_j}^2)$.

We can't use updating inversion because we don't have one so we need to somehow update our QR decomposition. This can be done by both Householder rotation as well as by Givens rotation, but because our $\vec{R}$ is already sparse matrix, Givens rotations are ideal tool for this. We've already described how this can be done. First we can use \ref{addingrowqr} to add row $\vec{x_i}$ to the decomposition and consequently \ref{removingrow} to remove $\vec{x_j}$  from the decomposition. We can see that this is slower than updating inversion directly. On the other hand this solution is numerically stable and requires less time than computing factorization again from scratch. 

Finally let's talk about matrix $\vec{\tilde{Z}}$ which we used \ref{matrixZ} for derivation of our equations. If we use \ref{qrcholesky} observation then we realize that if we make QR factorization of $\vec{A} = (\vec{X}, \vec{y})$ then

\begin{equation}
	\vec{\tilde{Z}} = 
	\begin{bmatrix}
		\vec{Z} & \vec{X}^T\vec{y} \\
    \vec{y}^T\vec{X} & \vec{y}^T\vec{y}
	\end{bmatrix} 
	= 
	\begin{bmatrix}
		\vec{R^T} & 0 \\
    \vec{\phi^T} & r
	\end{bmatrix} 
	\begin{bmatrix}
		\vec{R} & \vec{\phi} \\
     0 & r
	\end{bmatrix} 
\end{equation}

where 

\begin{equation}
	\vec{\tilde{R}} = 
	\begin{bmatrix}
		\vec{R} & \vec{\phi} \\
     0 & r
	\end{bmatrix} 
	= \vec{\tilde{Q^T}}\vec{A}
\end{equation}

is matrix from QR factorization of $\vec{A}$. 
Next we can realize that $\vec{R} \in \mathbb{R}^{p \times p}$ is actually matrix $\vec{R}$ which we can obtain by QR factorization of $\vec{X}$. 
Moreover $\vec{\phi} \in \mathbb{R}^{p \times 1}$ is column vector which is actually equal to
\begin{equation}
	\vec{\phi} =  \vec{Q^T}\vec{y}
\end{equation}
where $\vec{Q}$ is matrix $\vec{Q}$ from QR factorization of $\vec{R}$.
Due to this fact $\vec{\hat{w}}$ is solution of upper triangular system 
\begin{equation}
	\vec{R}\vec{\hat{w}} = \vec{\phi}
\end{equation}

Finally $r \in \mathbb{R}$ is scalar such that 

\begin{equation}
	r^2 = \vec{y}^T\vec{y} - \vec{y}^T\vec{X}\vec{\hat{w}} = RSS
\end{equation}

\begin{remark} \label{qnotrequiredspeedupremark}
	We can see that all quantities we use in the algorithm can be obtained from matrix $\vec{\tilde{R}}$ moreover if we didn't need to remove rows from $\vec{X}$ then matrix $\vec{\tilde{Q}}$ would not need \ref{qnotrequired}, thus speedup while inserting and rows would be possible. 
\end{remark}

Now we've described all properties of improved FSA algorithm. We can see that two different methods of computation are possible. One, using inversion $\vec{Z}^{-1}$ is faster, but numerically less stable, second using QR decomposition is slower due to updating matrix $\vec{R}$ and matrix $\vec{Q}$. We've also shown that using matrix $\vec{\tilde{R}}$ is useful because it contains all necessary quantities for the improved FSA algorithm, although it won't improve the speed of the algorithm. Later we'll introduce different algorithm where we won't need to remove rows from $\vec{X}$ so then we'll be ab1`A																		`e to use this observation in our favor.

\todo{time complexity QR qwerty}
\todo{time complexity tildeZQR is the same as QR}


\subsection{MMEA}
\todo{no taak. to je na stranku. to das}

\section{BAB algorithm}  % Also from Agulló, 2000
\todo{zminit podobnost s: Hoffmann, Kontoghiorghes, 2010 (Matrix strategies SUR)}
\section{BSA algorithm}
In this section we'll introduce another exact algorithm. It have a little different approach than previous algorithms. First of all we'll built a theoretical basis for this algorithm.

% \subsubsection{Domain of $\oflts$}
% We've already showed that we can derivate LTS objective function so it's domain is on discrete set  $Q^{(n,h)}$ see \ref{oflts_discrete}. We'll now show that we can transform this objective function to the non-discrete form so that $\oflts (\vec{w}) \vec{w} \in \mathbb{R}^p$ \cite{klouda2015exact}.
% We'll do this by relation $Z \subset \mathbb{R}^p \times Q^{(n,h)}$ so that $(\vec{w}, \vec{m}) \in Z$ only and if only
% \begin{equation}
% 	\sum\limits_{i=1}^h r^{2}_{(i:n)}(\vec{w})  =  \sum\limits_{i=1}^n m_{i}r^{2}_{i}(\vec{w}).
% \end{equation}
% $Z$ is not a mapping. To show this we can take simple example so that $r^{2}_{h} = r^{2}_{h+1}$. Then we have for given $\vec{w}$ two different vectors $\vec{m}$ that are in relation with it.

% If we want to find domain where $Z$ is mapping, we simply define $\mathcal{U} \subset \mathbb{R}^{p}$ as a maximal set where $Z$ is a mapping. Next we define 
% $\mathcal{H} = \mathbb{R}^{p}  \setminus   \mathcal{U}$
% as a complement of $\mathcal{U}$.

% Let's now describe some properties based on which $\vec{w}$ is in $\mathcal{U}$ or $\mathcal{H}$ set.

% \begin{theorem} \label{klouda1}
% 	$\vec{w} \in \mathcal{U}, \vec{w} \in \mathbb{R}^{p}$ if and only if 
% 	$r^{2}_{(h:n)}(\vec{w}) < r^{2}_{(h+1:n)}(\vec{w})$
% \end{theorem}
% \begin{proof}
% 	As shown in example above. If $r^{2}_{i}(\vec{w}) = r^{2}_{(h:n)}(\vec{w}) = r^{2}_{(h+1:n)}(\vec{w}) = r^{2}_{j}(\vec{w}), i,j \in   \{{1,2,\ldots , n\}} $ then $(\vec{w}, \vec{m_1}) \in Z$ and also $(\vec{w}, \vec{m_2}) \in Z$ where $\vec{m_1}$ has ones at indexes of $h$ smallest residuals and $\vec{m_2}$ has ones at the same indexes except swap $i$th one with $j$th zero.
% \end{proof}

% \begin{corollary}
% 	Complement of vectors $\vec{w}$ from theorem \ref{klouda1} are vectors such that 
% 	\begin{equation}
% 		r^{2}_{(h:n)}(\vec{w}) = r^{2}_{(h+1:n)}(\vec{w})
% 	\end{equation}
% 	thus
% 	\begin{equation}
% 		\mathcal{H} = \{{ \vec{w} \in \mathbb{R}^{p} | r^{2}_{(h:n)}(\vec{w}) = r^{2}_{(h+1:n)}(\vec{w}) \}}.
% 	\end{equation}
% 	That means that for each $\vec{w} \in \mathcal{H}$ there are two different $(\vec{x_i}, y_i)$ and  $(\vec{x_j}, y_j)$ so that
% 	\begin{equation}
% 		(y_i - \vec{x_i} \vec{w})^2 = r^{2}_i(\vec{w}) =  r^{2}_{(h:n)}(\vec{w}) = r^{2}_{(h+1:n)}(\vec{w}) =  r^{2}_j(\vec{w}) = (y_j - \vec{x_j} \vec{w})^2.
% 	\end{equation}

% 	We can see that 
% 	\begin{equation}
% 		(y_i - \vec{x_i} \vec{w})^2 =  (y_j - \vec{x_j} \vec{w})^2 \iff	  y_i \pm y_j + (\vec{x_i} \pm \vec{x_j})  \vec{w} = 0
% 	\end{equation}
% \end{corollary}

% Moreover if we assume that for all $i,j, \in \{{1,2 \ldots, n \}}$  if $ i \neq j$ then $\vec{x_i} \neq \pm \vec{x_j}$ and $\norm{\vec{x_i}} \neq 0$ then $ y_i \pm y_j + (\vec{x_i} \pm \vec{x_j})  \vec{w} = 0$ represents a hyperplane. 

% It's easy to show that set $\mathcal{U}$ is open and 	lebesgue measure of $\mathcal{H}$  is $0$ \cite{klouda2015exact}. Moreover $\mathcal{H}$ splits $\mathbb{R}^{p}$ into finite number of $m$ open disjoint subsets of $\mathcal{U}$. 

% \begin{definition}
% 	Let's define set of $m \in \mathbb{N}$ sets $\mathcal{U}^{(set)} = \{{ U_i\}}_{i=1}^{m}$ so that
% 	all $U_i$ are open and connected sets, $U_i$ $U_j$ are disjoined thus $U_i \cap  U_j = \emptyset$, $\cup_{i=1}^{m}	U_i = \mathcal{U}$ and $\cup_{i=1}^{m}	\partial U_i =  \mathcal{H}$.

% 	We define neighbor sets $U_i, U_j, i \neq j$ so that $ U_i \cap U_j \neq \emptyset$. 

% 	We also define $M^{(min)}$ set of $m$ vectors $\vec{m} \in Q^{(n,h)}$ so that 
% 	\begin{equation*}
% 		M^{(min)} = \{{ \vec{m_1}, \ldots, \vec{m_m} | \vec{m_i} = Z(\vec{w}) \vec{w} \in U_i  \}}.
% 	\end{equation*}
% 	where $Z(\vec{w})$ represent unique vector $\vec{m_i}$. That means $Z(\vec{w}) = Z(\vec{w^{\prime}})$ for all $w, w^{\prime} \in U_i$.
% \end{definition}

% For a better understanding definition above see \missingfigure{p=1 n=5 h=3. visualise all $U_i$ and elements of $\mathcal{H}$}

% \begin{theorem}
% 	If the matrix $\vec{X}$ have $h$-full rank then for every local minima at point satisfying weak-necessary condition $\vec{w_{0}}$ of the lts objective function, then there exists a vector $\vec{m} \in Q^{(n,h)}$ such that $(\vec{w_{0}}, \vec{m}) \in Z$.
% \end{theorem}

% Proof can be found in \cite{klouda2015exact}. 

% Given that we can see that 

% \begin{equation}
% 	\minim_{\vec{m} \in Q^{(n,h)}} \of^{(OLS, \vec{M}\vec{X}, \vec{M}\vec{Y})} (\vec{w}) =
% 	\minim_{m \in M^{(min)}} \of^{(OLS, \vec{m^i}\vec{X}, \vec{m^i}\vec{Y})} (\vec{w})
% \end{equation}

% where $\vec{M} = diag(\vec{m})$, $\vec{m^i} = diag(m)$. We can see that set $M^{(min)}$ is very useful because we can minimize only on this set and not on whole $Q^{(n,h)$.

% Then minimizing objective function $\oflts(\vec{m})$ from \ref{oflts_discrete} can be reformulated as 
% \begin{equation}
% 	\minim_{\vec{m} \in Q^{(n,h)}}  \oflts(\vec{m}) = \minim_{m \in M^{(min)}} \oflts(m)
% \end{equation}

% We now have all the theory necessary for describing two dimensional version of the algorithm. We'll now describe this algorithm and next we'll provide some more theory necessary to extend this algorithm to higher dimensions.

% First we'll show how to find fde
