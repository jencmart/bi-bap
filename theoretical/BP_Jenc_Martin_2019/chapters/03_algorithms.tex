\chapter{Algorithms}

% mathbb - double -> R,N,Z ...
% mathcal - curved -> N, O
% sigma tilde hat 
In previous chapter we've covered necessary theory needed to implement algorithms that are in this chapter. Let's quickly recap most important fact that we know so far.

With robust linear regression problem we assume that our model with intercept
\begin{equation}
		y_i = \vec{w}^T\vec{x_i} + \varepsilon_i
\end{equation}
where $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$ is $i.i.d.$ random variable has some displacements in upmost half of the explanatory $\vec{x_i}$ or dependent $y_i$ variables. Thus only some subset $\vec{\tilde{X}} = \vec{M}\vec{X}$ with corresponding $\vec{\tilde{y}} = \vec{}\vec{y}$ where $\vec{M} = diag(\vec{m})$ and $\vec{m} \in Q^{(n,h)}$ can be perceived so that 

\begin{equation}
	\tilde{y}_i \sim \mathcal{N}(\vec{w}^T\vec{\tilde{x}_i}, \sigma^2).
\end{equation}
We'll denote this subset simply as $h subset$ of data in order to simplify following text.
We've also learnt that finding solution of LTS we need to find correct $h$ subset and then calculate estimate of regression coefficients $\tilde{w}$. 
So to find exact solution of LTS we need to go through all h subsets, find the correct one and calculate ordinary least squares fit to get the regression coefficients. There are not much of other options because objective function is non-differentiable and non-convex with lots of local minima.  

We also know that exhaustive approach will fail due  exponential size of $Q^{(n,h)}$. So what are the possibilities? First attempts were based on iterative removal data samples whose residuum had the highest value based on OLS fit on whole dataset. Such attempts were proven to be completely wrong because initial OLS fit is usually already  heavily affected by outliers and we my end up removing data samples which represents original model.

Then there are algorithms based on purely on random approach. 

On such algorithm  is Random solution algorithm \cite{bai2003random} which is basically randomly selects $L$ $h$ subsets and subsequently compute OLS fit on each of them and selecting fit with smallest $RSS$ and consider it a approximate solution. Such approach is very simple, but in general probability of selecting at least one such subset from $L$ subsets which don't contain outliers thus has a chance of producing good result goes to zero for increasing number of data samples $n$ as we'll describe in detail in \ref{hrandomsamples}.

Another very similar algorithm called Resampling algorithm introduced in \cite{rousseeuw1987robust}
have basically just a little difference and that it select vectors from $Q^{(n,p+1)}$ instead of 
$Q^{(n, h)}$. This minor tweak has not only higher chance to succeed because number of vectors in this set is significantly lower than in $Q^{(n, h)}$ (at least if $h$ conservatively chosen thus $h = [(n/2] + [(p+1)/2]$) but also because probability of selecting $L$ subsets of size which is independent of $n$ gives nonzero probability of selecting at least one subset such that it don't include outliers see \ref{prandomsamples} for more details.

Generating all possible h subsets is computationally hard and relying on selecting random subsets don't produce sufficiently good results. So what are our options? In \cite{hawkins1999improved} two criterions called \defterm{necessary conditions} are introduced. They talk about necessary properties which some $h$ subset must satisfy so it could be set which leads to global optima of LTS. Let's introduce those two necessary conditions. For that it's convenient not to only label $h$ subset of used observations but also complementary subset of not used observations. We'll refer to this complementary subset as $g$ subset.

\begin{theorem} \label{strong_condition}Strong necessary condition.
The criterion cannot be improved by exchanging any of the observations from $g$ subset 
for any of the currently used observations in $h$ subset. Thus $\vec{m} \in Q^{(n, h)}$ meets the criterion if  $J(\vec{m}) \leq J(\vec{m_{swap}})$ where $\vec{m_{swap}}$ is any vector from $Q^{(n, h)}$ such that it has same values except one swapped.
\end{theorem}

\begin{proof}
	Trivial. LTS uses subset of $h$ observations that minimize it's objective function. To be this true none of the swaps between observations from $h$ subset and $g$ subset  must not improve (reduce) it's objective function. 
\end{proof}
Based on this idea algorithm can be created. We'll discuss it in detail in \ref{section_feasible_solution}.

Second necessary condition named \defterm{weak necessary condition} 
\begin{theorem}
	$\vec{m} \in Q^{(n, h)}$ meets the criterion if for each observation from $h$ subset has smaller (or equal) squared residual than any observation from $g$ subset.
\end{theorem}	
Again, based on this criterion an algorithm can be crated. Corollary of this criteria together with proof can be found in \ref{section_fast_lts}. 

Very interesting consequence which we'll use later gives us following lemma.

\begin{lemma} \label{lemma_conditions}
	Strong necessary condition is not satisfied unless weak necessary condition in satisfied. Thus if strong condition is satisfied then weak is also. 
\end{lemma}

\begin{proof}
	We'll make proof by contradiction. Let's assume that we have $\vec{m} \in Q^{(n, h)}$ and $J(\vec{m})$ for which strong necessary condition is satisfied but weak necessary condition is not. That means there exists $\vec{x_i}$ with $y_i$ from $h$ subset and $\vec{x_j}$ and $y_j$ from $g$ subset such that $r_j^2 < r_i^2$. Thus 

	\begin{equation} 
		\oflts(\vec{m}) > \oflts(\vec{m}) + r_j^2 - r_i^2  
	\end{equation}

	Now we just need to show that $\vec{m_{swap}}$ vector that is created by swapping that $j$th observation from $g$ subset with $i$th observation from $h$ subset leads to 

	\begin{equation} 
	  \oflts(\vec{m}) + r_j^2 - r_i^2  \geq \oflts(\vec{m_{swap}}).
	\end{equation}
	That's indeed trivial because $\oflts(\vec{m_{swap}})$ is in fact just OLS hat minimize objective function on given subset of observations. Thats of course contradiction with our assumption which says that strong necessary condition is already satisfied. 
\end{proof}
When we'll discuss algorithms based on this conditions we'll show that algorithm based on weak necessary condition is much faster than algorithm based on strong necessary condition which will lead us to another algorithm where we'll use \ref{lemma_conditions}.

Now we've covered all necessary theoretical background and it's time to introduce currently popular algorithms of computing LTS estimate.

\section{Computing OLS}  \label{ols:computing}
In this section we'll describe what algorithms of computing OLS exists. We'll see that beside computing OLS directly from the objective function there are better ways. Let's now start with this straightforward  approach.

\begin{lemma}
	Time complexity of OLS  on $\m{X}^{n \times p}$ and $\m{Y}^{n \times 1}$ is $O(p^2n)$.
\end{lemma}

\begin{proof}
	Normal equation of OLS is $\vec{\hat{w}} = (\m{X^T}\m{X})^{-1}\m{X^T}\m{Y}$.
	Time complexity  of matrix multiplication $\m{A}^{m \times n}$ and  $\m{B}^{n \times p}$ is $\sim \mathcal{O}(mnp)$.
	Time complexity of matrix $\m{C}^{m \times m}$ is $\sim \mathcal{O}(m^3)$
	So we need to compute 
	$\m{A} = \m{X^T}\m{X} \sim \mathcal{O}(p^2n)$ and
	$\m{B} = \m{X^T}\m{Y} \sim \mathcal{O}(pn)$ and
	$\m{C} = \m{A}^{-1} \sim \mathcal{O}(p^3)$ and finally 
	$\m{C}\m{B} \sim \mathcal{O}(p^2)$. 
	That gives us $\mathcal{O}(p^2n + pn + p^3 + p^2)$. Because  $\mathcal{O}(p^2n)$ and 
	$\mathcal{O}(p^3)$ asymptotically dominates over $\mathcal{O}(p^2)$ and $\mathcal{O}(pn)$ we can
	write $\mathcal{O}(p^2n + p^3)$.

	\todo{ CO zo toho je vic? Neni casove narocnejsi vynasobeni $\m{X^T}\m{X}$ nez inverze, kdyz bereme v uvahu $n >> p$ ???}
	
\end{proof}
\todo{complete this section with describing computation of OLS using matrix decomposition}

\section{Computing LTS}

\begin{note}
	When discussing following algorithms, we'll refer to given $\vec{X}$ and $y$ as to \textbf{\textit{data set}} and to $y_i$ with corresponding $\vec{x_i}$ as to \textbf{\textit{data sample}} or \textbf{\textit{observation}}. Sometimes it's also useful to refer to multiple observations as to subset of observations. When we want to mark subset of observations  
	$y_i\\, \vec{x_i}\\, i \in H\,, H \subset \{{1,2,\ldots , n\}}$ we can simply refer to it as to subset of observations $H$. Sometimes it's also useful to mark matrix $\vec{X}$ with only some subset of observations which we'll do by $\vec{X_H}$.  
\end{note}

% \textbf{X} - bold
% \pmb{X} - strong bolda
% \boldmath{X} - quite normal
%  $\textbf{X}^T$ - TRANSPOSED MATRIX

% change arrowed vector to the bold vector
% mathbf give bold 
% bold-symbol gives bold cursive
% **********************************************************************************
% *************% ************* ************ FAST - LTS *********************************************
% ************* ************ FAST - LTS *********************************************
% ************* ************ FAST - LTS *********************************************
% ************* ************ FAST - LTS *********************************************
% ************ FAST - LTS *********************************************
% **********************************************************************************
\section{FAST-LTS} \label{section_fast_lts}
In this section we will introduce FAST-LTS algorithm\cite{rouss:2000}. 
It is, as well as in other cases, iterative algorithm. We will discuss all main components
of the algorithm starting with its core idea called concentration step which '
authors simply call C-step.

% $\showVec{\hat{w}}{w}{n}$
% $\boldsymbol{Y}$ - bold cursive

\subsection{C-step}
We will show that from existing LTS estimate $\boldsymbol{\hat{w}_{old}}$ we 
can construct new LTS estimate $\boldsymbol{\hat{w}_{new}}$ which objective 
function is less or equal to the old one. Based on this property we will be able 
to create sequence of LTS estimates which will lead to better results.

% \usepackage{etoolbox}


%******************************** C-STEP theorem  ***************************************************%

\begin{theorem}
Consider dataset consisting of
$\vec{x_1}, \vec{x_2} \ldots,\vec{x_n}$ explanatory variables where 
$\vec{x_i}\in\mathbb{R}^p\,, \forall \vec{x_i} = (fx^i_1, x^i_2,\ldots,x^i_p)$ where $x^i_1 = 1$
and its corresponding $y_1, y_2,\ldots,y_n$ response variables. 
Let's also have $\vec{\hat{w}_0}\in\mathbb{R}^p$ any p-dimensional vector and 
$H_0 = \{{h_i ; h_i \in\mathbb{Z}\,, 1 \leq h_i \leq n\}}\,, |H_0| = h$. 
Let's now mark $RSS(\what{0}) = \sum_{i\in H_0} (r_0(i))^2$ where 
$r_0(i) = y_i - (w_1^0x^i_1 + w_2^0x^i_2 +\ldots+ w_p^0x^i_p$).
%Let's now mark  $H_1 = \{{h_i ;  1 \leq h_i \leq n\   \}}$ 
%uch that 
Let's take $\hat{n} = \{{1,2,\ldots,n\}}$ and mark
$\pi: \hat{n} \rightarrow \hat{n}$ permutation of $\hat{n}$ such that $|r_0({\pi(1)})| \leq |r_0({\pi(2)})| \leq \ldots \leq |r_0({\pi(n)})|$
and mark $H_1 = \{{\pi(1)\,, \pi(2)\,,... \pi(h)\}}$ set of $h$ indexes corresponding to $h$ smallest absolute residuals $r_0(i)$.
Finally take $\vec{\hat{w}^{OLS(H_1)}_1 }$ ordinary least squares fit on $H_1$ subset of observations
and its corresponding $RSS(\what{1}) = \sum_{i\in H_1} (r_1(i))^2$ sum of least squares. Then
\[ 
	RSS(\what{1}) \leq RSS(\what{0}) \numberthis
\]
\end{theorem}

\begin{proof}
	Because we take $h$ observations with smallest absolute residuals $r_0$, then for sure $\sum_{i\in H_1} (r_0(i))^2 \leq \sum_{i\in H_0} (r_0(i))^2 =  RSS(\what{0})$.
	When we take into account that Ordinary least squares fit $OLS_{H_1}$ minimize objective function of 
	$H_1$ subset of observations, then for sure  $RSS(\what{1}) =  \sum_{i\in H_1} (r_1(i))^2 \leq \sum_{i\in H_1} (r_0(i))^2$.
	Together we get $$RSS(\what{1})=\sum_{i\in H_1}(r_1(i))^2\leq\sum_{i\in H_1}(r_0(i))^2\leq\sum_{i\in H_0}(r_0(i))^2=RSS(\what{0})$$
\end{proof}

%******************************** C-STEP algorithm ***************************************************************************%

\begin{corollary} 
	Based on previous theorem, using some $\vec{\hat{w}^{OLS(H_{old})}}$  on $H_{old}$ subset of observations we can
	construct $H_new$ subset with corresponding $\vec{\hat{w}^{OLS(H_{new})}}$ such that $RSS(\vec{\hat{w}^{OLS(H_{new})}}) \leq RSS(\vec{\hat{w}^{OLS(H_{old})}})$. 
	With this we can apply above theorem again on $\vec{\hat{w}^{OLS(H_{new})}}$ with $H_{new}$. This will lead to the iterative sequence of
	$RSS(\what{{old}}) \leq RSS(\what{{new}}) \leq \ldots$. One step of this process is described by following pseudocode. Note that for C-step we actually need only $\vec{\hat{w}}$ 
	 without need of passing  $H$.
\end{corollary}
\todo{include and reference nice image showing one c-step }

\begin{algorithm}[H]
	\label{alg:Cstep}
    % \SetKwInOut{Input}{input}
    % \SetKwInOut{Output}{output}
    \KwIn{dataset consiting of $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ and $\boldsymbol{y} \in \mathbb{R}^{n \times 1}$,  $\what{{old}} \in \mathbb{R}^{p \times 1}$}
    \KwOut{ $\what{{new}}$, $H_{new}$ }
	\caption{C-step}
	
	$R \gets \emptyset$\;
	\For{$i \gets 1$ \textbf{to} $n$}{  
		$R \gets R \cup \{ |y_i - \what{{old}} \vec{x_i}^T |\}$\;
	}
	$H_{new} \gets $ select set of $h$ smallest absolute residuals from $R$\;
	$\what{{new}} \gets OLS(H_{new})$\;
	\Return{ $\what{{new}}$\,, $H_{new}$ }\;
\end{algorithm}

%******************************** C-STEP alg. time complexity ******************************************%
\begin{observation} 
	Time complexity of algorithm C-step \ref{alg:Cstep} is the same as time complexity as OLS. Thus $O(p^2n)$
	\todo{create better proof. And take into account both versions - directly vs. using decomposition}
\end{observation} 


\begin{proof}
	In C-step we must compute $n$ absolute residuals. Computation of one absolute residual consists of
	matrix multiplication of shapes $1 \times p$ and $p \times 1$ that gives us $\mathcal{O}(p)$. Rest is in constant time.
	So time of computation $n$ residuals is $\mathcal{O}(np)$.
	Next we must select set of $h$ smallest residuals which can be done in $\mathcal{O}(n)$ using modification of algorithm QuickSelect. \todo{reference or define quick select} 
	Finally we must compute $\hat{w}$ OLS estimate on $h$ subset of data.
	Because $h$ is linearly dependent on $n$, we can say that it is $\mathcal{O}(p^2n + p^3)$ which 
	is asymptotically dominant against previous steps which are $\mathcal{O}(np + n)$.
\end{proof}

As we stated above, repeating algorithm C-step will lead to sequence of $\what{1}, \what{2} \ldots$ 
on subsets $H_1, H_2 \ldots$ with corresponding residual sum of squares
$RSS(\what{{1}}) \geq RSS(\what{{2}}) \geq \ldots$. One could ask if this sequence will converge, so that
$RSS(\what{{i}}) == RSS(\what{{i+1}})$. 
Answer to this question will be presented by the following theorem.


%******************************** C-STEP alg. will converge ***********************%
\begin{theorem}
	Sequence of C-step will converge to $\what{{m}}$ after maximum of $m = {n \choose h}$
	so that $RSS(\what{{m}}) == RSS(\what{{n}})\,, \forall n\geq m$ where $n$ is number of data samples 
	and $h$ is size of subset $H_i$.
\end{theorem}

\begin{proof}
	Since  $RSS(\what{{i}})$ is non-negative and $RSS(\what{{i}}) \leq RSS(\what{{i+i}})$ the 
	sequence will converge. $\what{{i}}$  is computed out of subset 
	$H_i \subset \{{1,2,\ldots,n\}}$. When there is finite number of subsets of size $h$ out of $n$ samples, namely ${n \choose h}$, the sequence will converge at the latest after this number of steps.
\end{proof}

%******************************** ITERATE-C-STEP algorithm ****************************%
Above theorem gives us clue to create algorithm described by following pseudocode.

\begin{algorithm}[H]
	\label{alg:RepeatCstep}
	\KwIn{dataset consiting of $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ 
	and $\boldsymbol{y} \in \mathbb{R}^{n \times 1}$,  $\what{{old}} \in \mathbb{R}^{p \times 1}\,, H_0 $}
    \KwOut{ $\what{{final}}$, $H_{final}$ }
	\caption{Repeat-C-step}
	\SetKw{Break}{break}
	$\what{{new}} \gets \emptyset$\;
	$H_{new} \gets \emptyset$\;
	$RSS_{new} \gets \infty $\;

	\While{$True$}{
		$RSS_{old} \gets RSS(\what{{old}})$\;
		$\what{{new}}$\,, $H_{new} \gets \boldsymbol{X}\,, \boldsymbol{y}\,, \what{{old}}$\;
		$RSS_{new} \gets RSS(\what{{new}})$\;
		\If{$RSS_{old} == RSS_{new}$}{
			\Break
		  }
		$\what{{old}} \gets \what{{new}}$
	}

	\Return{ $\what{{new}}$, $H_{new}$ }\;
\end{algorithm}

It is important to note, that although maximum number of steps of this algorithm is ${n \choose h}$ in practice it is very low, most often under $20$ steps.
\todo{include sime nice grap which show this. or table ?}
That is not enough for the algorithm $Repeat-C-step$ to converge to global minimum, but it is necessary condition. That gives us an idea how to create the final algorithm. \cite{rouss:2000}

Choose a lot of initial subsets $H_1$ and on each of them apply algorithm Repeat-C-step. From all converged subsets with corresponding $\hat{w}$ estimates choose that which has lowest $RSS(\hat{w})$. 

Before we can construct final algorithm we must decide how to choose initial subset $H_1$ and how many of them mean ``\emph{a lot of}''. First let's focus on how to choose initial subset $H_1$.

% \renewcommand{\O}[1]{$\mathcal{O}(#1)$}

% ************************************************	INITIAL H_1 SUBSET *************************************%
\subsection{Choosing initial $H_1$subset}

It is important to note, that when we choose $H_1$ subset such that it contains outliers, then iteration of  C-steps
usually won't converge to good results, so we should focus on methods with non zero probability of selecting $H_1$ such that it won't contain outliers.
There are a lot of possibilities how to create initial $hH_1$ subset. Lets start with most trivial one.


% ************************************************** RANDOM SELECTION **************************************%
\subsubsection{Random selection}
Most basic way of creating $H_1$ subset is simply to choose random $H_1 \subset \{{1,2,\ldots , n\}}$. Following observation will show that it not the best way.

\begin{observation} \label{hrandomsamples}
	With increasing number of data samples, thus with increasing $n$, the probability of choosing among $m$ random selections of $H_{1_1}, \ldots ,H_{1_m}$ the probability of selecting
	at least one $H_{1_i}$ such that its corresponding data samples does not contains outliers, goes to $0$.
\end{observation}

\begin{proof}
	Consider dataset of $n$ containing $\epsilon > 0$ relative amount of outliers. Let $h=(n+p+1)/2$ and $m$ is number of selections random $|H| = h$ subsets. Then
	\begin{align*}
		P(one~random~data~sample~not~outliers) &= (1-\epsilon) \\
		P(one~subset~without~outliers) &= (1-\epsilon)^h \\
		P(one~subset~with~at~least~one~outlier) &= 1-(1-\epsilon)^h \\
		P(m~subsets~with~at~least~one~outlier~in~each) &= (1-(1-\epsilon)^h)^m \\
		P(m~subsets~with~at~least~one~subset~without~outliers) &= 1-(1-(1-\epsilon)^h)^m \\
	\end{align*}

	Because $n \rightarrow \infty 	
	\Rightarrow (1-\epsilon)^h  \rightarrow 0 	
	\Rightarrow 1- (1-\epsilon)^h  \rightarrow 1
	\Rightarrow (1-(1-\epsilon)^h)^m  \rightarrow 1
	\Rightarrow 1- (1-(1-\epsilon)^h)^m  \rightarrow 0 $
\end{proof}

That means that we should consider other options of selecting $H_1$ subset. Actually if we would like to continue with selecting some random subsets, previous observation gives us clue, that we should choose it independent of $n$. Authors of algorithm came with such solution and it goes as follows.

% ***************************************************** P - SELECTION ***************************************%
\subsubsection{P-subset selection}
Let's choose subset $J \subset \{{1,2,\ldots,n\}}\,, |J| = p$. Next compute rank of matrix $\m{X}_{J:}$. If $rank(\m{X}_{J:}) < p$ add randomly selected rows to $\m{X}_{J:}$ without repetition until $rank(\m{X}_{J:}) = p$. Let's from now on suppose that $rank(\m{X}_{J:}) = p$. Next let us mark $\what{0} = OLS(J)$ and corresponding $(r_0(1)), (r_0(2)), \ldots ,(r_0(n))$ residuals.  Now mark $\hat{n} = \{{1,2,\ldots,n\}}$ and let
$\pi: \hat{n} \rightarrow \hat{n}$ be permutation of $\hat{n}$ such that $|r({\pi(1)})| \leq |r({\pi(2)})| \leq \ldots \leq |r({\pi(n)})|$. Finally put $H_1 = \{{\pi(1)\,, \pi(2)\,,... \pi(h)\}}$ set of $h$ indexes corresponding to $h$ smallest absolute residuals $r_0(i)$.

\begin{observation} \label{prandomsamples}
	With increasing number of data samples, thus with increasing $n$, the probability of choosing among $m$ random selections of $J_{1_1}, \ldots ,J_{1_m}$ the probability of selecting
	at least one $J_{1_i}$ such that its corresponding data samples does not contains outliers, goes toho
	$$ 1-(1-(1-\epsilon)^h)^m  > 0$$
\end{observation}

\begin{proof}
	Similarly as in previous observation.
\end{proof}

\begin{itshape}
Note that there are other possibilities of choosing $H_1$ subset other than these presented in \cite{rouss:2000}.
We'll properly discuss them in chapter \todo{write reference to section where this'll be}
\end{itshape}

Last missing piece of the algorithm is determining number of $m$ initial $H_1$ subsets, which will maximize probability to at least one of them will converge to good solution. Simply put, the more the better. So before we will answer this question properly, let's discuss some key observations about algorithm.

% ***************************************************** SPEED-UP ***************************************%

\subsection{Speed-up of the algorithm}
In this section we will describe important observations which will help us to formulate final algorithm. In two subsections we'll briefly describe how to optimize current algorithm. 

% *************************************************SELECTIVE ITERATION  ***************************************%
\subsubsection{Selective iteration}
The most computationally demanding part of one C-step is computation of OLS on $H_i$ subset and then 
calculation of $n$ absolute residuals. How we stated above, convergence is usually achieved under 20 steps. 
So for fast algorithm run we would like to repeat C-step as little as possible and in the same time didn't loose performance of algorithm. 

Due to that convergence of repeating C-step is very fast, it turns out, that we are able to distinguish between starts that will lead to good solutions and those who won't even after very little C-steps iterations. <based on empiric observation, we can distinguish good or bad solution already after two or three iterations of C-steps based on $RSS(\what{3})$ or $RSS(\what{4})$ respectively. 

So even though authors don't specify size of $m$ explicitly, they propose that after a few C-steps we can choose (say~10) best solutions among all $H_1$ starts and continue C-steps till convergence only on those best solutions.
This process is called Selective iteration.

\begin{itshape}
	We can choose $m$ with respect to observation \ref{prandomsamples}. In ideal case we would like to have probability of existence at least one initial $H_1$ subset close to $1$. As we see $m$ is exponentially dependent on $p$ and at the same time in practice we don't know percentage of outliers in dataset. So it is difficult to mention exact value. Specific values of $m$ in respect to data size is visible in table \todo{include some nice table}. So we can say that with $p < 10$ choosing $m = 500$ is usually safe starting point.
\end{itshape}

\subsubsection{Nested extension}
C-step computation is usually very fast for small $n$. Problem starts with very high $n$ say $n > 10^3$ because we need to compute OLS on $H_i$ subset of size $h$ which is dependent on $n$. And then calculate $n$ absolute residuals.

Authors came up with solution they call Nested extension. We will describe it briefly now.
\begin{itemize}
	\item If $n$ is greater than limit $l$, we'll create subset of data samples $L\,, |L| = l$ and divide this subset into $s$ disjunctive sets $P_1,P_2,\ldots,P_s\,, |P_i| = \frac{l}{s}\,, P_i\cap P_j  = \emptyset\,, \bigcup_{i=1}^{s} P_{i} = L$.
	\item For every $P_i$ we'll set number of starts $m_{P_i} = \frac{m}{l}$. 
	\item Next in every $P_i$ we'll create $m_{P_i}$ number of initial $H_{P_{i_1}}$ subsets and iterate C-steps for two iterations.
	\item Then we'll choose $10$ best results from each subsets and merge them together. We'll get family of sets
	$F_{merged}$ containing $10$ best $H_{P_{i_3}}$ subsets from each $P_i$.
	\item On each subset from  $F_{merged}$ family of subsets we'll again iterate $2$ C-steps and then choose $10$ best results.
	\item Finally we'll use these best $10$ subsets and use them to iterate C-steps till convergence.
	\item As a result we'll choose best of those $10$ converged results.
\end{itemize} 

\subsubsection{Putting all together}
We've described all major parts of the algorithm FAST-LTS. One last thing we need to mention is that even though C-steps iteration usually converge under $20$ steps it is appropriate to introduce two parameters $max\_iteration$ and $threshold$ which will limit number of C-steps iterations in some rare cases when convergence is too slow. Parameter $max\_iteration$ denotes maximum number of iterations in final C-step iteration till convergence. Parameter $threshold$ denotes stopping criterion such that $| RSS(\what{{i}}) - RSS(\what{{i+1}})| \leq threshold$ instead of 
$RSS_{i} == RSS_{i+1}$ . When we put all together, we'll get FAST-LTS algorithm which is described by following pseudocode.


\begin{algorithm}[H]
	\label{alg:FAST-LTS}
	\KwIn{$\boldsymbol{X} \in \mathbb{R}^{n \times p}, \boldsymbol{y} \in \mathbb{R}^{n \times 1}, m, l, s, max\_iteration, threshold $}
    \KwOut{ $\vec{\hat{w}_{final}}$, $H_{final}$ }
	\caption{FAST-LTS}
	\SetKw{Break}{break}
	$\what{{final}} \gets \emptyset$\;
	$H_{final} \gets \emptyset$\;
	$F_{best} \gets \emptyset$\;

	\uIf{$n \geq l$}{
		$F_{merged} \gets \emptyset$\;
		% for each split
		\For{$i \gets 0$ \textbf{to} $s$}{
			% create xx number of starts
			$F_{selected}  \gets \emptyset$\;
			\For{$j \gets 0$ \textbf{to} $\frac{l}{s}$}{
			  $F_{initial} \gets Selective~iteration(\frac{m}{l})$\;
			  % on each starts iterate few c steps
			  \For{$H_i$ \textbf{in} $F_{initial}$}{
				$H_i \gets Iterate~C~step~few~times(H_i)$\;
				$F_{selected} \gets  F_{selected} \cup \{{ H_i \}} $\;
			  }
			}
			% among all starts select 10 best and add it to merged set
			$F_{merged} \gets F_{merged} \cup Select~10~best~subsets~from~F_{selected}$\;
		  }
		
		  % for each, say 50 best, iterate few and add it to best
		  \For{$H_i$ \textbf{in} $F_{merged}$}{
			$H_i \gets Iterate~C~step~few~times(H_i)$\;
			$F_{best} \gets F_{best} \cup \{{ H_i \}} $\;
		  }
		  $F_{best} \gets  Select~10~best~subsets~from~F_{best} $\;
	}
	\Else{
		$F_{initial} \gets Selective~iteration(m)$\;
		$F_{best} \gets  Select~10~best~subsets~from~F_{initial} $\;
	}

	% iterate till convergence on few best final results
	$F_{final} \gets \emptyset$\;
	$W_{final} \gets \emptyset$\;
	\For{$H_i$ \textbf{in} $F_{best}$}{
			$H_i, \what{i} \gets Iterate~C~step~till~convergence(H_i, max\_iteration, threshold)$\;
			$F_{final} \gets F_{final} \cup \{{ H_i \}}$\;
			$W_{final} \gets W_{final} \cup \{{ \what{i} \}}$\;
	}

	% select one best result
	$\what{{final}}, H_{final} \gets select~what~with~best~RSS(F_{final}, W_{final})$\;

	\Return{ $\what{{final}}, H_{final}$  }\;
\end{algorithm}


% \begin{description}
% 	\item[BP] 
% 	\item[DP] 
% \end{description}



% ****************************************************************************************************
% ************************* FEASIBLE SOLUTION ******************************************************
% **************************************************************************************************
\section{Feasible solution} \label{section_feasible_solution}
In this section we'll introduce feasible solution algorithm from \cite{hawkins:1994}.
It is based on strong necessary condition we've described at \ref{strong_condition}. 
The basic idea can be described as follows.

Let's consider that we have some $\vec{m} \in Q^{(n,h)}$. It'll be convenient when we'll mark in the following text $O_m = \{{i \in  \set{ 1,2,\ldots , n } ;w_i = 1\}}$ and $Z_m = \{{j \in  \set{ 1,2,\ldots , n } ;w_j = 0\}}$ thus sets of indexes of positions where is $0$ respectively $1$ in vector $\vec{m}$. We can think about it as indexes of observations in $h$ set and $g$ set respectively. Then we can mark $\vec{m}^{(i,j)}$ as a vector which is constructed by swap of its ith and jth element where $i \in O$ and $j \in Z$. Such vector correspond to vector $\vec{m_{swap}}$ which we marked at \ref{strong_condition}.

With this in mind we can mark let's mark 
\begin{equation}
	\Delta S^{(\vec{m})}_{i,j} = \oflts(\vec{m}^{(i,j)}) - \oflts(\vec{m})
\end{equation}
thus change of the LTS objective function by swapping one observation from $h$ subset with another from $g$ subset. To calculate this we can obviously first calculate  $\oflts(\vec{m})$ and $ \oflts(\vec{m}^{(i,j)})$ and finally subtract both results. Although it is a option, it it computationally hard. So the question is if there is easier way of calculating $\Delta S^{(\vec{m})}_{i,j}$ and the answer is positive. 

Let's mark  
$M = diag(\vec{m}$ and 
$M^(i,j) = diag(\vec{m}^{(i,j)})$ and also
$\vec{H} = (\vec{X}^T\vec{M}^T\vec{X})$
For now let's assume that we already calculated
$\vec{H}^{-1}$ and also $\vec{\hat{w}} = \vec{H}^{-1}\vec{X}^T\vec{M}\vec{y}$
we now want to calculate $\Delta S^{(\vec{m})}_{i,j}$
Let's mark vector of residuals 
$\vec{r}^{(m)} = \vec{Y} - \vec{X} \vec{\hat{w}} $
and also $d_{r,s} = \vec{x_r} \vec{H}^{-1}  \vec{x_s} $
then by equation introduced in \cite{atkinson1991simulated} we get

\begin{equation} \label{hawkins:rovnice}
	\Delta S^{(\vec{m})}_{i,j} = 
	\frac{({r}^{(m)}_{j})^2(1-d_{i,i})- ({r}^{(m)}_{i})^2(1+d_{j,j}) + 2{r}^{(m)}_{i}{r}^{(m)}_{j}d_{j,j}}
	{(1-d_{i,i})(1+d_{j,j}) + d_{i,j}^2}.
\end{equation}

Let's now describe core of the algorithm. It's a  similar to the FAST-LTS algorithm in the sense of iterative refinement of $h$ subset. So let's assume that we have some vector  $\vec{m} \in Q^{(n,h)}$. No we'll compute  $\Delta S^{(\vec{m})}_{i,j}$ for all $i \in O$ and $j \in Z$. This may lead to several different outcomes

\begin{enumerate}
	\item all $S^{(\vec{m})}_{i,j}$ are non-negative
	\item one $S^{(\vec{m})}_{i,j}$ is positive
	\item multiple $S^{(\vec{m})}_{i,j}$ are positive
\end{enumerate}

In the first case, all $\oflts(\vec{m}^{(i,j)})$ are higher than $\oflts(\vec{m})$ so none swap will lead to an improvement. That also means that strong necessary condition is satisfied and the algorithm ends.

In the second and third case strong necessary condition is not satisfied and we make the swap. In second case it's easy which one to choose, because we have only one. in the third case we have couple of options again. 
\begin{enumerate}
	\item use the first swap that leads to the improvement (so don't event try to find different swap)
	\item from all possible swaps choose one that has highest improvement value  $\oflts(\vec{m}^{(i,j)})$
	\item use the first swap that has improvement higher than some given threshold
\end{enumerate}
In the terms of complexity all three options give the same results \todo{O(n2p) or O(n3p) remains}, because to find feasible solution you need to evaluate all pair swaps. In practice third options is winner because it'll lead to least amount of iterations. On the other hand as we said this won't improve complexity of the algorithm, so from now on let's assume that we'll use case number two.
So if there positive  $S^{(\vec{m})}_{i,j}$ we'll make the swap and repeat the process again. 
Algorithm ends when there is no possible improvement i.e. when all $S^{(\vec{m})}_{i,j}$ are non-negative.
Number of iterations needed till algorithm stops is usually quite low, but for practical usage it's still convenient to use some parameter $max\_iteration$ after which algorithm will stop without finding $h$ subset satisfying strong necessary condition. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\todo{put sem part of the pseudocode describing core iteration?}
\begin{observation} 
	Time complexity of algorithm C-step \ref{alg:Cstep} is the same as time complexity as OLS. Thus $O(p^2n)$
	\todo{create better proof. And take into account both versions - directly vs. using decomposition}
\end{observation} 


\begin{proof}
	In C-step we must compute $n$ absolute residuals. Computation of one absolute residual consists of
	matrix multiplication of shapes $1 \times p$ and $p \times 1$ that gives us $\mathcal{O}(p)$. Rest is in constant time.
	So time of computation $n$ residuals is $\mathcal{O}(np)$.
	Next we must select set of $h$ smallest residuals which can be done in $\mathcal{O}(n)$ using modification of algorithm QuickSelect. \todo{reference or define quick select} 
	Finally we must compute $\hat{w}$ OLS estimate on $h$ subset of data.
	Because $h$ is linearly dependent on $n$, we can say that it is $\mathcal{O}(p^2n + p^3)$ which 
	is asymptotically dominant against previous steps which are $\mathcal{O}(np + n)$.
\end{proof}

As we stated above, repeating algorithm C-step will lead to sequence of $\what{1}, \what{2} \ldots$ 
on subsets $H_1, H_2 \ldots$ with corresponding residual sum of squares
$RSS(\what{{1}}) \geq RSS(\what{{2}}) \geq \ldots$. One could ask if this sequence will converge, so that
$RSS(\what{{i}}) == RSS(\what{{i+1}})$. 
Answer to this question will be presented by the following theorem.


%******************************** C-STEP alg. will converge ***********************%
\begin{theorem}
	Sequence of C-step will converge to $\what{{m}}$ after maximum of $m = {n \choose h}$
	so that $RSS(\what{{m}}) == RSS(\what{{n}})\,, \forall n\geq m$ where $n$ is number of data samples 
	and $h$ is size of subset $H_i$.
\end{theorem}

\begin{proof}
	Since  $RSS(\what{{i}})$ is non-negative and $RSS(\what{{i}}) \leq RSS(\what{{i+i}})$ the 
	sequence will converge. $\what{{i}}$  is computed out of subset 
	$H_i \subset \{{1,2,\ldots,n\}}$. When there is finite number of subsets of size $h$ out of $n$ samples, namely ${n \choose h}$, the sequence will converge at the latest after this number of steps.
\end{proof}


\todo{REWRITE UP FROM THIS TO MATCH FEASIBLE SOLUTION}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
One run of this iteration process will lead to some local optima i.e. set satisfying strong necessary condition. In \cite{hawkins:1994} they refer to to this set as to \defterm{feasible set}.
This because it is not global optima the algorithm needs to be run multiple times say $N$ times. As a final solution is taken such $h$  subset with the smallest residual sum of squares. 

We didn't yet mention how to create initial $h$ subset respectively initial $\vec{m}$. We already had this discussion when describing FAST-LTS algorithm. The \cite{hawkins:1994} describes only random starting $h$ subsets, but using $p$ subsets instead may lead to improvement \todo{experiment with this and refer here}. More importantly as we already suggested $h$ subset satisfying weak necessary condition don't need to satisfy strong necessary condition so passing such a $h$ subset as input to this algorithm is another option and we'll discuss it in detail later. For that reasons we'll now describe feasible algorithm with pseudocode and we'll assume that we already have some function that generates for us $h$ subsets e.g. random one. 

\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\begin{algorithm}[H]
	\label{alg:feasible_solution}
		\KwIn{$\boldsymbol{X} \in \mathbb{R}^{n \times p}, \boldsymbol{y} \in \mathbb{R}^{n \times 1},  max\_iteration, N $}
		\KwOut{ $\vec{\hat{w}_{final}}$, $H_{final}$ }
		
	\caption{Feasible solution}
	\SetKw{Break}{break}
	$\vec{\hat{w}_{final}} \gets \emptyset$\;
	$H_{final} \gets \emptyset$\;
	$Results \gets \emptyset$\;

	\For{$k \gets 0$ \textbf{to} $N$}{
		$\vec{m} \gets generate\_intial\_subset()$  \tcp*{e.g. random $\vec{m} \in Q^{(n,h)}$}
		\While{$True$}{ 
			$best_i \gets 0$ \;
			$best_j \gets 0$ \;
			$best_{delta} \gets 0$ \;
			\For{$ i \in O_m$}{ 
				\For{$j \in Z_m$}{ 
					$delta \gets calculate \Delta S^{(\vec{m})}_{i,j}$\;
					\If{$delta >  best_{delta}$}{
						$best_{delta} \gets delta$\;
						$best_i \gets i$ \;
						$best_j \gets j$ \;
					}
				}
			}
			\If{$best_{delta} > 0$}{
				$\vec{m} \gets \vec{m}^{(i,j)}$\;
			}
			\Else {
				$H \gets h\ subset\ corresponding\ to\ \vec{m}$\;
				$\vec{M} \gets diag(\vec{m})$\;
				${\hat{w}} \gets \of^{(OLS,\vec{M}\vec{X},  \vec{M}\vec{y} )}$\;
				$Results \gets Results \cup \{ H, {\hat{w}} \}$\;
				\Break\;
			}
		}		
	}
	$H_{final}, \vec{\hat{w}_{final}}  \gets $ select best from $Results$ based on smallest $RSS$\;
	\Return{ $\what{{final}}$\,, $H_{final}$ }\;
\end{algorithm}


\todo{move some other place, and finish}
\section{Combined algorithm}
Here we'll describe what we've indicated above and that is combination of both previous algorithms.
Two options. 
z
\begin{enumerate}
	\item Let fast-lts converge and then run FSA and let it converge
	\item Let fast-lts converge make one step of FSA and try fast-LTS again and iterate between this
\end{enumerate}

Second option would be faster ? Let's think about it.. etc etc..


% **********************************************************************************
% ************************* EXACT POLYNOMIAL ALGORITHM *****************************
% **********************************************************************************
\section{Improved FSA} % two exchanging algorithms from AgullÃ³, 2000
So far as we've described FSA algorithm we assumed that after each cycle of the algorithm (after each best swap) wee need to recalculate inversion $(\vec{X}^T\vec{X})$ together with $\vec{\hat{w}}$. In this section we'll introduce different approaches described in \cite{agullo2001new} which will improve it so that we'll be able to update it instead of recalculate. Moreover these ideas will lead us to bounding condition of FSA which will improve the speed and we'll also be able to construct another algorithms based on this idea. 

We've introduced additive formula \ref{hawkins:rovnice}. Let's now try to obtain similar formula but let's try to focus on how individual elements in our current algorithm will change not only on the swap of two rows but how those elements will be affected after adding one row and also removing one row. Moreover during this derivation we'll be able to obtain two different approaches of calculating one thing. Both are important and we'll recapitulate them after.


Let $\vec{A} = (\vec{X}, \vec{y})$ be a matrix $\vec{X}$ expended by one column of corresponding dependent variables and $\vec{Z} = \vec{X}^T\vec{X}$ and $\vec{\tilde{Z}} = \vec{A}^T\vec{A}$. Then

\[  \numberthis \label{matrixZ}
	\vec{\tilde{Z}} = \begin{bmatrix}
		\vec{Z} & \vec{X}^T\vec{y} \\
    \vec{y}^T\vec{X} & \vec{y}^T\vec{y}
  \end{bmatrix} .
\]
Notice that $Z$ is symmetric square matrix $\in \mathbb{R}^ {p \times p}$ moreover we suppose that $Z$ is regular. $\vec{X}^T\vec{y}$ is column vector $\in \mathbb{R}^ {1 \times p}$, $\vec{y}^T\vec{X}$ is vector $\in \mathbb{R}^ {p \times 1}$ and $\vec{y}^T\vec{y}$ is scalar.
OLS estimate $\vec{\hat{w}}$ is then
\begin{equation}
	\vec{\hat{w}^{(OLS, n)}} = \vec{Z}^{-1} \vec{X}^T\vec{y}
\end{equation}
and residual sum of squares 

\begin{equation}
	RSS = \vec{y}^T\vec{y} - \vec{y}^T\vec{X}\vec{\hat{w}}
\end{equation}

Also note that all necessary multiplications are already in the matrix $\vec{\tilde{Z}}$.
Let's now realize that $RSS$ can  be expressed as ratio of determinants $\det(\vec{\tilde{Z}})$ and $\det(\vec{Z})$ (using determinant rule for block matrices) so that 

\begin{align*} \numberthis \label{rssdeterminant} 
	RSS &= \frac{\det(\vec{\tilde{Z}})}{\det(\vec{Z})} \\ &= \frac{
    \det\begin{pmatrix}
			\vec{Z} & \vec{X}^T\vec{y} \\
			\vec{y}^T\vec{X} & \vec{y}^T\vec{y}
		\end{pmatrix}
		}{\det\left(\vec{M}\right)}\\
		 &=\frac{ \det(\vec{Z}) \det\left(\vec{y}^T\vec{y} -  \vec{y}^T\vec{X} \vec{Z}^{-1} \vec{y}\right) }
		{\det\left(\vec{Z}\right)} \\ 
		&= \vec{y}^T\vec{y} - \vec{y}^T\vec{X}\vec{\hat{w}}.
\end{align*}

If we assume $RSS > 0$ then $\vec{\tilde{Z}}^{-1}$ can be expressed as 

\[ \numberthis
	\vec{\tilde{Z}}^{-1} = 
	\begin{bmatrix}
		\vec{Z}^{-1}+\dfrac{\vec{\hat{w}} \vec{\hat{w}}^{T}}{RSS} & - \dfrac{\vec{\hat{w}}^T}{RSS} \\[6pt]
		- \dfrac{\vec{\hat{w}}}{RSS} & \dfrac{1}{RSS}
	\end{bmatrix}.
\]
For following equations it's important to notice that for any two observations $z_i = (x_i, y_i)$  and $\vec{z_j} = (\vec{x_j}, y_j)\, \vec{z_i}, z_i \in \mathbb{R}^{p+1 \times 1}$ is 

\begin{equation}
	\vec{z_i} \vec{\tilde{Z}}^{-1} \vec{z_i^T} = \dfrac{ ( y_i - \vec{x_i}\vec{\hat{w}} )^2 }{RSS}  + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}
\end{equation}
and
\begin{equation}
	\vec{z_j} \vec{\tilde{Z}}^{-1} \vec{z^T} = \dfrac{ ( y_j - \vec{x_j}\vec{\hat{w}} ) ( y_i - \vec{x_i}\vec{\hat{w}} ) }{RSS}  + \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}
\end{equation}
Using above let's express how the determinant $\det(\vec{Z})$ and inverse of $vec{Z}^{-1}$ will change when we'll add one observation $\vec{z_i} = (\vec{x_i}, y_i) \in \mathbb{R}^{p+1 \times 1} $ to the matrix $\vec{A}$. First lets notice that if we add this row to $\vec{A}$, then  $\vec{Z}$ will change 

\[ \numberthis \label{addedrow}
\begin{bmatrix}
	x_{11} & x_{12} & \dots  & x_{1n} & x_{1i}  \\
	x_{21} & x_{22} & \dots  & x_{2n} & x_{2i} \\
	\vdots & \vdots & \vdots & \ddots & \vdots \\
	x_{p1} & x_{p2} & \dots  & x_{pn} & x_{pi} 	
\end{bmatrix}
\begin{bmatrix}
	x_{11} & x_{12}  & \dots  & x_{1p} \\
	x_{21} & x_{22}  & \dots  & x_{2p} \\
	\vdots  & \vdots & \ddots & \vdots \\
	x_{n1} & x_{n2}  & \dots  & x_{np} \\
	x_{i1} & x_{i2}  & \dots  & x_{ip}
\end{bmatrix}
 = \vec{X}^T\vec{X} + \vec{x_i^T}\vec{x_i} = \vec{Z} + \vec{x_i^T}\vec{x_i},
\]
so determinant with appended row will be
\begin{equation} \label{udpateddeterminant}
	\det(\vec{Z} + \vec{x_i^T}\vec{x_i}) = det(\vec{Z})(1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T})
\end{equation}
and inversion can be obtained using Sherman-Morrison formula \cite{bartlett1951inverse} so that 
\begin{equation} \label{shermanmorris}
	(\vec{Z} + \vec{x_i^T}\vec{x_i})^{-1} = \vec{Z}^{-1} - \dfrac{\vec{Z}^{-1}\vec{x_i^T}\vec{x_i}\vec{Z}^{-1}}{1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}}
\end{equation}
For following equations it'll be convenient for us to mark 
\begin{equation}
	b = \dfrac{-1}{(1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T})},  b \in \mathbb{R},
\end{equation}
and 
\begin{equation} \label{agullo_u}
	\vec{u} = \vec{Z}^{-1}\vec{x_i^T},  	\vec{u} \in \mathbb{R}^{p \times 1},
\end{equation}
so that \ref{shermanmorris} can be written as
\begin{equation} \label{inversionplus}
	(\vec{Z} + \vec{x_i^T}\vec{x_i})^{-1} = \vec{Z}^{-1} + b\vec{u}\vec{u^T}.
\end{equation}
Given that last piece that we're missing to express updated $\vec{\hat{w}}$ is appended $\vec{X}^T\vec{y}$ by one row, which can be simply expressed by same idea as \ref{addedrow} so that updated $\vec{\hat{w}}$ which we'll denote as $\vec{\overline{\hat{w}}}$ is 
\begin{equation}
	\vec{\overline{\hat{w}}} = (\vec{Z}^{-1} + b\vec{u}\vec{u^T})(\vec{X^T}\vec{y} + y_i\vec{x_i^T}).
\end{equation}
This can be simplified so that we get
\todo{mistake in original paper where is w + (y - xw)bu}
\begin{equation} \label{thetaplus}
	\vec{\overline{\hat{w}}} = \vec{\hat{w}} - (y_i - \vec{x_i}\vec{\hat{w}})b\vec{u}.
\end{equation}
Last but not least we want to express updated $RSS$ which we denote as $\overline{RSS}$. This can be done easily from \ref{rssdeterminant} and \ref{udpateddeterminant} as 

\begin{equation}
	\overline{RSS} =  RSS + \dfrac{(y_i - \vec{x_i}\vec{\hat{w})^2}}{(1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T})}
\end{equation}
and again, it's convenient to mark 

\begin{equation}
	\gamma^{+}(\vec{z_i}) = \dfrac{(y_i - \vec{x_i}\vec{\hat{w})^2}}{(1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T})},
\end{equation}
so that 
\begin{equation}
	\overline{RSS} =  RSS + \gamma^{+}(\vec{z_i})
\end{equation}
We can see that $\gamma^{+}(\vec{z_i})$ measures how $RSS$ increase after we append our dataset with observation $\vec{z_i}$, thus we can see that $\gamma^{+}(\vec{z_i}) \geq 0$

Because we want to express both increment and decrement change in our dataset, let's now focus how $RSS$, $\vec{Z^{-1}}$ and $\vec{\hat{w}}$ will change after we exclude one observation. 

Consider that we've already included one observation $z_i$ in our dataset and mark $\vec{\overline{Z}} = \vec{Z} + \vec{x_i} \vec{x_i^T} $ If we exclude one observation $\vec{z_j} = (\vec{x_j}, y_j) \in \mathbb{R}^{p+1 \times 1} $ from already updated matrix $\vec{A}$ then determinant $\det(\vec{\overline{Z}})$  will change as

\begin{equation} 
	\det(\vec{\overline{Z}} - \vec{x_j^T}\vec{x_j}) = det(\vec{\overline{Z}})(1 - \vec{x_j}\vec{\overline{Z}}^{-1}\vec{x_j^T})
\end{equation}
and inversion will change (again according to Sherman-Morrison formula) as 
\begin{equation}  \label{updatedinversion2}
	(\vec{\overline{Z}} - \vec{x_j^T}\vec{x_j})^{-1} = \vec{\overline{Z}}^{-1} + \dfrac{\vec{\overline{Z}}^{-1}\vec{x_j^T}\vec{x_j}\vec{\overline{Z}}^{-1}}{1 - \vec{x_j}\vec{\overline{Z}}^{-1}\vec{x_j^T}}.
\end{equation}
Once again, it's convenient to denote
\begin{equation}
	\overline{b} = \dfrac{-1}{(1  \vec{x_j}\vec{\overline{Z}}^{-1}\vec{x_j^T})},  \overline{v} \in \mathbb{R},
\end{equation}
and 
\begin{equation}
	\vec{\overline{u}} = \vec{\overline{Z}}^{-1}\vec{x_j^T},  	\vec{\overline{u}} \in \mathbb{R}^{p \times 1},
\end{equation}
so that we can write
\begin{equation}
	(\vec{\overline{Z}} + \vec{x_j^T}\vec{x_j})^{-1} = \vec{Z}^{-1} - \overline{b}\vec{\overline{u}}\vec{\overline{u}^T}.
\end{equation}
Using same approach as before we can denote express downdated estimate which we denote as $\vec{\overline{\overline{\hat{w}}}}$

\begin{equation} \label{thetaminus}
	\vec{\overline{\overline{\hat{w}}}} =  +\vec{\overline{\hat{w}}} (y_j - \vec{x_j}\vec{\overline{\hat{w}}}) \overline{b} \vec{\overline{u}}.
\end{equation}

Finally lets also express updated $\overline{RSS}$ which we denote as $\overline{\overline{RSS}}$. This can be done easily from \ref{rssdeterminant} and \ref{udpateddeterminant} and \ref{updatedinversion2} as 

\begin{equation}
	\overline{\overline{RSS}} =  \overline{RSS} - \dfrac{(y_j - \vec{x_j}\vec{\overline{\hat{w}})^2}}{(1 - \vec{x_j}\vec{\overline{{Z}}}^{-1}\vec{x_j^T})}
\end{equation}
and let's also mark

\begin{equation}
	\gamma^{-}(\vec{z_j}) = \dfrac{(y_j - \vec{x_j}\vec{\overline{\hat{w}})^2}}{(1 - \vec{x_j}\vec{\overline{{Z}}}^{-1}\vec{x_j^T})},
\end{equation}
so that 
\begin{equation}
	\overline{\overline{RSS}} =  \overline{RSS} - \gamma^{-}(\vec{z_j})
\end{equation}

Lets now express equation for including and removing observation at once. First let's notice that from \ref{updatedinversion2} we can express 

\begin{equation} \label{xjoverlinez}
	\vec{x_j}\vec{\overline{{Z}}}^{-1}\vec{x_j^T}) =  \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}) - 
	\dfrac{( \vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2}{1 +  \vec{x_i}\vec{Z}^{-1}\vec{x_i^T})},
\end{equation}
 so that 

 \begin{gather}
 \begin{align*} \numberthis
	&\det(\vec{Z} + \vec{x_i^T}\vec{x_i} - \vec{x_j^T}\vec{x_j}) = \\
	\det(\vec{Z})(1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T} -& \vec{x_j}\vec{Z}^{-1}\vec{x_j^T} +  ( \vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2 - \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}\vec{x_j}\vec{Z}^{-1}\vec{x_j^T} )
\end{align*}
\end{gather}

Finally we can express $\overline{\overline{RSS}} $ as

\begin{equation} \label{updaterss}
	\overline{\overline{RSS}}  = RSS\rho(z_i, z_j),
\end{equation}
where
\begin{equation} \label{agullo:rovnice}
	\rho(z_i, z_j) =
	 \dfrac
	 {(1+\vec{x_i}\vec{Z}^{-1}\vec{x_i^T} + \dfrac{e_j^2}{RSS})
		(1 - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T} - \dfrac{e_i^2}{RSS} )+
		(\vec{x_i}\vec{Z}^{-1}\vec{x_j^T} + \dfrac{e_i e_j}{RSS} )^2}
	{1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}  - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}  + ( \vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2 -   \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}\vec{x_j}\vec{Z}^{-1}\vec{x_j^T} },
\end{equation}
where $e_i = y_i - x_i\vec{\hat{w}}$ and $e_j = y_j - x_j\vec{\hat{w}}$.

We can see that this formula is similar to the one \ref{hawkins:rovnice} but here the $\rho(z_i, z_j)$ represents multiplicative increment. Moreover $0 < \rho(z_i, z_j) $ and if $0 < \rho(z_i, z_j)< 1 $ then the swap leads to improvement in terms of feasible solution.

With all this we are now able to change FSA so that we don't need to recompute $\vec{\hat{w}}$ and inversion $(\vec{X}^T \vec{X})^{-1}$ but we can update it. That means after we find $z_i, z_j$ that improve the current RSS the most (that means we find smallest $\rho(z_i, z_j)$ by \ref{agullo:rovnice}). We can then update $RSS$ by \ref{updaterss}, update $(\vec{X}^T \vec{X})^{-1}$ by \ref{inversionplus} and \ref{updatedinversion2} and finally update $\vec{\hat{w}}$ by \ref{thetaplus} and \ref{thetaminus}.

Note that right now it doesn't matter if we use \ref{hawkins:rovnice} with stopping criterion $	\Delta S^{(\vec{m})}_{i,j} \geq 0$  or \ref{agullo:rovnice} and use $\rho(z_i, z_j) \geq 1 $ stopping criterion. With both results we can update $RSS$. But the advantage of \ref{agullo:rovnice} is that we can use following bounding condition to improve performance.

$\rho(z_i, z_j)$ is expressed as ratio. We can see that in numerator we have

\begin{equation}
	(1+\vec{x_i}\vec{Z}^{-1}\vec{x_i^T} + \dfrac{e_j^2}{RSS})
		(1 - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T} - \dfrac{e_i^2}{RSS} )+
		(\vec{x_i}\vec{Z}^{-1}\vec{x_j^T} + \dfrac{e_i e_j}{RSS} )^2
\end{equation}

and because $ \dfrac{e_i e_j}{RSS} )^2 \geq 0$ then whole numerator is greater or equal to

\begin{equation}
	(1+\vec{x_i}\vec{Z}^{-1}\vec{x_i^T} + \dfrac{e_j^2}{RSS})
(1 - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T} - \dfrac{e_i^2}{RSS} ).
\end{equation}

On the other hand we can see that denominator is

\begin{equation}
	1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}  - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}  + (\vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2 -   \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}\vec{x_j}\vec{Z}^{-1}\vec{x_j^T} 
\end{equation}

and because $(\vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2$ and $\vec{x_i}\vec{Z}^{-1}\vec{x_i^T}\vec{x_j}\vec{Z}^{-1}\vec{x_j^T} $ are actually inner products of $x_i$ and $x_j$  ($\vec{Z}^{-1}$ is positive definite) so 
\begin{equation}
	(\vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2 \leq \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}\vec{x_j}
\end{equation}
using Cauchy-Schwarz inequality. That means that denominator is less or equal to
\begin{equation}
	1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}  - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}.
\end{equation}
Given that we can denote $\rho_b(z_i, z_j)$ so that


\begin{equation} \label{boundingcondition}
	\rho_b(z_i, z_j) = \dfrac{(1+\vec{x_i}\vec{Z}^{-1}\vec{x_i^T} + \dfrac{e_j^2}{RSS})
	(1 - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T} - \dfrac{e_i^2}{RSS} )}{1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}  - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}} \quad \leq \rho(z_i, z_j)
\end{equation}

So the actual speed improvement is given by we don't need to compute $\vec{x_i}\vec{Z}^{-1}\vec{x_j^T}$ in each of $h(n-h)$ pairs swap comparison. Every time that algorithm starts to compare all pairs we set $\rho_{min} :=1$. Then every time we only compute $\rho_b(z_i, z_j)$ and if it is greater of equal to $\rho_{min}$ we can continue to next pair without computing $\rho(z_i, z_j)$. On the other hand if $\rho_b(z_i, z_j)$ is less that  $\rho_{min}$ we compute $\rho(z_i, z_j)$ and set $\rho_{min} := \rho(z_i, z_j)$ This of course won't improve $\mathcal{n^2}$ time required to compare all $z_i, z_j$ pairs.
Finally lets note that \cite{agullo2001new} call FSA  which compute $\rho(z_i, z_j)$  \ref{agullo:rovnice} as \defterm{Optimal exchange algorithm (OEA)} and OEA using bounding condition \ref{boundingcondition} as  \defterm{Modified optimal exchange algorithm (MOEA)}.

\subsubsection{Different method of computing Improved FSA}
We've described way how to modify FSA so that we can update $\vec{hat{w}}$ and inversion $(\vec{X}^T \vec{X})^{-1}$. This requires to compute inversion at the start of the algorithm (this is also the case for the FSA). In practice, however, this is not the way how OLS estimate $\vec{\hat{w}}$ is computed. That is primarily due to the fact that computing inversion of large matrix is computationally exhaustive and first of all not numerically stable. In practice we usually use Cholesky factorization. In this subsection we'll describe it and also show that we can perform all above computations using such decomposition. Moreover we'll show close connection between Cholesky factorization and QR decomposition.

\begin{definition}
	Let $\vec{A} \in \mathbb{R}^{n \times n}$ be an symmetric positive definite matrix. Then it is possible to find lower triangular matrix $L$ so that 

	\begin{equation}
		\vec{A} = \vec{L}\vec{L^T}
	\end{equation}
	We call this decomposition as \defterm{Cholesky factorization} (sometimes also  \defterm{Cholesky decomposition})
\end{definition}
	
If we look at our problem of finding solution to 
\begin{equation}
	\vec{X}^T\vec{X}\vec{w} = \vec{X}^T\vec{y}
\end{equation}
We can easily rewrite it as 
\begin{equation}
	\vec{Z}\vec{w} = \vec{b}
\end{equation}
where $\vec{Z} = \vec{X}^T\vec{X}$ is symmetric positive definite matrix and $\vec{b} = \vec{X}^T\vec{y} $. Because $\vec{Z}$ can be factorized as $\vec{L}\vec{L^T}$ the solution can be found easily by substitution

\begin{align}
	\vec{L}\vec{d} = \vec{b} \\
	\vec{L}^T\vec{w} = \vec{d}.
\end{align}
where $\vec{d}$ is solved by forward substitution and $\vec{w}$ is then solved by backward substitution which is our $\vec{\hat{w}}$ estimate.

So the algorithm for solving OLS using Cholesky factorization goes as follows:
\begin{enumerate}
  \item Calculate $\vec{X}^T\vec{X}$ and   $\vec{X}^T\vec{y}$ 
  \item Calculate Cholesky factorization $\vec{Z} = \vec{L}\vec{L}^T$ where $\vec{Z} = \vec{X}^T\vec{X}$
  \item Solve lower triangular system $\vec{L}\vec{d} = \vec{b}$  where $ \vec{b} = \vec{X}^T\vec{y}$  for $\vec{d}$ using forward substitution.
  \item Solve upper triangular system $	\vec{L}^T\vec{w} = \vec{d}$ for $\vec{w}$
\end{enumerate}

\begin{observation}
	Time complexity of solving OLS using Cholesky factorization is $\mathcal{O}(np^2)$
\end{observation}
\begin{proof}
	First step requires two matrix multiplication $\vec{X}^T\vec{X}$ which is $\mathcal{O}(np^2)$ and  $\vec{X}^T\vec{y}$ which is $\mathcal{O}(np)$.
	
	Second steps is calculating Cholesky factorization. This can be done in $\mathcal{O}(\frac{1}{3}p^3)$ \cite{krishnamoorthy2013matrix}

	In the third and fourth step we are solving triangular systems both of them require
	 $\mathcal{O}(\frac{1}{2}p^2)$, so together $\mathcal{O}(p^2)$

	Putting all steps together we get  
	\begin{equation} \label{timecholeskywhole}
		\mathcal{O}(np^2 + np + \frac{1}{3}p^3 + p^2)
	\end{equation}
	and because we assume $n \geq p$ and most usually  $n \gg p$ then multiplication $\vec{X}^T\vec{X}$ asymptotically dominates so whole time complexity is $\mathcal{O}(np^2)$
\end{proof}
\begin{note}
	In case we have $\vec{X}^T\vec{X}$ and  $\vec{X}^T\vec{y}$ computed in advance, then it is asymptotically dominated by Cholesky factorization 
	\begin{equation} \label{timecholesky}
		\mathcal{O}(\frac{1}{3}p^3) \sim \mathcal{O}(p^3)
	\end{equation}
\end{note}

Let's now look on different method of computing OLS which don't require multiplying $\vec{X}^T\vec{X}$.

\begin{definition}
	Let $\vec{A} \in \mathbb{R}^{n \times n}$ be any square matrix. Then it is possible to find matrices $\vec{Q}$ and $\vec{R}$ so that 

	\begin{equation}
		\vec{A} = \vec{Q}\vec{R}
	\end{equation}
	Where $\vec{Q} \in \mathbb{R}^{n \times n}$ is orthogonal matrix and $\vec{R} \in \mathbb{R}^{n \times n}$ is upper triangular matrix. 
	We call this decomposition as \defterm{QR decomposition}.
\end{definition}
If matrix $\vec{A} \in \mathbb{R}^{m \times n}$ thus is not square, then decomposition can also be found as

	\begin{equation}
		\vec{A} = \vec{Q}\vec{R} = \vec{Q} \begin{bmatrix}
			\vec{R_1} \\
			\vec{0}
		\end{bmatrix}
		=  \begin{bmatrix}
			\vec{Q_1} & \vec{Q_2}
		\end{bmatrix}  \begin{bmatrix}
			\vec{R_1} \\
			\vec{0}
		\end{bmatrix} 
		= \vec{Q_1} \vec{R_1}
	\end{equation}
	Where $\vec{Q} \in \mathbb{R}^{m \times m}$ is orthogonal matrix, $\vec{R_1} \in \mathbb{R}^{n \times n} $ is upper triangular matrix, $\vec{0} \in \mathbb{R}^{ (m-n) \times n} $  is zero matrix,  Where $\vec{Q_1} \in \mathbb{R}^{n \times n}$ is matrix with orthogonal columns and  $\vec{Q_2} \in \mathbb{R}^{(m-n) \times n}$ is also matrix with orthogonal columns.
	
So that means that $\vec{X}$ can be factorized so that 
\begin{equation}
	\vec{X} = \vec{Q}\vec{R}.
\end{equation}
Then
\begin{equation}
	\vec{X}^T\vec{X} = (\vec{Q}\vec{R})^T \vec{Q}\vec{R} = \vec{R}^T\vec{Q}^T\vec{Q}\vec{R}
\end{equation}
and because $\vec{Q}$ is orthogonal, then $\vec{Q}^T\vec{Q} = \vec{I}$ so that
\begin{equation}
	\vec{X}^T\vec{X} = \vec{R}^T\vec{R}.
\end{equation}
Because $\vec{X} \in  \mathbb{R}^{n \times p}$ is not square matrix then as we shown $\vec{R} = \begin{bmatrix}
	\vec{R_1} \\
	\vec{0}
\end{bmatrix}$
so that 
\begin{equation}
	\vec{X}^T\vec{X} = \vec{R_1}^T\vec{R_1}.
\end{equation}
 Where $\vec{R_1^T}$ is lower triangular matrix. 
%  More importantly 
%  \begin{equation}
% 	\vec{R_1^T}<C = \vec{L}
% \end{equation}
Solution to
\begin{equation}
	\vec{X}^T\vec{X}\vec{w} = \vec{X}^T\vec{y}
\end{equation}
is then given by 
\begin{equation}
	\vec{R_1}^T\vec{R_1}\vec{w} = \vec{R_1}^T\vec{Q_1}^T \vec{y}
\end{equation}
and because we assume $\vec{X}$ have full column rank, thus is invertible, we can simplify it to
\begin{equation}
	\vec{R_1}\vec{w} = \vec{b_1}
\end{equation}
where $ \vec{b_1} = \vec{Q_1}^T \vec{y}$. Because $\vec{R_1}$ is upper triangular matrix, solution of $\vec{w}$ is trivial using backward substitution. 

\begin{note}
		We can see that Cholesky factorization  $\vec{X}^T\vec{X} = \vec{L}\vec{L}^T$ is closely connected to the QR decomposition $\vec{X} = \vec{Q_1}\vec{R_1}$ so that  
		\begin{equation} \label{qrcholesky}
				\vec{R_1^T} = \vec{L}
			\end{equation}
\end{note}

So the algorithm for solving OLS using QR decomposition can goes as follows:
\begin{enumerate}
  \item Calculate QR decomposition $\vec{X} = \vec{Q}\vec{R}^T = \vec{Q_1}\vec{R_1}^T$ 
  \item Calculate $\vec{Q_1}^T \vec{y}$ which we denote as $\vec{b_1}$ 
  \item Solve upper triangular system $	\vec{R}\vec{w} = \vec{b_1}$ for $\vec{w}$
\end{enumerate}

Today there are two popular methods of computing QR decomposition these are Householder transformations and Givens rotations. We'll describe Givens rotation in detail later. 
\begin{observation}
	Time complexity of solving OLS using QR decomposition is $\mathcal{O}(2np^2 - \frac{2}{3}p^3)$
\end{observation}
\begin{proof}
	
	In the first step we calculate QR decomposition. Householder rotations can be done in $\mathcal{O}(2np^2 - \frac{2}{3}p^3)$  \todo{nevim kde citovat} which is slightly faster than Givens rotation $\mathcal{O}(3np^2 - p^3)$ \todo{nevim kde citovat, ale asi mozna spocitam}
	Second step consist of matrix multiplication $\vec{Q_1}^T \vec{y}$ which is $\mathcal{O}(np)$
	In the last step we are solving upper triangular systems which is
	 $\mathcal{O}(\frac{1}{2}p^2)$.

	Putting all steps together we get  
	\begin{equation} \label{timewholeQR}
		\mathcal{O}(2np^2 - \frac{2}{3}p^3 + np + \frac{1}{2}p^2)
	\end{equation}
	We can see that again $\mathcal{O}(2np^2)$ asymptotically dominates.
\end{proof}

% \begin{equation}
% 	\det(\vec{Z} + )
% 	\vec{z_j} \vec{\tilde{Z}}^{-1} \vec{z^T} = \dfrac{ ( y_j - \vec{x_j}\vec{\hat{w}} ) ( y_i - \vec{x_i}\vec{\hat{w}} ) }{RSS}  + \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}
% \end{equation}
\todo{cele toto presunout nekam do prvni kapitoly}
\todo{rozdelit kapitoly nejak inteligentne. tohle je spatny...}

\subsubsection{Calculation of imporved FSA using QR}
So let's focus on our problem. As we said using decomposition is better than calculating inversion primarily due to higher numerical stability. \todo{nevim jestli zminovat ze muzu delat i SVD dekompozici ktera je nejstabilnejsi ze vsech...}
We'll show that both FSA as well as improved FSA can be calculated using QR decomposition.

Let's start with \ref{agullo:rovnice}. Here inversion is need to calculate 
$\vec{x_i}\vec{Z}^{-1}\vec{x_i^T}$, $\vec{x_j}\vec{Z}^{-1}\vec{x_j^T}$ and $\vec{x_i}\vec{Z}^{-1}\vec{x_j^T}$. This can also be done without inversion, only using the decomposition. We can write this equation
\begin{equation} \label{solveimi}
	\vec{x_i}\vec{Z}^{-1}\vec{x_i^T} = \vec{v}^T\vec{v} 	\iff \vec{x_i}(\vec{R}^T\vec{R})^{-1}\vec{x_i^T} = \vec{v}^T\vec{v}
\end{equation} 
where $\vec{v}$ can be obtained by solving lower triangular system
\begin{equation} \label{vsolution}
	\vec{R}^T\vec{v} = \vec{x_i}^T.
\end{equation} 
The same can be done with  $\vec{x_j}\vec{Z}^{-1}\vec{x_j^T}$. Last but not least we need to solve 
$\vec{x_i}\vec{Z}^{-1}\vec{x_j^T}$. We can write this 
\begin{equation}
	\vec{x_i}\vec{Z}^{-1}\vec{x_j^T} = \vec{x_j}\vec{u} 	\iff \vec{x_i}(\vec{R}^T\vec{R})^{-1}\vec{x_j^T} = \vec{x_j}^T\vec{u}
\end{equation} 
Where column vector $\vec{u}$ is defined by \ref{agullo_u}. We can see that 

\begin{equation}
	\vec{u} = \vec{Z}^{-1}\vec{x_i^T} 	\iff (\vec{R}^T\vec{R})^{-1}\vec{x_i^T} = \vec{u}
\end{equation}
so that $\vec{u}$ can be obtained ba solving upper triangular system 

\begin{equation} \label{solve_u_qr}
	\vec{R}\vec{u} = \vec{v}
\end{equation}
where $\vec{v}$ is solution of \ref{vsolution}. Other quantities of \ref{agullo:rovnice} does not require $\vec{Z}^{-1}$.

When optimal exchange $\vec{z_i}, \vec{z_j}$ is found then we need to update $RSS$ which can be done same way by \ref{updaterss}. Updating $\vec{\hat{w}}$ to  $\vec{\overline{\hat{w}}}$ by \ref{thetaplus} requires
$\vec{u}$  which in this case we calculate by \ref{solve_u_qr} and $b$ which requires  $\vec{x_j}\vec{Z}^{-1}\vec{x_j^T}$ but that we've also already covered by idea \ref{solveimi}. Analogous operations can be used to update $\vec{\overline{\hat{w}}}$ to $\vec{\overline{\overline{\hat{w}}}}$  by means of \ref{thetaminus}. Only thing wee need to realize that we can express \ref{xjoverlinez} as $\vec{x_j}\vec{\overline{{Z}}}^{-1}\vec{x_j^T} = \vec{x_i}\vec{{{Z}}}^{-1}\vec{x_i^T} + b(\vec{u}\vec{x_j}^2)$.

We can't use updating inversion because we don't have one so we need to somehow update our QR decomposition. This can be done by both Householder rotation as well as by Givens rotation, but because our $\vec{R}$ is already sparse matrix, Givens rotations are ideal tool for this.
We can use first \ref{addingrowqr} to add row $\vec{x_i}$ to the decomposition and consequently \ref{removingrow} to remove $\vec{x_j}$  from the decomposition. We can see that this is slower  than updating inversion directly. On the other hand this solution is numerically stable and requires less time than computing factorization again from scratch. 
% After such update we need also to recalculate $\vec{Q}\vec{y}$ which is 

Finally let's talk about matrix $\vec{\tilde{Z}}$ which we used \ref{matrixZ} for derivation of our equations. If we use \ref{qrcholesky} observation then we realize that if we make QR factorization of $\vec{A} = (\vec{X}, \vec{y})$ then

\begin{equation}
	\vec{\tilde{Z}} = 
	\begin{bmatrix}
		\vec{Z} & \vec{X}^T\vec{y} \\
    \vec{y}^T\vec{X} & \vec{y}^T\vec{y}
	\end{bmatrix} 
	= 
	\begin{bmatrix}
		\vec{R^T} & 0 \\
    \vec{\phi^T} & r
	\end{bmatrix} 
	\begin{bmatrix}
		\vec{R} & \vec{\phi} \\
     0 & r
	\end{bmatrix} 
\end{equation}

where 

\begin{equation}
	\vec{\tilde{R}} = 
	\begin{bmatrix}
		\vec{R} & \vec{\phi} \\
     0 & r
	\end{bmatrix} 
	= \vec{\tilde{Q^T}}\vec{A}
\end{equation}

is matrix from QR factorization of $\vec{A}$. 
Next we can realize that $\vec{R} \in \mathbb{R}^{p \times p}$ is actually matrix $\vec{R}$ which we can obtain by QR factorization of $\vec{X}$. 
Moreover $\vec{\phi} \in \mathbb{R}^{p \times 1}$ is column vector which is actually equal to
\begin{equation}
	\vec{\phi} =  \vec{Q^T}\vec{y}
\end{equation}
where $\vec{Q}$ is matrix $\vec{Q}$ from QR factorization of $\vec{R}$.
Due to this fact $\vec{\hat{w}}$ is solution of upper triangular system 
\begin{equation}
	\vec{R}\vec{\hat{w}} = \vec{\phi}
\end{equation}

Finally $r \in \mathbb{R}$ is scalar such that 

\begin{equation}
	r^2 = \vec{y}^T\vec{y} - \vec{y}^T\vec{X}\vec{\hat{w}} = RSS
\end{equation}

\begin{remark} \label{qnotrequiredspeedupremark}
	We can see that all quantities we use in the algorithm can be obtained from matrix $\vec{\tilde{R}}$ moreover if we didn't need to remove rows from $\vec{X}$ then matrix $\vec{\tilde{Q}}$ would not be need \ref{qnotrequired}, thus speedup while inserting and rows would be possible. 
\end{remark}

Now we've described all properties of improved FSA algorithm. We can see that two different methods of computation are possible. One, using inversion $\vec{Z}^{-1}$ is faster, but numerically less stable, second using QR decomposition is slower due to updating matrix $\vec{R}$ and matrix $\vec{Q}$. We've also shown that using matrix $\vec{\tilde{R}}$ is useful because it contains all necessary quantities for the improved FSA algorithm, although it won't improve the speed of the algorithm. Later we'll introduce different algorithm where we won't need to remove rows from $\vec{X}$ so then we'll be able to use this observation in our favor.

\todo{time complexity inverze}
\todo{time complexity QR}
\todo{time complexity tildeZQR is the same as QR}






















\todo{tento text prepoklada ze jsme givens uz popsali. pritom je ale pospany az v dalsi sekci!}
\subsubsection{Updating QR decomposition using Givens rotations}
First of all we need to introduce idea of Givens rotations and how it can be used to calculate QR decomposition. Then it won't be hard to describe how updating for both including and excluding row are done.

Computing QR Factorization.
We compute the QR factorization of $\vec{A} \in \mathbb{R}^{m \times n}$ which has full column rank so that we apply orthogonal transformation by matrix $\vec{Q}^T$ so that \cite{hammarling2008updatingqr}

\begin{equation}
	\vec{Q}^T\vec{A} = \vec{R}
\end{equation}
 
where $\vec{Q}$ is product of orthogonal matrices. One such example of orthogonal matrix can be:

\begin{equation} \label{givensmatrix}
\vec{Q} = 
\begin{bmatrix} 
\cos(\varphi) & \sin(\varphi) \\
- \sin(\varphi) & \cos(\varphi)
\end{bmatrix}
\end{equation}
This matrix is indeed orthogonal since 
\begin{align*} 
\vec{Q}\vec{Q}^T = \vec{Q}^T\vec{Q}    
\end{align*}
 \begin{equation*}
	= \begin{bmatrix} 
		\cos(\varphi)^2 + \sin(\varphi)^2  & \sin(\varphi)\cos(\varphi) - \cos(\varphi)\sin(\varphi) \\
		\cos(\varphi)\sin(\varphi) - \sin(\varphi)\cos(\varphi) & \cos(\varphi)^2 + \sin(\varphi)^2
	\end{bmatrix} \\ = \vec{I}
	\numberthis
\end{equation*}
Moreover if we multiply this orthogonal with some column vector $\vec{x} \in \mathbb{R}^{2 \times 1}$ thus $\vec{Q}\vec{x}$ as a result we'll get vector of same length but is rotated clockwise by  $\varphi$ radians. When we say that vector will have same length we mean than L-2 norm of such vector will be the same.This is important property of all orthogonal matrices. We can simply verify correctness of this claim. Let's have any orthogonal matrix $\vec{Q} \in \mathbb{R}^{m \times m}$ and any column vector $\vec{x} \in  \mathbb{R}^{m \times 1}$ then

\begin{equation}
	\norm{\vec{Q}\vec{x}}^2 = (\vec{Q}\vec{x})^T (\vec{Q}\vec{x}) = \vec{x}^T\vec{Q}^T\vec{Q}\vec{x} = 
	\vec{x}^T\vec{I}\vec{x} = \vec{x}^T\vec{x} = \norm{\vec{x}}^2
\end{equation}

So we would like to create series of orthogonal matrices which will gradually rotate column vectors of $\vec{A}$ so there will be zeros under diagonal. One such method - \defterm{Givens rotation} uses \defterm{Givens matricies} that will zero one element under diagonal at a time.
The example of \ref{givensmatrix} was not random. Let's look at this orthogonal matrix one more time and lets multiply this matrix with some vector.

\begin{equation}
	\begin{bmatrix} 
		\cos(\varphi) & \sin(\varphi) \\
		- \sin(\varphi) & \cos(\varphi)
		\end{bmatrix}
		\begin{bmatrix} 
			a \\
			b
			\end{bmatrix} = 
			\begin{bmatrix} 
				r \\
				z
				\end{bmatrix}
\end{equation}

To introduce this zeroing effect we need to rotate this vector so that it will be parallel to $
\begin{bmatrix} 
	1 \\
	0
\end{bmatrix}
$. Now it's only up to find  $\cos(\varphi) a + \sin(\varphi) = r $.
As we know rotation with $\vec{Q}$ preserve L-2 norm. That means is we want to $z = 0$  then $r$ must be equal to L-2 norm of vector  
$
\begin{bmatrix} 
	a \\
	b
\end{bmatrix}
$  thus $r = \sqrt{a^2 + b^2}$. 
Then the solution seems trivial. We don't even need to calculate $\varphi$ because if we put
\begin{equation} \label{givens_cos}
	\cos(\varphi) = \frac{a}{\sqrt{a^2 + b^2}} 
\end{equation}
and   
\begin{equation} \label{givens_sin}
	\sin(\varphi) = \frac{b}{\sqrt{a^2 + b^2}} 
\end{equation}
then 
\begin{align}
	r &= \frac{a^2}{\sqrt{a^2 + b^2}} + \frac{b^2}{\sqrt{a^2 + b^2}} = \sqrt{a^2 + b^2} \\
	z &= \frac{-ab}{\sqrt{a^2 + b^2}} +  \frac{ab}{\sqrt{a^2 + b^2}} = 0.
\end{align}

Practically used algorithm of computing $\cos{\varphi}$ and  $\sin{\varphi}$ is slightly different because we want to prevent overflow. Real algorithm is described by following pseudocode:


\begin{algorithm}[H]
	\label{alg:Cstep}
    % \SetKwInOut{Input}{input}
    % \SetKwInOut{Output}{output}
    \KwIn{$a, b$}
    \KwOut{$cos, sin$ }
	\caption{Rotate}
	$sin \gets \emptyset$\;
	$cos \gets \emptyset$\;
	\uIf{$b == 0$}{
		$sin \gets 0$\;
		$cos \gets 1$\;
	}
	\uElseIf{$abs(b) \geq abs(a)$}{
		$cotg \gets \frac{a}{b}$\;
		$sin \gets \frac{1}{\sqrt{1 + (cotg)^2}}$\;
		$cos \gets sin\ cotg$\;
	}
	\Else{
		$tan \gets \frac{b}{b} $\;
		$cos \gets \frac{1}{\sqrt{1 + (tan)^2}}$\;
		$sin \gets cos\ tan$\;
	}
	\Return{ $cos$\,, $sin$ }\;
\end{algorithm}

We can scale this same idea to higher dimensions. Let's denote matrix $\vec{Q}(i, j) \in \mathbb{R}^{m \times m}$ defined as 
\renewcommand{\kbldelim}{[}% Left delimiter
\renewcommand{\kbrdelim}{]}% Right delimiter
\[
  \vec{Q}(i, j) = \kbordermatrix{
    &   &       & i &       & j &      &  \\
    & 1 & \dots & 0 & \dots & 0 &\dots & 0 \\
    & \vdots & \ddots & \vdots & & \vdots & & \vdots \\
   i  & 0 & \dots & c & \dots & s & \dots & 0 \\
	 & \vdots &  & \vdots & \ddots & \vdots & & \vdots \\
	 j  & 0 & \dots & -s & \dots & c & \dots & 0 \\
	 & \vdots &  & \vdots &  & \vdots & \ddots & \vdots \\
	 & 0 & \dots & 0 & \dots & 0 &\dots & 1 \\
  }
\]

where $c = 	\cos(\varphi) $ and $ s = \sin(\varphi)$ for some $\varphi$. That means this matrix is orthogonal. Now if we have some column vector $\vec{x} \in \mathbb{R}^{m \times 1}$ and calculate $c$ and $s$ by means of \ref{givens_cos} and \ref{givens_sin} for $a := x_i $ and $b := x_j$. 
Finally if we multiply $\vec{Q}(i, j) \vec{x} = \vec{p}$ we can see that $\vec{p}$ is same as vector $\vec{x}$ except $p_i$ and $p_j$ so that:

\[
	\vec{Q}(i, j) \vec{x} = \vec{Q}(i,j) 
	\kbordermatrix{
    &   \\
    & x_1 \\
    & \vdots \\
   i  & x_i  \\
	 & \vdots \\
	 j  & x_j  \\
	 & \vdots \\
	 & x_m \\
  }
	=  \vec{p} = \kbordermatrix{
    &   \\
    & x_1 \\
    & \vdots \\
   i  & r  \\
	 & \vdots \\
	 j  & 0  \\
	 & \vdots \\
	 & x_m \\
  }
\]

where $r = \sqrt{x_i^2 + x_j^2}$. If we have matrix $\vec{A} \in \mathbb{R}^{n \times p}$ instead of only one column $\vec{x}$ it works the same way. So if we have such matrix we need to create matrix $	\vec{Q}(i, j)$ for each $a_{ij}$ under the diagonal in order to create upper triangular matrix. Usually we are zeroing columns so that we start with $a_{12}$ then $a_{13} \ldots a_{1_n}$.
Then we start with second column $a_{23}$ and so on. That means we need to create in total exactly
$e = \frac{p^2 - p}{2} + np - p^2$ matrices $\vec{Q}(i, j)$. We can denote this sequence of matrices as $\vec{Q_1},  \vec{Q_2}, \ldots \vec{Q_e}$.
The QR decomposition then looks like

\begin{equation}
	\vec{Q_e}\ldots\vec{Q_2}\vec{Q_1}\vec{A} = \vec{R}
\end{equation}
where $\vec{Q_e}\ldots\vec{Q_2}\vec{Q_1}$ is actually $\vec{Q}^T$. So $\vec{Q}$ is obtained by 

\begin{equation}
 \vec{Q} = \vec{Q_1^T}\vec{Q_2^T}\ldots\vec{Q_e^T}.
\end{equation}

We can see that for each row of matrix we need one additional matrix $\vec{Q_i}$ and with such matrix we are actually multiplying only two rows. Moreover the rows are getting shorter after each finished column. So time we see that time complexity is equal to
\todo{zapsat algoritmus?}
\begin{equation}
	\sum\limits_{i=1}^n  \sum\limits_{j=i+1}^p 6(n-i+1) \approx 6np^2 - 3np^2 - 3p^3 + 2p^3 = 3np^2 - p^3
\end{equation}
\todo{move all this about rotations somewhere else}

So we are now in situation when we have QR decomposition of $\vec{X}$ and we need to exchange $i$th row for $j$th row. We can simulate this by first adding $j$th row and consequently removing $i$th row.

First let's discuss how to update QR decomposition when row is added. This is indeed very simple. 
If we add row to $\vec{A}$ our decomposition looks like
\begin{equation}
	\vec{R^{+}} = \vec{Q_e}\ldots\vec{Q_2}\vec{Q_1}\vec{A_{new}}  = 
	\begin{bmatrix}
		\times & \cdots & \cdots & \cdots & \times \\
		0 &\times & \cdots & \cdots & \times \\
		\vdots& 0&\ddots & \cdots & \vdots \\
		\vdots& \vdots&0 & \ddots &  \vdots \\
		\vdots& \vdots& \vdots& 0& \times \\
		\vdots& \vdots& \vdots& \vdots& 0  \\
		\vdots& \vdots& \vdots& \vdots& \vdots \\
		\times & \cdots & \cdots & \cdots & \times \\
	\end{bmatrix}
\end{equation}

So all we need to do is create matrices $\vec{Q_{e+1}}\ldots\vec{Q_{e+p}}$ to zero newly added row.
So the $R$ then will be equal to $\vec{R_{new}} = \vec{R}\vec{Q_{e+1}}\vec{Q_e}\ldots\vec{Q_2}\vec{Q_1}\vec{A}$. \todo{blbe znaceni ale nevim jak lepe}
$\vec{Q}$ will be updated in the same way thus $\vec{Q_{new}} = \vec{Q}\vec{Q_e^T}\vec{Q_{e+1}^T}\ldots\vec{Q_{e+p}^T}$.


\todo{napsat pseudokod}
\begin{algorithm}[H]
	\label{addingrowqr}
    % \SetKwInOut{Input}{input}
    % \SetKwInOut{Output}{output}
    \KwIn{$\vec{Q}, \vec{R}, \vec{x_i}$}
		\KwOut{$\vec{Q^{+}}, \vec{R^{+}}$ }
	  \caption{QR insert}

	\Return{ $\vec{Q^{+}}$\,, $\vec{R^{+}}$ }\;
\end{algorithm}
Computing $\vec{R^{+}}$ is $\mathcal{O}(p^2)$ and computing $\vec{Q^{+}}$ is $\mathcal{O}(np)$.

\begin{note} \label{qnotrequired}
	For adding rows matrix $\vec{Q}$ is actually not needed so that we can update $\vec{R}$ to $\vec{R^{+}}$. Later we'll show that this is actually useful. 
\end{note}

When we are deleting the row $\vec{x_i}$ from matrix $\vec{A}$ we can use following trick 
\cite{hammarling2008updatingqr}. First we move such row as a first row of matrix $\vec{A}$ so ve create permutation matrix $\vec{P}$ so that 
\begin{equation}
	\vec{P}\vec{A} = \begin{bmatrix}
		\vec{x_i} \\
		\vec{A(1:i-1 , 1:p)} \\
		\vec{A(1:i+1 , 1:p)} 
	\end{bmatrix}
	= 
	\begin{bmatrix}
		\vec{x_i} \\
		\vec{A^{-}}
	\end{bmatrix}
	= \vec{P}\vec{Q}\vec{R}
\end{equation} 
where $\vec{A(a:b , 1:p)} $ means rows of matrix $\vec{A}$ from $a$ to $b$.
No we can se that we only need to zero first row $\vec{q_1}$ of matrix $\vec{Q}$. We can do this by $n-1$ matrixes matrixes $\vec{Q}(i,j) \in \mathbb{R}^{n \times n}$ so that

\begin{equation}
	\vec{Q}(n-1,n) \ldots \vec{Q}(1,2)\vec{q_1^T} = 	\begin{bmatrix}
		1 \\
		0\\
		\vdots \\
	\end{bmatrix}
\end{equation}

To propagate the change into $\vec{R}$ we then consequently need to update $\vec{R}$ so that


\begin{equation}
	\vec{Q}(n-1,n) \ldots \vec{Q}(1,2)\vec{q_1^T} \vec{R} = 	\begin{bmatrix}
		\times \\
		\vec{R^{-}}\\
		\vdots \\
	\end{bmatrix}.
\end{equation}

Thus result is

\begin{equation}
	\vec{P}\vec{A} = (\vec{P}\vec{Q}  \vec{Q^T}(1,2) \ldots   \vec{Q^T}(n-1,n)) (\vec{Q}(n-1,n) \ldots \vec{Q}(1,2)\vec{q_1^T} \vec{R} ) \\
	= 
	\begin{bmatrix}
		1 & 0 \\
		0 & \vec{Q^{-}}
	\end{bmatrix}
	\begin{bmatrix}
		\times \\
		\vec{R^{-}}
	\end{bmatrix},
\end{equation}
so that 
\begin{equation}
	\vec{A^{-}} = \vec{Q^{-}}\vec{R^{-}}
\end{equation}

\todo{napsat pseudokod}
\begin{algorithm}[H]
	\label{removingrow}
    % \SetKwInOut{Input}{input}
    % \SetKwInOut{Output}{output}
    \KwIn{$\vec{Q}, \vec{R}, i$}
		\KwOut{$\vec{Q^{-}}, \vec{R^{-}}$ }
	  \caption{QR delete}

	\Return{ $\vec{Q^{-}}$\,, $\vec{R^{-}}$ }\;
\end{algorithm}

Computing $\vec{R^{-}}$ is $\mathcal{O}(p^2)$ and computing $\vec{Q^{-}}$ is $\mathcal{O}(np)$.
\todo{presunout i toto pridavani / odebirani radku spolu s Givens nekam jinam. }



\subsection{Bounding-FSA aka. Modified Optimum exchange (MOEA)}
\subsection{Min-Max-FSA aka. Min Max exchange (MMEA)}


\todo{make CHAPTER approximate ALGORITHMS AND chapter EXACT ALGORITHMS?}
dsafsdsa

\chapter{Exact algorithms} %

\section{Branch and bound aka. BAB}  % Also from AgullÃ³, 2000
\section{Adding row algorithm} % Hoffmann, Kontoghiorghes, 2010 (Matrix strategies SUR)
\section{Klouda algorithm}


Time complexity matrix multiplication

School  $\mathcal{O}(n^{3})$
Although faster algorithm exists like
Strassen algorithm $\mathcal{O}(n^{2.8074})$ or even
CoppersmithâWinograd $\mathcal {O}(n^{2.375477})$.
In practice, however, none of those two algorithm is used not only high constant factors but also due to lower numerical stability. Others factors include worse cash optimization.
Moreover current implementations of the linear algebra libraries such as LAPACK and more importantly BLAS level 3 routines implementing  matrix-matrix multiplication which are used in such linear algebra software implement $\mathcal{O}(n^{3})$ matrix-matrix multiplication. 
... In every way it's sort of accurate in this paper to say that matrix multiplication is $\mathcal{O}(n^{3})$
blas dgemm

$ \vec{A} \in \mathbb{R}^{m \times n}, \vec{B} \in \mathbb{R}^{n\times p}$ then $ \mathcal{O}(n^{3})$