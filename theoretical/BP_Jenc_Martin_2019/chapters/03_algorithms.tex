\chapter{Algorithms}

In the previous chapter, we cover the theoretical background required to implement algorithms that are in this chapter. We introduced the discrete version of the LTS objective function which minimum is equivalent to the continuous one. Finding the minimum of this function requires to find the correct $h$ subset and then calculate the estimate of regression coefficients $\vec{w}$. To achieve this, we need to examine all $h$ subsets. The exhaustive approach fails due to the exponential size of $Q^{(n,h)}$. So what are the possibilities? 

\subsection{First attempts}
Initial algorithms were based on iterative removal data samples whose residuum had the highest value based on OLS fit on the whole dataset. Such attempts are known to be flawed \cite{hawkins:1994} because the initial OLS fit can be already profoundly affected by outliers and we may remove data samples which represent the actual model.

Other algorithms were based purely on a random approach. On such algorithm is Random solution algorithm \cite{bai2003random} which randomly selects $l$ of $h$ subsets and subsequently compute OLS fit on each of them and chooses fit with a minimum value of the objective function.  Such approach is straightforward, but the probability of selecting at least one subset from $l$ subsets which don't contain outliers thus has a chance of producing good result tends to zero for an increasing number of data samples $n$ as we'll describe in detail in section~\ref{section:random:h:samples}.

Another very similar algorithm called Resampling algorithm introduced in \cite{rousseeuw1987robust}. It selects vectors from $Q^{(n,p+1)}$ instead of $Q^{(n, h)}$. This minor tweak has a higher chance to succeed. Mainly because the probability of selecting $l$ subsets of size $p+1$ thus it is independent of $n$ gives the nonzero probability of selecting at least one subset such that it does not include outliers (see section~\ref{section:random:p:samples} for more details). Besides that the number of vectors in this set is significantly lower than in $Q^{(n, h)}$ (at least if $h$ is conservatively chosen so that $h = [(n/2] + [(p+1)/2]$).
 
\subsection{Strong and weak necessary conditions}
Generating all possible $h$ subsets is computationally exhaustive and relying on randomly generating  ``good'' $h$-element subsets does not lead to reliable results. So what are our options? In \cite{hawkins1999improved} two criteria called \defterm{weak necessary conditions} and \defterm{strong necessary conditions} are introduced. They state necessary properties which some $h$ subset must satisfy to be subset which leads to the global optimum of the LTS objective function. Let us introduce those two necessary conditions. For that, it is convenient not only to label a $h$ subset of \defterm{non-trimmed} observations but also the complementary subset of not used observations. We'll refer to this complementary subset as to \defterm{trimmed} subset.

%%%%%%%%%%% STRONG CONDITION %%%%%%%%%%%%%
\begin{definition} \label{strong_condition}
 A $h$-element subset corresponding to $\vec{m} \in Q^{(n, h)}$ satisfies  the \defterm{strong necessary condition} if for any vector $\vec{m_{swap}}$ which differs from $\vec{m}$ only by swapping an $1$ with one $0$, we get  $\oflts(\vec{m}) \leq \oflts(\vec{m_{swap}})$. Thus the value of discrete LTS objective function cannot be reduced by swapping one non-trimmed observation with one trimmed observation.
\end{definition}

Based on this fact an algorithm can be created. We'll discuss it in detail in section~\ref{section_feasible_solution}.

%%%%%%%%%%% WEAK CONDITION %%%%%%%%%%%%%
The second necessary condition is named \defterm{weak necessary condition} 

\begin{definition}
A $h$-element subset corresponding to $\vec{m} \in Q^{(n, h)}$ satisfies  the \defterm{strong necessary condition} if 
$r_i^2(\vec{\hat{w}^{(OLS, \vec{M}\vec{X, \vec{M}\vec{y})} } })$ for all trimmed observation is greater of equal to the greatest non-trimmed squared residuum $r_j^2(\vec{\hat{w}^{(OLS, \vec{M}\vec{X, \vec{M}\vec{y})} }})$.
\end{definition}    
Again, based on this criteria an algorithm can be created. The corollary of this criteria together with proof can be found in  section~\ref{section_fast_lts}. 

% %%%%%%    WEAK AND STRONG CONDITIONS LEMMA    %%%%%%%%%
The interesting consequence which we'll use later gives us the following lemma.

\begin{lemma} \label{lemma_conditions}
    The strong necessary condition is not satisfied unless the weak necessary condition is satisfied. Thus if a strong condition is satisfied then weak is also. 
\end{lemma}

\begin{proof}
		We make proof by contradiction. Let us assume that we have $\vec{\hat{w}^{(OLS, \vec{M}\vec{X, \vec{M}\vec{y})} }}$ where $\vec{m} \in Q^{(n, h)}$ for which strong necessary condition is satisfied, but the weak necessary condition is not. That means there exists $\vec{x_i}$ with $y_i$ from non-trimmed subset and $\vec{x_j}$ and $y_j$ from trimmed subset such that
		\begin{equation*} 
			r_j^2(\vec{\hat{w}^{(OLS, \vec{M}\vec{X, \vec{M}\vec{y})} }}) < r_i^2(\vec{\hat{w}^{(OLS, \vec{M}\vec{X, \vec{M}\vec{y})} }}).
	\end{equation*}
		 Thus 
    \begin{equation} 
        \oflts(\vec{m}) > \oflts(\vec{m}) + r_j^2 - r_i^2  
    \end{equation}
Now we need to show that $\vec{m_{swap}}$ vector that is created by swapping $j$th observation from trimmed subset with $i$th observation from non-trimmed subset leads to 
    \begin{equation} 
      \oflts(\vec{m}) + r_j^2 - r_i^2  \geq \oflts(\vec{m_{swap}}).
    \end{equation}
That is true because the value of $\oflts(\vec{m_{swap}})$  is minimum on the given subset of observations. That is, of course, contradiction with our assumption which says that strong necessary condition is already satisfied. 
\end{proof}

Now we have covered all the necessary theoretical background, and it is time to introduce currently known algorithms for computing the LTS estimate.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%   SECTION  COMPUTING THE OLS  (60 -- 300)   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Computing OLS}
In this section, we describe a few of many methods that can be used to obtain  $\vec{\hat{w}}^{(OLS,n)}$. Those methods are parts of the algorithms used to calculate the LTS estimate. 

In the following algorithms, we describe its time complexity. Because matrix multiplication is the fundamental part of all the following algorithms, it is, therefore, appropriate to mention a few facts about the time complexity of matrix multiplication. 

There are multiple algorithms for matrix multiplication, for example:
\begin{itemize}
  \item Naive algorithm; its time complexity is $\mathcal{O}(n^{3})$.
  \item Strassen algorithm; its time complexity is $\mathcal{O} (n^{2.8074})$ \cite{strassen1969gaussian}. 
  \item Coppersmith-Winograd; its time complexity is $\mathcal {O}(n^{2.375477}) \cite{coppersmith1990matrix}$
\end{itemize}

In practice, however, the naive algorithm is usually used. Even though some of those algorithms have been efficiently implemented and are known to be numerically stable (primarily variations of Strassen algorithm), their error bound is weaker than in case of the naive algorithm. For this reason, they are not used even when they might improve performance \cite{ballard2015improving}.

Moreover current implementations of the linear algebra namely LAPACK and more importantly BLAS level 3 routines implementing matrix-matrix multiplication which are widely used in such linear algebra software implement naive matrix-matrix multiplication \cite{laug}. 

For that reason we assume in this work that matrix multiplication is  $\mathcal{O}(n^{3})$. For not square matrices $ \vec{A} \in \mathbb{R}^{m \times n}, \vec{B} \in \mathbb{R}^{n\times p}$ it is then$ \mathcal{O}(mnp)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%  I N V E R S I O N    %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Computation using a matrix inversion}
Solving OLS using matrix inversion can be done as follows:

\begin{enumerate}
 \item Compute  $\vec{A} = \vec{X}^T\vec{X}$ and $\vec{B} = \vec{X}\vec{y}$.
  \item Find inversion of $\vec{A}$.
  \item Multiply $\vec{A}^{-1}\vec{B}$.
\end{enumerate}

\begin{observation} \label{time:complexity:ols:inversion}
    Time complexity of computing the OLS  estimate on $\vec{X} \in \mathbb{R}^{n \times p}$ and $\vec{y}  \in \mathbb{R}^{n \times 1}$ using matrix inversion is $O(p^2n)$ .
\end{observation}

\begin{proof}
First, we compute 
$\vec{A} = \vec{X^T}\vec{X}$  and
$\vec{B} = \vec{X^T}\vec{y}$. 
That gives us $\mathcal{O}(p^2n + pn)$.
Next, we compute inversion  $\m{C} = \m{A}^{-1}$ which gives us $\mathcal{O}(p^3)$.
Finally, we compute
$ \vec{\hat{w}}^{(OLS,n)} = \m{C}\m{B}$ which is $\mathcal{O}(p^2)$. 
Together we get $ \mathcal{O}(p^2n + pn + p^3 + p^2)$.
$p^2n$ and $p^3$ asymptotically dominates over the rest so 
\begin{equation}
\mathcal{O}(p^2n + pn + p^3 + p^2) \sim \mathcal{O}(p^2n + p^3).
\end{equation}
Moreover if we assume that $n \ge p$ and usually even $n \gg p$  we get $\mathcal{O}(p^2n + p^3) \sim \mathcal{O}(p^2n)$.
\end{proof}

Computing OLS estimate using matrix inversion is possible, however in most cases not used in practice because of the low numerical stability computing matrix inversion.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%  C H O L E S K Y    %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Computation using the Cholesky decomposition}

\begin{definition}
    Let $\vec{A} \in \mathbb{R}^{n \times n}$ be an symmetric positive definite matrix. Then it is possible to find lower triangular matrix $L$ so that 
    \begin{equation}
        \vec{A} = \vec{L}\vec{L^T}
    \end{equation}
    We call this decomposition as \defterm{Cholesky factorization} (sometimes also  \defterm{Cholesky decomposition})
\end{definition}
    
If we look at our problem of finding a solution to 
\begin{equation}
    \vec{X}^T\vec{X}\vec{w} = \vec{X}^T\vec{y},
\end{equation}
we can easily rewrite it as 
\begin{equation}
    \vec{Z}\vec{w} = \vec{b}
\end{equation}
where $\vec{Z} = \vec{X}^T\vec{X}$ is symmetric positive definite matrix and $\vec{b} = \vec{X}^T\vec{y} $. Because $\vec{Z}$ can be factorized as $\vec{L}\vec{L^T}$ the solution can be found easily by substitution

\begin{align}
    \vec{L}\vec{d} = \vec{b} \\
    \vec{L}^T\vec{w} = \vec{d},
\end{align}
where $\vec{d}$ is solved by forward substitution and $\vec{w}$ is then solved by backward substitution which ten represents $\vec{\hat{w}}^{(OLS,n)}$ estimate.

%%%%%%% C H O L E S K Y     A L G O R I T H M %%%%%%%%%%%%%%

So the algorithm for solving OLS using Cholesky factorization goes as follows:
\begin{enumerate}
  \item Compute $\vec{X}^T\vec{X}$ and   $\vec{X}^T\vec{y}$.
  \item Compute Cholesky factorization $\vec{Z} = \vec{L}\vec{L}^T$ where $\vec{Z} = \vec{X}^T\vec{X}$.
  \item Solve lower triangular system $\vec{L}\vec{d} = \vec{b}$  where $ \vec{b} = \vec{X}^T\vec{y}$  for $\vec{d}$ using forward substitution.
  \item Solve upper triangular system $ \vec{L}^T\vec{w} = \vec{d}$ for $\vec{w}$.
\end{enumerate}

\begin{observation} \label{time:complexity:ols:cholesky}
    The time complexity of solving OLS using Cholesky factorization is $\mathcal{O}(p^2n)$
\end{observation}

\begin{proof}
    First step requires two matrix multiplication $\vec{X}^T\vec{X}$ which is $\mathcal{O}(np^2)$ and  $\vec{X}^T\vec{y}$ which is $\mathcal{O}(np)$.
    Second step represents computing Cholesky factorization. This can be done in $\mathcal{O}(\frac{1}{3}p^3)$ \cite{krishnamoorthy2013matrix}
    In the third and fourth step, we are solving triangular systems both of them require  $\mathcal{O}(\frac{1}{2}p^2)$, so together $\mathcal{O}(p^2)$.

    Putting all steps together we get  

    \begin{equation} 
        \mathcal{O}(np^2 + np + \frac{1}{3}p^3 + p^2)
    \end{equation}
    and because we assume $n \geq p$ and most usually  $n \gg p$ then multiplication $\vec{X}^T\vec{X}$ asymptotically dominates over the rest so we get $\mathcal{O}(p^2n)$.
\end{proof}

% \begin{note}
%     In case we have $\vec{X}^T\vec{X}$ and  $\vec{X}^T\vec{y}$ computed in advance, then it is asymptotically dominated by Cholesky factorization 
%     \begin{equation} \label{time:complexity:ols:cholesky:we:have:xx:xy}
%         \mathcal{O}(\frac{1}{3}p^3) \sim \mathcal{O}(p^3)
%     \end{equation}
% \end{note}

Computation OLS estimate using the Cholesky factorization is more numerically stable than using matrix inversion and time complexity is asymptotically similar. This approach is however not usually used in practice as well. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%  Q R     D E C O M P O S I T I O N     %%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Computation using the QR decomposition}

Let us now look on a similar method of computing OLS estimate and which does not require multiplying $\vec{X}^T\vec{X}$. 

\begin{definition}
    Let $\vec{A} \in \mathbb{R}^{n \times n}$ be any square matrix. Then it is possible to find matrices $\vec{Q}$ and $\vec{R}$ so that 

    \begin{equation}
        \vec{A} = \vec{Q}\vec{R},
    \end{equation}
    where $\vec{Q} \in \mathbb{R}^{n \times n}$ is orthogonal matrix and $\vec{R} \in \mathbb{R}^{n \times n}$ is upper triangular matrix. 
This decomposition is known as \defterm{QR decomposition}.
\end{definition}
If matrix $\vec{A} \in \mathbb{R}^{m \times n}$ thus is not square, then decomposition can be found as

    \begin{equation}
        \vec{A} = \vec{Q}\vec{R} = \vec{Q} \begin{bmatrix}
            \vec{R_1} \\
            \vec{0}
        \end{bmatrix}
        =  \begin{bmatrix}
            \vec{Q_1} & \vec{Q_2}
        \end{bmatrix}  \begin{bmatrix}
            \vec{R_1} \\
            \vec{0}
        \end{bmatrix} 
        = \vec{Q_1} \vec{R_1}
    \end{equation}
    Where $\vec{Q} \in \mathbb{R}^{m \times m}$ is orthogonal matrix, $\vec{R_1} \in \mathbb{R}^{n \times n} $ is upper triangular matrix, $\vec{0} \in \mathbb{R}^{ (m-n) \times n} $  is zero matrix,  $\vec{Q_1} \in \mathbb{R}^{n \times n}$ is matrix with orthogonal columns and  $\vec{Q_2} \in \mathbb{R}^{(m-n) \times n}$ is also matrix with orthogonal columns.
    
That means that  $\vec{X}$ can be factorized as
\begin{equation}
    \vec{X} = \vec{Q}\vec{R}.
\end{equation}
Then
\begin{equation}
    \vec{X}^T\vec{X} = (\vec{Q}\vec{R})^T \vec{Q}\vec{R} = \vec{R}^T\vec{Q}^T\vec{Q}\vec{R}
\end{equation}
and because $\vec{Q}$ is orthogonal, then $\vec{Q}^T\vec{Q} = \vec{I}$ so that
\begin{equation}
    \vec{X}^T\vec{X} = \vec{R}^T\vec{R}.
\end{equation}
Because $\vec{X} \in  \mathbb{R}^{n \times p}$ is not square matrix then as we shown $\vec{R} = \begin{bmatrix}
    \vec{R_1} \\
    \vec{0}
\end{bmatrix}$
so that 
\begin{equation}
    \vec{X}^T\vec{X} = \vec{R_1}^T\vec{R_1}.
\end{equation}
 Where $\vec{R_1^T}$ is lower triangular matrix. 

Solution to
\begin{equation}
    \vec{X}^T\vec{X}\vec{w} = \vec{X}^T\vec{y}
\end{equation}
is then given by 
\begin{equation}
    \vec{R_1}^T\vec{R_1}\vec{w} = \vec{R_1}^T\vec{Q_1}^T \vec{y}
\end{equation}
and because we assume $\vec{X}$ have full column rank, thus is invertible, we can simplify it to
\begin{equation}
    \vec{R_1}\vec{w} = \vec{b_1}
\end{equation}
where $ \vec{b_1} = \vec{Q_1}^T \vec{y}$. Because $\vec{R_1}$ is upper triangular matrix, solution of $\vec{w}$ is trivial using backward substitution.  $\vec{w}$ then represents our OLS estimate $\vec{\hat{w}}^{(OLS,n)}$.

\begin{note}
        We can see that Cholesky factorization  $\vec{X}^T\vec{X} = \vec{L}\vec{L}^T$ is closely connected to the QR decomposition $\vec{X} = \vec{Q_1}\vec{R_1}$ so that  
        \begin{equation} \label{qrcholesky}
                \vec{R_1^T} = \vec{L}
            \end{equation}
\end{note}

%%%%%%      Q R    A L G O R I T H M      %%%%%%%%%%%%

So the algorithm for solving OLS using QR decomposition can go as follows:
\begin{enumerate}
  \item Calculate QR decomposition $\vec{X} = \vec{Q}\vec{R}^T = \vec{Q_1}\vec{R_1}^T$.
  \item Calculate $\vec{b_1} = \vec{Q_1}^T \vec{y}$.
  \item Solve upper triangular system $\vec{R_1}\vec{w} = \vec{b_1}$ for $\vec{w}$.
\end{enumerate}

QR factorization can be calculated in multiple ways. The most basic method is applying the Gram–Schmidt process
to columns of matrix $\vec{X}$. This approach is not numerically stable so in practice is not used as much as two other methods. Those are \defterm{Householder transformations} and \defterm{Givens rotations}.
The time complexity of both algorithms is similar. QR decomposition of matrix $\vec{X} \in \mathbb{R}^{n \times p}$ is 

\begin{equation}
    \mathcal{O}(2p^2n - \frac{2}{3}p^3).
\end{equation}
\todo{find citation}

On the other hand, using Givens rotation for QR decomposition of the same matrix is 
\begin{equation} \label{givenstimenoproof}
    \mathcal{O}(3np^2 - p^3)
\end{equation}
so Householder transformations are about $50\%$ faster, but Givens rotations are more numerically stable. 
Moreover, Givens rotations and are suitable for sparse matrices.  This property of Givens rotations we use in section \todo{ref}.
That is why we describe Givens rotation in detail in \ref{givensrotation}. We also make proof of \eqref{givenstimenoproof} time complexity there.

For now, let us look at time complexity of solving OLS using Householder transformations. 
\begin{observation}
    The time complexity of solving OLS estimate using QR decomposition by Householder transformations is $\mathcal{O}(2p^2n - \frac{2}{3}p^3)$
\end{observation}

\begin{proof}
    In the first step, we calculate QR decomposition. Householder transformations can be done in $\mathcal{O}(2np^2 - \frac{2}{3}p^3)$.
    Second step consist of matrix multiplication $\vec{Q_1}^T \vec{y}$ which is $\mathcal{O}(np)$.  In the last step we are solving upper triangular systems which is $\mathcal{O}(\frac{1}{2}p^2)$.
    Putting all steps together we get  

    \begin{equation} \label{time:complexity:ols:qr:householder}
        \mathcal{O}(2np^2 - \frac{2}{3}p^3 + np + \frac{1}{2}p^2)
    \end{equation}
    We can see that $p^2n$ asymptotically dominates.
\end{proof}

The QR decomposition is considered as a standard way of computing OLS estimate because of its high numerical stability. Let us mention that a little slower, but a more stable method of computing OLS estimate exists, and that is the singular value decomposition (SVD). On the other hand, the QR decomposition is sufficiently stable for most cases so describing SVD decomposition is out of the scope of this work.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%        FAST    LTS       %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{FAST-LTS} \label{section_fast_lts}
In this section, we introduce the FAST-LTS algorithm\cite{rouss:2000}. 
It is, as well as other algorithms we introduce, iterative algorithm. We discuss all main components of the algorithm starting with its core idea called a concentration step which authors call a \defterm{C-step}.

\subsection{C-step}
We show that from an existing LTS estimate $\boldsymbol{\hat{w}}$ we can construct a new LTS estimate $\boldsymbol{\hat{w}_{new}}$ so that value the objective function at $\boldsymbol{\hat{w}_{new}}$ is less or equal to the value at $\boldsymbol{\hat{w}}$. Based on this property, the algorithm creates a sequence of LTS estimates which leading to better results.


%%%%%%%%%     C-STEP theorem       %%%%%%%%%%%%

\begin{theorem}
Consider 
$\vec{X} \in \mathbb{R}^{n \times p}$ and 
$\vec{y} \in \mathbb{R}^{n \times 1}$.  
Let us also have $\vec{w_0}\in\mathbb{R}^p$ and $\vec{m_0} \in Q^{(n,h)}$. 
Let us put $L_0 = \sum\limits_{i=1}^n m^{(0)}_i r_{i}^2(\vec{w_0})$ where $r_{i}^2(\vec{w_0}) = y_i - (w_1^0x^i_1 + w_2^0x^i_2 +\ldots+ w_p^0x^i_p$).
Let us take $\hat{n} = \{{1,2,\ldots,n\}}$ and mark
$\pi: \hat{n} \rightarrow \hat{n}$ permutation of $\hat{n}$ such that $|r_0({\pi(1)})| \leq |r_0({\pi(2)})| \leq \ldots \leq |r_0({\pi(n)})|$
and mark   $\vec{m_1} \in Q^{(n,h)}$  such that $m^1_i = 1$ for $i \in \{{\pi(1)\,, \pi(2)\,,... \pi(h)\}}$ and  $m^1_i = 0$  otherwise. 
This means that  $\vec{m_1}$ corresponds to $h$ subset with smallest absolute residuals $r_{i}^2(\vec{w_0})$.

Finally, take
$\of^{(OLS,\vec{M_1}\vec{X},  \vec{M_1}\vec{y} )} (\vec{w})$ least squares fit on $m_1$ subset of observations and its corresponding 
$L_1 = \sum\limits_{i=1}^n m^{(1)}_i r_{i}^2(\vec{w_1})$. Then
\begin{equation} 
    L_1  \leq L_0
\end{equation}
\end{theorem}

\begin{proof}
    Because we take $h$ observations with smallest absolute residuals $r_{i}^2(\vec{w_0})$, then for sure
$\sum\limits_{i=1}^n m^{(1)}_i r_{i}^2(\vec{w_0})
\leq
\sum\limits_{i=1}^n m^{(0)}_i r_{i}^2(\vec{w_0}) =  L_0
$.
When we take into account that the OLS estimate minimizes the objective function of $\vec{m_1}$ subset of observations, then for sure  
$L_1 =  \sum\limits_{i=1}^n m^{(1)}_i r_{i}^2(\vec{w_1})
 \leq 
\sum\limits_{i=1}^n m^{(1)}_i r_{i}^2(\vec{w_0})$.

Together we get 
$
L_1 = \sum\limits_{i=1}^n m^{(1)}_i r_{i}^2(\vec{w_1})
\leq
\sum\limits_{i=1}^n m^{(1)}_i r_{i}^2(\vec{w_0})
\leq
\sum\limits_{i=1}^n m^{(0)}_i r_{i}^2(\vec{w_0}) =  L_0
$
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%    C-STEP algorithm    %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{corollary} 
    Based on the previous theorem, using some $\vec{\hat{w}_{old}^{OLS(h, n)}}$  of $\vec{m_{old}}$ subset of observations we can construct $\vec{m_{new}}$ subset with corresponding $\vec{\hat{w}_{new}^{OLS(H_{new})}}$ such that $L_{new} \leq L_{old}$. 
    Applying the theorem repetitively leads to the iterative sequence of
    $L_{1} \leq L_{2} \leq \ldots$. One step, called \defterm{C-step} of this process is described by the following pseudocode. Note that for C-step we  need only $\vec{\hat{w}}$ 
     without the need of passing  $\vec{m}$.
\end{corollary}

\begin{figure}[h]
\centering
\missingfigure{Image visualizing  one C-step}
\caption{todo caption}
\label{figure:one:c:step}
\end{figure}

\begin{algorithm}[H]
    \label{alg:Cstep}
    % \SetKwInOut{Input}{input}
    % \SetKwInOut{Output}{output}
    \KwIn{dataset consiting of $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ and $\boldsymbol{y} \in \mathbb{R}^{n \times 1}$,  $\what{{old}} \in \mathbb{R}^{p \times 1}$}
    \KwOut{ $\what{{new}}$, $m_{new}$ }
    \caption{C-step}
    
    $R \gets \emptyset$\;
    \For{$i \gets 1$ \textbf{to} $n$}{  
        $R \gets R \cup \{ |y_i - \what{{old}} \vec{x_i}^T |\}$\;
    }
    $m_{new} \gets $ select set of $h$ smallest absolute residuals from $R$\;
    $\vec{\hat{w_{new}}} \gets$ OLS fit on $m_{new}$ subset;
    \Return{ $\what{{new}}\,, m_{new}$ }\;
\end{algorithm}

One step of algorithm C-step is visualized on figure~\ref{figure:one:c:step}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%    C-STEP time complexity    %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{observation} 
    The time complexity of algorithm C-step~\ref{alg:Cstep} is asymptotically similar to the time complexity as OLS fit. Thus $O(p^2n)$.
\end{observation} 

\begin{proof}
    In C-step we must compute $n$ absolute residuals. Computation of one absolute residual consists of
    matrix multiplication of shapes $1 \times p$ and $p \times 1$ that gives us $\mathcal{O}(p)$. So time of computation $n$ residuals is $\mathcal{O}(np)$.
    Next, we must select a set of $h$ smallest residuals which can be done in $\mathcal{O}(n)$ using a modification of algorithm QuickSelect \cite{hoare1961algorithm}.
    Finally, we must compute $\hat{w}$ OLS estimate on a $h$ subset of data. Because $h$ is linearly dependent on $n$, we can say that it is $\mathcal{O}(p^2n + p^3)$ which is asymptotically dominant against previous steps which are $\mathcal{O}(np + n)$. Because we assume$n \ge p$ then $\mathcal{O}(p^2n + p^3) \sim \mathcal{O}(p^2n) $
\end{proof}

As we stated above, repeating C-step leads to sequence of $\what{1}, \what{2} \ldots$ 
on subsets $\vec{m_1}, \vec{m_2} \ldots$ with corresponding 
$L_1 \geq L_2\geq \ldots$. One could ask if this sequence converges, so that $L_i == L_{i+1}$. 
Answer to this question is presented by the following theorem.



%%%%%%%%       C-STEP  converges        %%%%%%%%%%%%%%%

\begin{theorem}
    Sequence of C-step converges to $\what{{k}}$ after maximum of $k = {n \choose h}$ so that
\begin{equation}
    L_k = L_i \,, \forall i \geq k.
\end{equation}
\end{theorem}

\begin{proof}
    Since  $\oflts(\what{{i}})$ is non-negative and $\oflts(\what{{i}}) \leq \oflts(\what{{i+i}})$ the sequence  converges. $\what{{i}}$  is computed out of subset 
    $\vec{m_i} \in Q^{(n, h)}$. Since $Q^{(n, h)}$ is finite,  namely its size is ${n \choose h}$, the sequence converges at the latest after this number of steps.
\end{proof}


%%%%%%%%       C-STEP  iteration        %%%%%%%%%%%%%%%%

The theorem gives us a clue to create algorithm described by the following pseudocode.

\begin{algorithm}[H]
    \label{alg:RepeatCstep}
    \KwIn{dataset consiting of $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ 
    and $\boldsymbol{y} \in \mathbb{R}^{n \times 1}$,  $\what{{old}} \in \mathbb{R}^{p \times 1}\,, \vec{m_0}$}
    \KwOut{ $\what{{final}}$, $\vec{m_{final}}$ }
    \caption{Repeat-C-step}
    \SetKw{Break}{break}
    $\what{{new}} \gets \emptyset$\;
    $\vec{m_{new}} \gets \emptyset$\;
    $L_{new} \gets \infty $\;

    \While{$True$}{
        $L_{old} \gets \oflts(\what{{old}})$\;
        $\what{{new}}$\,, $H_{new} \gets $ C-step$(\boldsymbol{X}\,, \boldsymbol{y}\,, \what{{old}})$\;
        $L_{new} \gets \oflts(\what{{new}})$\;
        \If{$L{old} == L{new}$}{
            \Break
          }
        $\what{{old}} \gets \what{{new}}$
    }

    \Return{ $\what{{new}}$, $\vec{m_{new}}$ }\;
\end{algorithm}

It is important to note, that although the maximum number of steps of this algorithm is ${n \choose h}$ in practice, it is shallow, most often under $20$ steps as can be seen on figure~\ref{figure:repeat:c:steps:cnt:converge}.

\begin{figure}[h]
\centering
\missingfigure{Statistic graph visualizing  number of steps required to convergence of algorithm }
\caption{todo caption}
\label{figure:repeat:c:steps:cnt:converge}
\end{figure}

That is not enough for the algorithm Repeat-C-step to converge to the global minimum. That gives us an idea of how to create the final algorithm. \cite{rouss:2000}

Choose a lot of initial subsets $\vec{m_1}$ and on each of them apply algorithm Repeat-C-step. From all converged subsets with corresponding $\vec{\hat{w}}$ estimates choose the one with the lowest value of $\oflts(\hat{w})$. 

Before we can construct final the algorithm, we must decide how to choose initial subset $\vec{m_1}$ and how many of them mean ``\emph{a lot}''. First, let us focus on how to choose an initial subset $\vec{m_1}$.



%%%%%%%%      INITIAL M_1 SUBSET      %%%%%%%%%%%%%

\subsection{Choosing initial $\vec{m_1}$subset}

It is important to note, that when we choose $\vec{m_1}$ subset such that it contains outliers, then iteration of  C-steps usually does not converge to good results, so we should focus on methods with non zero probability of selecting $\vec{m_1}$ such that it does not contain outliers.
There are many possibilities of how to create an initial $\vec{m_1}$ subset. Let us start with the most trivial one.


%%%%%%%%      RANDOM SELECTION         %%%%%%%%%%%%%
\subsubsection*{Random selection} \label{section:random:h:samples}
Most basic way of creating $\vec{m_1}$ subset is simply to choose random $\vec{m_1} \in Q^{(n, h)}$. The following observation shows that it is not the best way.

\begin{observation} \label{hrandomsamples}
    With an increasing number of data samples, thus with increasing $n$, the probability of choosing among $k$ random selections of $\vec{m_{1_1}}, \ldots ,\vec{m_{1_k}}$ the probability of selecting at least one $\vec{m_{1_i}}$ such that its corresponding data samples does not contains outliers, goes to $0$.
\end{observation}

\begin{proof}
    Consider dataset of $n$ containing $\epsilon > 0$ relative amount of outliers. Let $h$ be chosen conservatively so that $h = [(n/2] + [(p+1)/2]$ and $k$ is the number of selections random $h$ subsets. Then
    \begin{align*}
        P(one~random~data~sample~not~outliers) &= (1-\epsilon) \\
        P(one~subset~without~outliers) &= (1-\epsilon)^h \\
        P(one~subset~with~at~least~one~outlier) &= 1-(1-\epsilon)^h \\
        P(m~subsets~with~at~least~one~outlier~in~each) &= (1-(1-\epsilon)^h)^k \\
        P(m~subsets~with~at~least~one~subset~without~outliers) &= 1-(1-(1-\epsilon)^h)^k \\
    \end{align*}

    Because
$n \rightarrow \infty$, then    
$ (1-\epsilon)^h  \rightarrow 0 $, then   
$ 1- (1-\epsilon)^h  \rightarrow 1$, then
$ (1-(1-\epsilon)^h)^m  \rightarrow 1$, then 
$1- (1-(1-\epsilon)^h)^m  \rightarrow 0 $
\end{proof}

That means that we should consider other options for selecting $\vec{m_1}$ subset. If we would like to continue with selecting some random subsets, previous observation gives us a clue, that we should choose it independent of $n$. Authors of the algorithm came with such a solution, and it goes as follows.


%%%%%%%%%%%     P - SELECTION %%%%%%%%%%%

\subsubsection*{P-subset selection} \label{section:random:p:samples}

Let us choose vector $\vec{c} \in Q^{(n, p)}$. 
Next we compute rank of matrix $\vec{X}_{C} = \vec{C}\vec{X}$, where $C = diag(c)$. If $rank(\vec{X}_{C}) < p$ add randomly selected rows to $\vec{X}_{C}$ without repetition until $rank(\vec{X}_{C}) = p$. Let us from now on suppose that $rank(\vec{X}_{C}) = p$. Next let us mark $\what{0} = \oflts(c)$ and corresponding $r_1(\what{0}), r_2(\what{0}), \ldots ,r_n(\what{0})$ residuals.  Now mark $\hat{n} = \{{1,2,\ldots,n\}}$ and let
$\pi: \hat{n} \rightarrow \hat{n}$ be permutation of $\hat{n}$ such that $|r({\pi(1)})| \leq |r({\pi(2)})| \leq \ldots \leq |r({\pi(n)})|$. 
 Finally, mark $\vec{m_1} \in Q^{(n,h)}$  such that $m^1_i = 1$ for $i \in \{{\pi(1)\,, \pi(2)\,,... \pi(h)\}}$ and  $m^1_i = 0$  otherwise.

\begin{observation} \label{prandomsamples}
    With the increasing number of data samples, thus with increasing $n$, the probability of choosing among $k$ random selections of $\vec{c_{1_1}}, \ldots, \vec{c_{1_k}}$ the probability of selecting at least one $\vec{c_{1_i}}$ such that its corresponding data samples do not contains outliers, goes to
\begin{equation}
    1-(1-(1-\epsilon)^h)^k  > 0.
\end{equation}
\end{observation}

\begin{proof}
    Similarly as in previous observation.
\end{proof}

Last missing piece of the algorithm is determining the number of $k$ initial $\vec{m_1}$ subsets, which maximize the probability to at least one of them converges to correct solution. Simply put, the more, the better. So before we answer this question accurately, let us discuss some key observations about the algorithm.

%%%%%%%%%%%     SPEED-UP      %%%%%%%%%%%

\subsection{Speed-up of the algorithm}
In this section, we describe essential observations which help us to formulate the final algorithm. In two subsections we describe how to optimize the current algorithm. 

%%%%%%%%%%%     SELECTIVE ITERATION      %%%%%%%%%%%
\subsubsection*{Selective iteration}
The most computationally demanding part of one C-step is computation of OLS fit on $\vec{m_i}$ subset and then the calculation of $n$ absolute residuals. How we stated above, convergence is usually achieved under 20 steps. So for fast algorithm run, we would like to repeat C-step as little as possible and at the same time do not lose the performance of the algorithm. 

Due to that convergence of repeating C-step is very fast, it turns out, that we can distinguish between starts that leads to good solutions and those which does not, even after a small amount of the C-steps iterations. Based on empiric observation, we can distinguish good or bad solution already after two or three iterations of C-steps based on $\oflts(\what{3})$ or $oflts(\what{4})$ respectively. 

So even though authors do not specify the size of $k$ explicitly, they propose that after a few C-steps we can choose (say~10) best solutions among all $\vec{m_1}$ starting subsets and continue iteration of the C-steps till convergence only on those best solutions.
Authors refer to this process as to \defterm{selective iteration}.

\subsubsection*{Nested extension}
C-step computation is usually very fast for small $n$. Problem starts with very high $n$ say $n > 10^3$ because we need to compute OLS on $\vec{m_i}$ subset of size $h$ which is dependent on $n$. And then calculate $n$ absolute residuals.

Authors came up with a solution they call \defterm{nested extension}. We describe it briefly now.
\begin{itemize}
    \item If $n$ is greater than limit $l$, we create subset of data samples $L\,, |L| = l$ and divide this subset into $s$ disjunctive sets $P_1,P_2,\ldots,P_s\,, |P_i| = \frac{l}{s}\,, P_i\cap P_j  = \emptyset\,, \bigcup_{i=1}^{s} P_{i} = L$.
    \item For every $P_i$ we set number of starts $m_{P_i} = \frac{m}{l}$. 
    \item Next in every $P_i$ we create $m_{P_i}$ number of initial $H_{P_{i_1}}$ subsets and iterate C-steps for two iterations.
    \item Then we choose $10$ best results from each subsets and merge them together. We get family of sets
    $F_{merged}$ containing $10$ best $H_{P_{i_3}}$ subsets from each $P_i$.
    \item On each subset from  $F_{merged}$ family of subsets we again iterate $2$ C-steps and then choose $10$ best results.
    \item Finally we use these best $10$ subsets and use them to iterate C-steps till convergence.
    \item As a result we choose best of those $10$ converged results.
\end{itemize} 

%%%%%%%%    PUTTING ALL TOGETHER    %%%%%%%%%%%


\subsection{Putting all together}
We described all major parts of the algorithm FAST-LTS. One last thing we need to mention is that even though C-steps iteration usually converges under $20$ steps, it is appropriate to introduce two parameters 
$i_{max}$ and $t$ which limits the number of C-steps iterations in some rare cases when convergence is too slow. Parameter $i_{max}$ denotes the maximum number of iterations in final C-step iteration till convergence. Parameter $t$ denotes threshold stopping criterion such that $| \oflts(\what{{i}}) - \oflts(\what{{i+1}})| \leq t$ instead of 
$\oflts(\what{{i}}) = \oflts(\what{{i+1}})$ . When we put all together, we get \defterm{FAST-LTS} algorithm which is described by the following pseudocode.

\begin{algorithm}[H]
    \label{alg:FAST-LTS}
    \KwIn{$\boldsymbol{X} \in \mathbb{R}^{n \times p}, \boldsymbol{y} \in \mathbb{R}^{n \times 1}, m, l, s, i_{max}, t $}
    \KwOut{ $\vec{\hat{w}_{final}}$, $\vec{m_{final}}$ }
    \caption{FAST-LTS}
    \SetKw{Break}{break}
    $\what{{final}} \gets \emptyset$\;
    $\vec{m_{final}} \gets \emptyset$\;
    $F_{best} \gets \emptyset$\;

    \uIf{$n \geq l$}{
        $F_{merged} \gets \emptyset$\;
        % for each split
        \For{$i \gets 0$ \textbf{to} $s$}{
            % create xx number of starts
            $F_{selected}  \gets \emptyset$\;
            \For{$j \gets 0$ \textbf{to} $\frac{l}{s}$}{
              $F_{initial} \gets Selective~iteration(\frac{m}{l})$\;
              % on each starts iterate few c steps
              \For{$\vec{m_i}$ \textbf{in} $F_{initial}$}{
                $\vec{m_i} \gets Iterate~C~step~few~times(\vec{m_i})$\;
                $F_{selected} \gets  F_{selected} \cup \{{ \vec{m_i} \}} $\;
              }
            }
            % among all starts select 10 best and add it to merged set
            $F_{merged} \gets F_{merged} \cup Select~10~best~subsets~from~F_{selected}$\;
          }
        
          % for each, say 50 best, iterate few and add it to best
          \For{$\vec{m_i}$ \textbf{in} $F_{merged}$}{
            $\vec{m_i} \gets Iterate~C~step~few~times(\vec{m_i})$\;
            $F_{best} \gets F_{best} \cup \{{ \vec{m_i} \}} $\;
          }
          $F_{best} \gets  Select~10~best~subsets~from~F_{best} $\;
    }
    \Else{
        $F_{initial} \gets Selective~iteration(m)$\;
        $F_{best} \gets  Select~10~best~subsets~from~F_{initial} $\;
    }

    % iterate till convergence on few best final results
    $F_{final} \gets \emptyset$\;
    $W_{final} \gets \emptyset$\;
    \For{$\vec{m_i}$ \textbf{in} $F_{best}$}{
            $\vec{m_i}, \what{i} \gets Iterate~C~step~till~convergence(\vec{m_i}, i_{max}, t)$\;
            $F_{final} \gets F_{final} \cup \{{ \vec{m_i} \}}$\;
            $W_{final} \gets W_{final} \cup \{{ \what{i} \}}$\;
    }

    % select one best result
    $\what{{final}}, \vec{m_{final}} \gets select~\vec{\hat{w_i}}~and~\vec{m_{i}}~with~smallest~\oflts(\vec{\hat{w_i}})~from~F_{final}$\;

    \Return{ $\what{{final}}, \vec{m_{final}}$  }\;
\end{algorithm}







% 645 - 800
% ****************************************************************************************************
% ************************* FEASIBLE SOLUTION ******************************************************
% **************************************************************************************************

% 645 - 800
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%        FEASIBLE SOLUTION        %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Feasible solution} \label{section_feasible_solution}
In this section we introduce feasible solution algorithm from \cite{hawkins:1994}.
It is based on the strong necessary condition described at definition \ref{strong_condition}. 
The basic idea can be described as follows.

Let us consider that we have some $\vec{m} \in Q^{(n,h)}$ and denote $O_m = \{{i \in  \set{ 1,2,\ldots , n } ;w_i = 1\}}$ and $Z_m = \{{j \in  \set{ 1,2,\ldots , n } ;w_j = 0\}}$ thus sets of indexes of positions where is $0$ respectively $1$ in vector $\vec{m}$. We can think about it as indexes of observations in $h$ set and $g$ set respectively. Then we can mark $\vec{m}^{(i,j)}$ as a vector which is constructed by swap of its ith and jth element where $i \in O$ and $j \in Z$. Such vector corresponds to vector $\vec{m_{swap}}$ which we marked at definition \ref{strong_condition}.

With this in mind, let us mark 
\begin{equation}
    \Delta S^{(\vec{m})}_{i,j} = \oflts(\vec{m}^{(i,j)}) - \oflts(\vec{m})
\end{equation}
thus a change of the LTS objective function by swapping one observation from not trimmed subset with another from trimmed subset. To calculate this we can obviously first calculate the  $\oflts(\vec{m})$ and  the $ \oflts(\vec{m}^{(i,j)})$ and finally subtract both results. Although it is a option, it it computationally exhaustive. So the question is if there is easier way of calculating $\Delta S^{(\vec{m})}_{i,j}$. The answer is positive and we describe it now.

Let us mark  
$M = diag(\vec{m})$ and 
$M^{(i,j)} = diag(\vec{m}^{(i,j)})$ and also
$\vec{Z_M} = (\vec{X}^T\vec{M}^T\vec{X})$
For now let us assume that we also have
$\vec{Z_M}^{-1}$ and $\vec{\hat{w}} = \vec{Z_M}^{-1}\vec{X}^T\vec{M}\vec{y}$.
We now want to calculate $\Delta S^{(\vec{m})}_{i,j}$.
Let us mark vector of residuals 
$\vec{r}^{(\vec{m})} = \vec{Y} - \vec{X} \vec{\hat{w}} $
and also $d_{r,s} = \vec{x_r} \vec{Z_M}^{-1}  \vec{x_s} $
then by equation introduced in \cite{atkinson1991simulated} we get

\begin{equation} \label{hawkins:rovnice}
    \Delta S^{(\vec{m})}_{i,j} = 
    \frac{({r}^{(\vec{m})}_{j})^2(1-d_{i,i})- ({r}^{(\vec{m})}_{i})^2(1+d_{j,j}) + 2{r}^{(\vec{m})}_{i}{r}^{(\vec{m})}_{j}d_{j,j}}
    {(1-d_{i,i})(1+d_{j,j}) + d_{i,j}^2}.
\end{equation}

Let us now describe the core of the algorithm. It is similar to the FAST-LTS algorithm in the sense of iterative refinement of a $h$ subset. So let us assume that we have some vector  $\vec{m} \in Q^{(n,h)}$. No we compute  $\Delta S^{(\vec{m})}_{i,j}$ for all $i \in O$ and $j \in Z$. This may lead to several different outcomes:
\begin{enumerate}
    \item all $S^{(\vec{m})}_{i,j}$ are non-negative
    \item one $S^{(\vec{m})}_{i,j}$ is negative
    \item multiple $S^{(\vec{m})}_{i,j}$ are negative
\end{enumerate}

In the first case, all $\oflts(\vec{m}^{(i,j)})$ are higher or the same as the $\oflts(\vec{m})$ so none swap will lead to an improvement. That also means that strong necessary condition is satisfied and the algorithm ends.

In the second and third case, the strong necessary condition is not satisfied, and we can make the swap. In the second case, it is easy which one to choose because we have only one. In the third case, we have a couple of options again:
\begin{enumerate}
    \item use the first swap that leads to the improvement
    \item from all possible swaps choose one that has highest improvement value  $\oflts(\vec{m}^{(i,j)})$
    \item use the first swap that has improvement higher than some given threshold
\end{enumerate}

In terms of complexity, all three options give the same results, because to find a feasible solution you need to evaluate all pair swaps. In practice, the third option is the winner because it leads to the least amount of iterations. On the other hand, as we said this does not improve the complexity of the algorithm, so from now on let us assume that we use the case number two.
So if there negative  $S^{(\vec{m})}_{i,j}$, we choose the one with the lowest value, make the swap and repeat the process.  

The algorithm ends when there is no possible improvement, i.e., when all $S^{(\vec{m})}_{i,j}$ are non-negative.
The number of iterations needed till algorithm stops is usually quite low, but for practical usage, it is still convenient to use some parameter $i_{max}$ after which algorithm stops without finding $h$ subset satisfying the strong necessary condition. 
One step of this algorithm we call optimal swap additive algorithm (OSAA) is described by the following pseudocode.

\begin{algorithm}[H]
    \label{alg:optimal:improvement}
    \KwIn{$\vec{Z_M}^{-1} \in \mathbb{R}^{p \times p}$, 
    $\vec{r}^{(\vec{m})} \in \mathbb{R}^{n \times 1}$, 
    $O_m$, $Z_m$, $\vec{X} \in \mathbb{R}^{n \times p} $}
    \KwOut{ $\what{{new}}$, $m_{new}$ }
    \caption{OSAA}
    
    $S \gets 0$\;
    $i_{swap} \gets \emptyset$\;
    $j_{swap} \gets \emptyset$\;

    \For{$m_i \in  O_m$}{  
        \For{$m_j \in  Z_m$}{ 
            $r_i^{(m)} = \vec{r}^{(\vec{m})}_{m_i}$ \;
            $r_j^{(m)} = \vec{r}^{(\vec{m})}_{m_j}$ \;
            $d_{i,i} = \vec{x_{m_i}} \vec{Z_M}^{-1} \vec{x^T_{m_i}}$ \;
            $d_{i,j} = \vec{x_{m_i}} \vec{Z_M}^{-1} \vec{x^T_{m_j}}$ \;
            $d_{j,j} = \vec{x_{m_j}} \vec{Z_M}^{-1} \vec{x^T_{m_j}}$ \;
            $S_{tmp} = calculate~\Delta S^{(\vec{m})}_{i,j}~by$ \eqref{hawkins:rovnice} \;
            \If{$S_{tmp} < S$}{
                $S \gets S_{tmp}$\;
                $i_{swap} \gets m_i$\;
                $j_{swap} \gets m_j$\;
            }
        }
    }
    \Return{ $i_{swap}, j_{swap}, S$ }\;
\end{algorithm}



\todo{and its time compleixty}
\begin{observation} 
     The time complexity of the OSAA \ref{alg:optimal:improvement} is $\mathcal{O}(n^2p^2)$
\end{observation} 


\begin{proof}
All $d_{i,i}$ and $d_{j,j}$ can be calculated before the for loops. 
To calculate $d_{i,i}$ resp. $d_{j,j}$ we need co multiply vector $\in \mathbb{R}^p$ with matrix  $\in \mathbb{R}^{p \times p}$ and vector $\in \mathbb{R}^p$ that is $\mathcal{O}(p^2)$. For all $d_{i,i}$ this have to be done $h$ times and for $d_{j,j}$ $n-h$ times. So all together it is $\mathcal{O}(np^2)$. 

The two loops go through all pairs; thus it is $\mathcal{O}(n^2)$. $d_{i,j}$ can be calculated in the loop, and it can be done in $\mathcal{O}(p^2)$.

If we put everything together we get 
$\mathcal{O}(np^2 + n^2p^2) \sim \mathcal{O}(n^2p^2)$.
\end{proof}


One run of this iteration process leads to some local optimum, i.e., set satisfying the strong necessary condition. In \cite{hawkins:1994} they refer to to this set as to \defterm{feasible set}.
This because it is not global optima the algorithm needs to be run multiple times say $t$ times. A $h$  subset with the smallest value of the objective function is chosen as the final solution.

Discussion about how to find the initial $h$ subset resp. initial $\vec{m}$ was already discussed when describing FAST-LTS algorithm.
More importantly, as we already suggested $h$ subset satisfying weak necessary condition do not need to satisfy strong necessary condition so passing such a $h$ subset as input to this algorithm is another option and we discuss it in detail later. We now describe a feasible solution algorithm (FSA) with pseudocode, and we assume that we already have some function that generates for us $h$ subsets, e.g., random one. 

\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\begin{algorithm}[H]
    \label{alg:feasible_solution}
        \KwIn{$\boldsymbol{X} \in \mathbb{R}^{n \times p}, \boldsymbol{y} \in \mathbb{R}^{n \times 1},  i_{max}, t $}
        \KwOut{ $\vec{\hat{w}_{final}}$, $\vec{m_{final}}$ }
        
    \caption{FSA}
    \SetKw{Break}{break}
    $\vec{\hat{w}_{final}} \gets \emptyset$\;
    $\vec{m_{final}} \gets \emptyset$\;
    $R \gets \emptyset$\;

    \For{$k \gets 0$ \textbf{to} $t$}{
        $\vec{m} \gets generate\_intial\_subset()$  \tcp*{e.g. random $\vec{m} \in Q^{(n,h)}$}

    $l \gets 0$\;
        \While{$True$}{           
           $\vec{M} \gets diag(\vec{m})$\;
       $\vec{Z_M} = (\vec{X}^T\vec{M}^T\vec{X})$\;
       $\vec{Z_M}^{-1} = calculate~inversion~of~\vec{Z_M}$\;
       $\vec{\hat{w}} \gets \of^{(OLS,\vec{M}\vec{X},  \vec{M}\vec{y} )}$\;
       $\vec{r}^{(\vec{m})} = \vec{Y} - \vec{X} \vec{\hat{w}}$\;
      
    $S_{i,j}, i, j$ = 
       $OSAA$(
            $\vec{Z_M}^{-1}$, 
                $\vec{r}^{(\vec{m})}$, 
                $O_m$, 
                        $Z_m$, 
                        $\vec{X}$)\;

            \If{$ S_{i,j} \geq 0$ \textbf{or}  $l \geq i_{max}$ }{
                $\vec{m_{final}} \gets \vec{m}$\;
                $\vec{\hat{w}} \gets \of^{(OLS,\vec{M}\vec{X},  \vec{M}\vec{y} )}$\;
                $R \gets R \cup \{\vec{m_{final}} , \vec{\hat{w}} \}$\;
                \Break\;
            }
            \Else {
                $\vec{m} \gets \vec{m}^{(i,j)}$\;
            }
        }        
    }
    $\vec{m_{final}}, \vec{\hat{w}_{final}}  \gets $ select best from $R$ based on smallest value of $\oflts$\;
    \Return{ $\what{{final}}$\,, $\vec{m_{final}}$ }\;
\end{algorithm}



\begin{observation}  \label{time:complexity:fsa}
In the main loop beside running OSAA which time complexity is $\mathcal{O}(n^2p^2)$ we need to recalculate $\vec{\hat{w}}$ using the matrix inversion which time complexity is $\mathcal{O}(p^2n)$ (see observation \ref{time:complexity:ols:inversion}).
Main loop of the FSA runs up to $l$ iterations for each start $t$. So the time complexity of whole algorithm is  $\mathcal{O}( lt(n^2p^2 + p^2n)  )$. Because $l$ and $t$ are usually quite low, we can see that the FSA time complexity is dominated by the OSAA. 
\end{observation} 

In this section, we've described the FSA algorithm. In the next section, we introduce a very similar algorithm, but which have higher numerical stability and also contains various optimizations for higher performance.


















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% IMPROVED by FSA %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{OEA, MOEA, MMEA} 
%three exchanging algorithms from Agulló, 2000

In the FSA algorithm, we assumed that after each cycle of OSAA 
\ref{alg:optimal:improvement} we need to recalculate inversion $(\vec{X}^T\vec{X})$ together with $\vec{\hat{w}}$. In this section, we introduce a different approach described in \cite{agullo2001new} so that we can update it instead of recalculating. Moreover, these ideas lead to bounding condition of FSA which not only improves the speed but also gives options to construct different algorithms. 

In section \ref{section_feasible_solution} we introduce additive formula \ref{hawkins:rovnice} which corresponds to $\oflts(\vec{m}^{(i,j)}) - \oflts(\vec{m})$. Let us now try to obtain similar formula but with the focus on how the individual elements in our current algorithm change namely the inversion $\vec{Z^{-1}}$ and $\vec{\hat{w}}$. We also split the idea of swap the $i, j$ into the insertion of $j$ and remove of $i$. Thus we describe how individual elements are affected after adding one row and also removing one row. Moreover, during this derivation, we obtain two different approaches to calculate it. Both are important, and we recapitulate them after.




%%%%%%% MULTIPLICATIVE INCREMENT FORMULA   %%%%%%%%%

\subsection{Multiplicative formula}


\todo{rename H in FSA to Z}
$\vec{Z} = \vec{X}^T\vec{X}$
Let us denote $\vec{A} = (\vec{X}, \vec{y})$, a matrix $\vec{X}$ expanded by one column of corresponding dependent variables and and $\vec{\tilde{Z}} = \vec{A}^T\vec{A}$. Then

\[  \numberthis \label{matrixZ}
    \vec{\tilde{Z}} = \begin{bmatrix}
        \vec{Z} & \vec{X}^T\vec{y} \\
    \vec{y}^T\vec{X} & \vec{y}^T\vec{y}
  \end{bmatrix} .
\]
Notice that $\vec{Z}$ is symmetric square matrix $\in \mathbb{R}^ {p \times p}$, moreover we suppose that $Z$ is regular. $\vec{X}^T\vec{y}$ is $p$ dimensional column vector, $\vec{y}^T\vec{X}$ is $p$ dimensional row vector and $\vec{y}^T\vec{y}$ is a scalar.
OLS estimate $\vec{\hat{w}}$ is then given by
\begin{equation}
    \vec{\hat{w}^{(OLS, n)}} = \vec{Z}^{-1} \vec{X}^T\vec{y}
\end{equation}
and the RSS by 

\begin{equation}
    RSS = \vec{y}^T\vec{y} - \vec{y}^T\vec{X}\vec{\hat{w}}.
\end{equation}
Also, note that all necessary matrix multiplications are already in the matrix $\vec{\tilde{Z}}$.

Let us show that $RSS$ can  be expressed as ratio of determinants $\det(\vec{\tilde{Z}})$ and $\det(\vec{Z})$ (using determinant rule for block matrices) so that 

\begin{align*} \numberthis \label{rssdeterminant} 
    RSS &= \frac{\det(\vec{\tilde{Z}})}{\det(\vec{Z})} \\ &= \frac{
    \det\begin{pmatrix}
            \vec{Z} & \vec{X}^T\vec{y} \\
            \vec{y}^T\vec{X} & \vec{y}^T\vec{y}
        \end{pmatrix}
        }{\det\left(\vec{M}\right)}\\
         &=\frac{ \det(\vec{Z}) \det\left(\vec{y}^T\vec{y} -  \vec{y}^T\vec{X} \vec{Z}^{-1} \vec{y}\right) }
        {\det\left(\vec{Z}\right)} \\ 
        &= \vec{y}^T\vec{y} - \vec{y}^T\vec{X}\vec{\hat{w}}.
\end{align*}

If we assume that $RSS > 0$ then $\vec{\tilde{Z}}^{-1}$ can be expressed as 

\[ \numberthis
    \vec{\tilde{Z}}^{-1} = 
    \begin{bmatrix}
        \vec{Z}^{-1}+\dfrac{\vec{\hat{w}} \vec{\hat{w}}}{RSS} & - \dfrac{\vec{\hat{w}}^T}{RSS} \\[6pt]
        - \dfrac{\vec{\hat{w}^{T}}}{RSS} & \dfrac{1}{RSS}
    \end{bmatrix}.
\]
For following equations it is important to notice that for any two row vectors $\vec{c_i} = (\vec{x_i}, y_i)$  and $\vec{c_j} = (\vec{x_j}, y_j)$, $\vec{c_i}, \vec{c_j} \in \mathbb{R}^{1 \times p+1}$ is 

\begin{equation}
    \vec{c_i} \vec{\tilde{Z}}^{-1} \vec{c_i^T} = \dfrac{ ( y_i - \vec{x_i}\vec{\hat{w}} )^2 }{RSS}  + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}
\end{equation}
and
\begin{equation}
    \vec{c_j} \vec{\tilde{Z}}^{-1} \vec{c_j^T} = \dfrac{ ( y_j - \vec{x_j}\vec{\hat{w}} ) ( y_i - \vec{x_i}\vec{\hat{w}} ) }{RSS}  + \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}
\end{equation}



%%%%%%% INCLUDING OBSERVATION %%%%%%%%%%%

\subsubsection*{Including the observation}

Using above let us express how the determinant $\det(\vec{Z})$ and the inverse of the $vec{Z}^{-1}$ changes when observation $\vec{c_i} = (\vec{x_i}, y_i)$ is added to the matrix $\vec{A}$. First let us notice that if we add this row to $\vec{A}$, then  $\vec{Z}$ changes as

\[ \numberthis \label{addedrow}
\begin{bmatrix}
    x_{11} & x_{12} & \dots  & x_{1n} & x_{i_1}  \\
    x_{21} & x_{22} & \dots  & x_{2n} & x_{i_2} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{p1} & x_{p2} & \dots  & x_{pn} & x_{i_p}     
\end{bmatrix}
\begin{bmatrix}
    x_{11} & x_{12}  & \dots  & x_{1p} \\
    x_{21} & x_{22}  & \dots  & x_{2p} \\
    \vdots  & \vdots & \ddots & \vdots \\
    x_{n1} & x_{n2}  & \dots  & x_{np} \\
    x_{i_1} & x_{i_2}  & \dots  & x_{i_p}
\end{bmatrix}
 = \vec{X}^T\vec{X} + \vec{x_i^T}\vec{x_i} = \vec{Z} + \vec{x_i^T}\vec{x_i},
\]
so determinant with appended row changes as
\begin{equation} \label{udpateddeterminant}
    \det(\vec{Z} + \vec{x_i^T}\vec{x_i}) = det(\vec{Z})(1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}).
\end{equation}
Finally the inversion $\vec{Z}^{-1}$ can be obtained using Sherman-Morrison formula \cite{bartlett1951inverse} so that 
\begin{equation} \label{shermanmorris}
    (\vec{Z} + \vec{x_i^T}\vec{x_i})^{-1} = \vec{Z}^{-1} - \dfrac{\vec{Z}^{-1}\vec{x_i^T}\vec{x_i}\vec{Z}^{-1}}{1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}}
\end{equation}

It is now convenient to denote 
\begin{equation}
    b = \dfrac{-1}{(1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T})},  b \in \mathbb{R},
\end{equation}
and 
\begin{equation} \label{agullo_u}
    \vec{u} = \vec{Z}^{-1}\vec{x_i^T},      \vec{u} \in \mathbb{R}^{p \times 1}.
\end{equation}

Then \eqref{shermanmorris} can be written as
\begin{equation} \label{inversionplus}
    (\vec{Z} + \vec{x_i^T}\vec{x_i})^{-1} = \vec{Z}^{-1} + b\vec{u}\vec{u^T}.
\end{equation}

Given that last missing piece to express updated $\vec{\hat{w}}$ is appended $\vec{X}^T\vec{y}$ by one row, which can be simply expressed by same idea as \eqref{addedrow} so that updated $\vec{\hat{w}}$ which we denote as $\vec{\overline{\hat{w}}}$ is 
\begin{equation}
    \vec{\overline{\hat{w}}} = (\vec{Z}^{-1} + b\vec{u}\vec{u^T})(\vec{X^T}\vec{y} + y_i\vec{x_i^T}).
\end{equation}
This can be simplified so that we get
\todo{mention the mistake in original paper? w + (y - xw)bu}
\begin{equation} \label{thetaplus}
    \vec{\overline{\hat{w}}} = \vec{\hat{w}} - (y_i - \vec{x_i}\vec{\hat{w}})b\vec{u}.
\end{equation}
Least but not last we want to express updated $RSS$ which we denote as $\overline{RSS}$. This can be done easily from \ref{rssdeterminant} and \ref{udpateddeterminant} as 

\begin{equation}
    \overline{RSS} =  RSS + \dfrac{(y_i - \vec{x_i}\vec{\hat{w})^2}}{(1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T})}.
\end{equation}
It is convenient to mark 
\begin{equation} \label{gamma:plus}
    \gamma^{+}(\vec{c_i}) = \dfrac{(y_i - \vec{x_i}\vec{\hat{w})^2}}{(1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T})},
\end{equation}
so that 
\begin{equation} \label{rssplus}
    \overline{RSS} =  RSS + \gamma^{+}(\vec{c_i})
\end{equation}
We can see that $\gamma^{+}(\vec{c_i})$ measures how $RSS$ increase after we append our dataset with observation $\vec{c_i}$, thus $\gamma^{+}(\vec{c_i}) \geq 0$



%%%%%%% EXCLUDING OBSERVATION %%%%%%%%%%%

\subsubsection*{Excluding the observation}

Because we want to express both increment and decrement change in our dataset, let us now focus how $RSS$, $\vec{Z^{-1}}$ and $\vec{\hat{w}}$  changes after we exclude one observation. 

Consider that we already included one observation $\vec{c_i}$ in our dataset and mark $\vec{\overline{Z}} = \vec{Z} + \vec{x_i} \vec{x_i^T} $. If we exclude one observation $\vec{c_j} = (\vec{x_j}, y_j) \in \mathbb{R}^{p+1 \times 1} $ from already updated matrix $\vec{A}$ then the determinant $\det(\vec{\overline{Z}})$  changes as

\begin{equation} 
    \det(\vec{\overline{Z}} - \vec{x_j^T}\vec{x_j}) = det(\vec{\overline{Z}})(1 - \vec{x_j}\vec{\overline{Z}}^{-1}\vec{x_j^T})
\end{equation}
and the inversion changes (again, according to Sherman-Morrison formula) as 
\begin{equation}  
    (\vec{\overline{Z}} - \vec{x_j^T}\vec{x_j})^{-1} = \vec{\overline{Z}}^{-1} + \dfrac{\vec{\overline{Z}}^{-1}\vec{x_j^T}\vec{x_j}\vec{\overline{Z}}^{-1}}{1 - \vec{x_j}\vec{\overline{Z}}^{-1}\vec{x_j^T}}.
\end{equation}

Once again, it is convenient to denote
\begin{equation}
    \overline{b} = \dfrac{-1}{(1  \vec{x_j}\vec{\overline{Z}}^{-1}\vec{x_j^T})},  \overline{b} \in \mathbb{R},
\end{equation}
and 
\begin{equation}
    \vec{\overline{u}} = \vec{\overline{Z}}^{-1}\vec{x_j^T},      \vec{\overline{u}} \in \mathbb{R}^{p \times 1},
\end{equation}
so that we can write
\begin{equation} \label{updatedinversion2}
    (\vec{\overline{Z}} + \vec{x_j^T}\vec{x_j})^{-1} = \vec{Z}^{-1} - \overline{b}\vec{\overline{u}}\vec{\overline{u}^T}.
\end{equation}

Using the same approach as before we can denote express down-dated estimate which we denote as $\vec{\overline{\overline{\hat{w}}}}$

\begin{equation} \label{thetaminus}
    \vec{\overline{\overline{\hat{w}}}} =  +\vec{\overline{\hat{w}}} (y_j - \vec{x_j}\vec{\overline{\hat{w}}}) \overline{b} \vec{\overline{u}}.
\end{equation}

Finally lets also express updated $\overline{RSS}$ which we denote as $\overline{\overline{RSS}}$. This can be done easily from \ref{rssdeterminant} and \ref{udpateddeterminant} and \eqref{updatedinversion2} as 

\begin{equation}
    \overline{\overline{RSS}} =  \overline{RSS} - \dfrac{(y_j - \vec{x_j}\vec{\overline{\hat{w}})^2}}{(1 - \vec{x_j}\vec{\overline{{Z}}}^{-1}\vec{x_j^T})}.
\end{equation}

Let us mark

\begin{equation} \label{gamma:minus}
    \gamma^{-}(\vec{c_j}) = \dfrac{(y_j - \vec{x_j}\vec{\overline{\hat{w}})^2}}{(1 - \vec{x_j}\vec{\overline{{Z}}}^{-1}\vec{x_j^T})},
\end{equation}
so that 
\begin{equation} \label{rssminus}
    \overline{\overline{RSS}} =  \overline{RSS} - \gamma^{-}(\vec{c_j}).
\end{equation}



%%%%%%      S W A P P I N G    T W O   O B S E R V A T I O N S     %%%%%

\subsubsection*{Swapping two observations}

Let us now express the equation for including and excluding observation at once. First, notice that from \eqref{updatedinversion2} we can express 

\begin{equation} \label{xjoverlinez}
    \vec{x_j}\vec{\overline{{Z}}}^{-1}\vec{x_j^T}) =  \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}) - 
    \dfrac{( \vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2}{1 +  \vec{x_i}\vec{Z}^{-1}\vec{x_i^T})},
\end{equation}
 so that 

 \begin{gather}
 \begin{align*} \numberthis
    &\det(\vec{Z} + \vec{x_i^T}\vec{x_i} - \vec{x_j^T}\vec{x_j}) = \\
    \det(\vec{Z})(1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T} -& \vec{x_j}\vec{Z}^{-1}\vec{x_j^T} +  ( \vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2 - \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}\vec{x_j}\vec{Z}^{-1}\vec{x_j^T} )
\end{align*}
\end{gather}

Finally we can express the $\overline{\overline{RSS}} $ as

\begin{equation} \label{updaterss}
    \overline{\overline{RSS}}  = RSS\rho(\vec{c_i}, \vec{c_j}),
\end{equation}
where
\begin{equation} \label{agullo:rovnice}
    \rho(\vec{c_i}, \vec{c_j}) =
     \dfrac
     {(1+\vec{x_i}\vec{Z}^{-1}\vec{x_i^T} + \dfrac{e_j^2}{RSS})
        (1 - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T} - \dfrac{e_i^2}{RSS} )+
        (\vec{x_i}\vec{Z}^{-1}\vec{x_j^T} + \dfrac{e_i e_j}{RSS} )^2}
    {1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}  - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}  + ( \vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2 -   \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}\vec{x_j}\vec{Z}^{-1}\vec{x_j^T} },
\end{equation}
where $e_i = y_i - x_i\vec{\hat{w}}$ and $e_j = y_j - x_j\vec{\hat{w}}$.

We can see that this formula is similar to the \eqref{hawkins:rovnice} but here the $\rho(\vec{c_i}, \vec{c_j})$ represents multiplicative increment. Moreover $0 < \rho(\vec{c_i}, \vec{c_j}) $ and if $0 < \rho(\vec{c_i}, \vec{c_j})< 1$ then the swap leads to improvement in terms of feasible solution.

We are now able to modify the FSA so that we do not need to recompute $\vec{\hat{w}}$ and inversion $(\vec{X}^T \vec{X})^{-1}$ but we can update it. Authors call this algorithm optimal exchange algorithm (OEA)





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%     O E A    A L G O R I TH M        %%%%%%%%%%%%%

\subsection{The OEA and its properties}
We can apply theory from the previous section to the FSA. Sets $O_m$ and $Z_m$ (see section \ref{section_feasible_solution}) contains indexes of observations to exclude and include respectively.
The matrix $\vec{Z_M}$ can be used instead of the matrix $\vec{Z}$ and
 $\vec{X_M}$ with $\vec{y_M}$ to calculate $\vec{\hat{w}}$. 

In terms of the FSA \ref{alg:feasible_solution} we first need to change the OSAA.  There are only minor tweaks. First, we need to pass one more argument, and that is $RSS$. Second, We want to find minimal $\rho(\vec{c_i}, \vec{c_j})$ calculated by \eqref{agullo:rovnice} so that $0 < \rho(\vec{c_i}, \vec{c_j}) < 1 $. Other parts of the algorithm remains the same. Time complexity thus remains the same.

This algorithm produces $\rho(\vec{c_i}$, and indexes  $i_{swap} \in O_m$ and $j_{swap} \in Z_m$ of observations we want to swap. Given that, we can update the $RSS$ by \eqref{updaterss}, $\vec{Z_M^{-1}}$ by \eqref{inversionplus} and \eqref{updatedinversion2} and finally update $\vec{\hat{w}}$ by \eqref{thetaplus} and \eqref{thetaminus}.

Let us now talk about the time complexity of such a solution. As we said, the time complexity of modifies OSAA is $\mathcal{O}( n^2 p^2)$. Thus there is no asymptotic improvement. On the other hand, there are some significant constants outside the OSAA so let us look at how they improve.

When an improvement is found, thus when $0 < \rho(\vec{c_i}, \vec{c_j}) < 1 $, then we update $RSS$, which is constant. Next we update inversion by
\eqref{inversionplus} which is $\mathcal{O}(4p^2)$ and \eqref{updatedinversion2} which is also $\mathcal{O}(4p^2)$.  Finally we need to update $\vec{\hat{w}}$ by \eqref{thetaplus} and \eqref{thetaminus} which are both $\mathcal{O}(p^2 +p)$. 

\begin{observation}
Time complexity of updating all the quantities is 
$\mathcal{O}(8p^2 + 2p^2 + p) \sim \mathcal{O}(p^2)$.
That is quite an improvement if we compare it to time complexity $\mathcal{O}(p^2n)$ of updating those quantities in the FSA (see observation~\ref{time:complexity:fsa}).
\end{observation}


Note that right now it actually does not matter if we use additive formula \eqref{hawkins:rovnice} with the stopping criterion $\Delta S^{(\vec{m})}_{i,j} \geq 0$---thus unmodified OSAA---or multiplicative formula \eqref{agullo:rovnice} with the stopping criterion $\rho(\vec{c_i}, \vec{c_j}) \geq 1 $. Both results we can update $RSS$ and other quantities can be used to calculate both formulas. 

However, the advantage of the multiplicative formula \ref{agullo:rovnice} is that we can use the following bounding condition to improve the performance of the modified OSAA.


%%%%%       B O U N D I N G   C O N D I T I ON    S P E E D U P       %%%%%

\subsubsection*{Bounding condition improvement}

The $\rho(\vec{c_i}, \vec{c_j})$ is expressed as a ratio. We can see that in the numerator we have

\begin{equation}
    (1+\vec{x_i}\vec{Z}^{-1}\vec{x_i^T} + \dfrac{e_j^2}{RSS})
        (1 - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T} - \dfrac{e_i^2}{RSS} )+
        (\vec{x_i}\vec{Z}^{-1}\vec{x_j^T} + \dfrac{e_i e_j}{RSS} )^2,
\end{equation}
and because $ \dfrac{e_i e_j}{RSS} )^2 \geq 0$ then whole numerator is greater or equal to
\begin{equation}
    (1+\vec{x_i}\vec{Z}^{-1}\vec{x_i^T} + \dfrac{e_j^2}{RSS})
(1 - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T} - \dfrac{e_i^2}{RSS} ).
\end{equation}
On the other hand, we can see that denominator is

\begin{equation}
    1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}  - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}  + (\vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2 -   \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}\vec{x_j}\vec{Z}^{-1}\vec{x_j^T}, 
\end{equation}
and because $(\vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2$ and $\vec{x_i}\vec{Z}^{-1}\vec{x_i^T}\vec{x_j}\vec{Z}^{-1}\vec{x_j^T} $ are actually inner products of $\vec{x_i}$ and $\vec{x_j}$  ($\vec{Z}^{-1}$ is positive definite) thus
\begin{equation}
    (\vec{x_i}\vec{Z}^{-1}\vec{x_j^T})^2 \leq \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}\vec{x_j}
\end{equation}
using the Cauchy-Schwarz inequality. This means that denominator is less or equal to
\begin{equation}
    1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}  - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}.
\end{equation}
Given that we can denote $\rho_b(\vec{c_i}, \vec{c_j})$  as

\begin{equation} \label{boundingcondition}
\rho_b(\vec{c_i}, \vec{c_j}) = \dfrac{(1+\vec{x_i}\vec{Z}^{-1}\vec{x_i^T} + \dfrac{e_j^2}{RSS})
    (1 - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T} - \dfrac{e_i^2}{RSS} )}{1 + \vec{x_i}\vec{Z}^{-1}\vec{x_i^T}  - \vec{x_j}\vec{Z}^{-1}\vec{x_j^T}} \quad \leq \rho(\vec{c_i}, \vec{c_j}).
\end{equation}

The actual speed improvement is then given by that we do not need to compute $\vec{x_i}\vec{Z}^{-1}\vec{x_j^T}$ in each of $h(n-h)$ pairs swap comparison. As we know this quantity cannot be computed outside the loop; thus it is the reason for such a high time complexity. The modified OSAA can be further modified as follows.

First we set $\rho_{min} :=1$. Then for each pair we only compute $\rho_b(\vec{c_i}, \vec{c_j})$ and if it is greater of equal to $\rho_{min}$ we can continue to next pair without computing $\rho(\vec{c_i}, \vec{c_j})$. 
It is very useful because all quantities necessary for calculating $\rho_b(\vec{c_i}, \vec{c_j})$ can be precalculated outside of the loop.

If $\rho_b(\vec{c_i}, \vec{c_j})$ is less that  $\rho_{min}$ we actually compute $\rho(\vec{c_i}, \vec{c_j})$ and set $\rho_{min} := \rho(\vec{c_i}, \vec{c_j})$.

That means in the double loop which time complexity is $\mathcal{n^2}$  we do not always need to calculate $\rho(\vec{c_i}, \vec{c_j})$ which time complexity is $\mathcal{n^2}$. This does not improve the asymptotic time complexity, but (as can be seen in the table \todo{crate experiment}) it can improve the speed of the algorithm. 

Finally, let us note that \cite{agullo2001new} calls this algorithm with bounding condition as \defterm{modified optimal exchange algorithm} (MOEA).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%     M M E A        %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Minimum-maximum exchange algorithm}
The minimum-maximum exchange algorithm (MMEA) is very similar to the FSA respectively to its modified version the OEA. The main difference is the greediness of this algorithm. What we mean by that is that this algorithm does not find the optimal swap, but rather first find the $\vec{c_j}$  which increase the $RSS$ by the minimum value and include this observation.
Next, it finds $\vec{c_j}$ such that in decrease $RSS$ by maximum and exclude this observation.

The minimum increase can be found by calculating $\gamma^{+}(\vec{c_i}) $ by \ref{gamma:plus} for each trimmed observation in  $Z_m$.

Next, this observation is included, so we get $h+1$ untrimmed observations. We update 
$\vec{Z_M^{-1}}$ to  $\vec{\overline{Z_M}}^{-1}$ by \eqref{inversionplus} and  and $\vec{\hat{w}}$ to $\vec{\overline{\hat{w}}}$ by \eqref{thetaplus}.

Then we can find maximum  $\gamma^{-}(\vec{c_j})$ by \eqref{gamma:minus} among $O_m^{(i)}$ where $m^{(i)}$ denotes included observation $c_i$.

Next, we can update $\vec{\overline{Z_M}}^{-1}$ to the   $\vec{\overline{\overline{Z_M}}}^{-1}$ by \eqref{updatedinversion2} and 
$\vec{\overline{\hat{w}}}$ to $\vec{\overline{\overline{\hat{w}}}}$ by 
\eqref{thetaminus}.

Finally, we can update $RSS$ to $\overline{\overline{RSS}}$ by \ref{rssplus} and  \ref{rssminus}. We can repeat this process until $\gamma^{-}(\vec{c_j}) > \gamma^{+}(\vec{c_i})$.

\begin{observation}
    Because we are not iterating through all pairs but only over $n-h$ followed by $h+1$ subsets the loops takes only $\mathcal{O}(n)$ time. In the loops, we are computing $\gamma^{-}(\vec{c_j}) $ and $\gamma^{+}(\vec{c_i})$ both takes $\mathcal{O}(p^2)$ time. Outside the loops, all the each of the quantities we are updating take $\mathcal{O}(p^2)$ time.
That means the one loop of the whole algorithm last $\mathcal{O}(p^2n)$.
As in the case of the FSA, if we introduce parameters $t$ and $l$, then the time complexity of the algorithm is $\mathcal{O}(tlp^2n)$
\end{observation}




















\subsection{Different method of computation}

In last section way how to modify OEA so that we can update $\vec{\hat{w}}$ and inversion $(\vec{X}^T \vec{X})^{-1}$ was introduced. This however requires to compute inversion at the start of the algorithm (this is also the case for the FSA) As we know, calculating inversion is not practical due to low numerical stability. In practice we usually use the QR decomposition. In this section we describe how we can modify OEA to use the QR decomposition (the same idea can be applied to the MOEA and the MMEA). Let us start by describing Givens rotations algorithm in detail, because it is critical part of this modified computation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%    H E R E   W E R E   O L S   M E T H O D S %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% QR DECOMPOSITION - GIVENS ROTATION  %%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Givens Rotation} \label{givensrotation}
In this section, we describe Givens rotations in detail. Moreover, we also show how to update QR decomposition in terms of including and excluding row from factorized matrix $\vec{X}$ which help us in our algorithm.

We compute the QR factorization of $\vec{A} \in \mathbb{R}^{m \times n}$ which has full column rank so that we apply orthogonal transformation by a matrix $\vec{Q}^T$ so that \cite{hammarling2008updatingqr}

\begin{equation}
    \vec{Q}^T\vec{A} = \vec{R}
\end{equation}
 
where $\vec{Q}$ is product of orthogonal matrices. One such example of orthogonal matrix can be:

\begin{equation} \label{givensmatrix}
\vec{Q} = 
\begin{bmatrix} 
\cos(\varphi) & \sin(\varphi) \\
- \sin(\varphi) & \cos(\varphi)
\end{bmatrix}
\end{equation}
This matrix is indeed orthogonal since 
\begin{align*} 
\vec{Q}\vec{Q}^T = \vec{Q}^T\vec{Q}    
\end{align*}
 \begin{equation*}
    = \begin{bmatrix} 
        \cos(\varphi)^2 + \sin(\varphi)^2  & \sin(\varphi)\cos(\varphi) - \cos(\varphi)\sin(\varphi) \\
        \cos(\varphi)\sin(\varphi) - \sin(\varphi)\cos(\varphi) & \cos(\varphi)^2 + \sin(\varphi)^2
    \end{bmatrix} \\ = \vec{I}
    \numberthis
\end{equation*}
Moreover if we multiply this orthogonal with some column vector $\vec{x} \in \mathbb{R}^{2 \times 1}$ thus $\vec{Q}\vec{x}$ as a result we get vector of same length but is rotated clockwise by  $\varphi$ radians. When we say that vector have same length we mean than L-2 norm of such vector stays the same.This is important property of all orthogonal matrices. We can simply verify correctness of this claim. Let us have any orthogonal matrix $\vec{Q} \in \mathbb{R}^{m \times m}$ and any column vector $\vec{x} \in  \mathbb{R}^{m \times 1}$ then

\begin{equation}
    \norm{\vec{Q}\vec{x}}^2 = (\vec{Q}\vec{x})^T (\vec{Q}\vec{x}) = \vec{x}^T\vec{Q}^T\vec{Q}\vec{x} = 
    \vec{x}^T\vec{I}\vec{x} = \vec{x}^T\vec{x} = \norm{\vec{x}}^2
\end{equation}

So we would like to create a series of orthogonal matrices which gradually rotates column vectors of $\vec{A}$, so we obtain zeros under diagonal. One such method - \defterm{Givens rotation} uses \defterm{Givens matricies} that zero one element under diagonal at a time.
The example of \ref{givensmatrix} is not random. Let us look at this orthogonal matrix one more time and let us multiply this matrix with some vector.

\begin{equation}
    \begin{bmatrix} 
        \cos(\varphi) & \sin(\varphi) \\
        - \sin(\varphi) & \cos(\varphi)
        \end{bmatrix}
        \begin{bmatrix} 
            a \\
            b
            \end{bmatrix} = 
            \begin{bmatrix} 
                r \\
                z
                \end{bmatrix}
\end{equation}

To introduce this zeroing effect, we need to rotate this vector so that it is parallel to $
\begin{bmatrix} 
    1 \\
    0
\end{bmatrix}
$. Now it is only up to find  $\cos(\varphi) a + \sin(\varphi) = r $.
As we know rotation with $\vec{Q}$ preserve L-2 norm. That means if we want to $z = 0$  then $r$ must be equal to L-2 norm of the vector  
$
\begin{bmatrix} 
    a \\
    b
\end{bmatrix}
$  thus $r = \sqrt{a^2 + b^2}$. 
Then the solution seems trivial. We do not even need to calculate $\varphi$ because if we put
\begin{equation} \label{givens_cos}
    \cos(\varphi) = \frac{a}{\sqrt{a^2 + b^2}} 
\end{equation}
and   
\begin{equation} \label{givens_sin}
    \sin(\varphi) = \frac{b}{\sqrt{a^2 + b^2}} 
\end{equation}
then 
\begin{align}
    r &= \frac{a^2}{\sqrt{a^2 + b^2}} + \frac{b^2}{\sqrt{a^2 + b^2}} = \sqrt{a^2 + b^2} \\
    z &= \frac{-ab}{\sqrt{a^2 + b^2}} +  \frac{ab}{\sqrt{a^2 + b^2}} = 0.
\end{align}

Practically used algorithm of computing $\cos{\varphi}$ and  $\sin{\varphi}$ is slightly different because we want to prevent overflow. This algorithm is described by the following pseudocode:


\begin{algorithm}[H]
    \label{alg:givens}
    % \SetKwInOut{Input}{input}
    % \SetKwInOut{Output}{output}
    \KwIn{$a, b$}
    \KwOut{$cos, sin$ }
    \caption{Rotate}
    $sin \gets \emptyset$\;
    $cos \gets \emptyset$\;
    \uIf{$b == 0$}{
        $sin \gets 0$\;
        $cos \gets 1$\;
    }
    \uElseIf{$abs(b) \geq abs(a)$}{
        $cotg \gets \frac{a}{b}$\;
        $sin \gets \frac{1}{\sqrt{1 + (cotg)^2}}$\;
        $cos \gets sin\ cotg$\;
    }
    \Else{
        $tan \gets \frac{b}{b} $\;
        $cos \gets \frac{1}{\sqrt{1 + (tan)^2}}$\;
        $sin \gets cos\ tan$\;
    }
    \Return{ $cos$\,, $sin$ }\;
\end{algorithm}

We can scale this same idea to higher dimensions. Let us denote matrix $\vec{Q}(i, j) \in \mathbb{R}^{m \times m}$ defined as 
\renewcommand{\kbldelim}{[}% Left delimiter
\renewcommand{\kbrdelim}{]}% Right delimiter
\[
  \vec{Q}(i, j) = \kbordermatrix{
    &   &       & i &       & j &      &  \\
    & 1 & \dots & 0 & \dots & 0 &\dots & 0 \\
    & \vdots & \ddots & \vdots & & \vdots & & \vdots \\
   i  & 0 & \dots & c & \dots & s & \dots & 0 \\
     & \vdots &  & \vdots & \ddots & \vdots & & \vdots \\
     j  & 0 & \dots & -s & \dots & c & \dots & 0 \\
     & \vdots &  & \vdots &  & \vdots & \ddots & \vdots \\
     & 0 & \dots & 0 & \dots & 0 &\dots & 1 \\
  }
\]

where $c =     \cos(\varphi) $ and $ s = \sin(\varphi)$ for some $\varphi$. That means this matrix is orthogonal. Now if we have some column vector $\vec{x} \in \mathbb{R}^{m \times 1}$ and calculate $c$ and $s$ by means of \ref{givens_cos} and \ref{givens_sin} for $a := x_i $ and $b := x_j$. 
Finally if we multiply $\vec{Q}(i, j) \vec{x} = \vec{p}$ we can see that $\vec{p}$ is same as vector $\vec{x}$ except $p_i$ and $p_j$ so that:

\[
    \vec{Q}(i, j) \vec{x} = \vec{Q}(i,j) 
    \kbordermatrix{
    &   \\
    & x_1 \\
    & \vdots \\
   i  & x_i  \\
     & \vdots \\
     j  & x_j  \\
     & \vdots \\
     & x_m \\
  }
    =  \vec{p} = \kbordermatrix{
    &   \\
    & x_1 \\
    & \vdots \\
   i  & r  \\
     & \vdots \\
     j  & 0  \\
     & \vdots \\
     & x_m \\
  }
\]

where $r = \sqrt{x_i^2 + x_j^2}$. If we have matrix $\vec{A} \in \mathbb{R}^{n \times p}$ instead of only one column $\vec{x}$ it works the same way. So if we have such matrix we need to create matrix $    \vec{Q}(i, j)$ for each $a_{ij}$ under the diagonal in order to create upper triangular matrix. Usually we are zeroing columns so that we start with $a_{12}$ then $a_{13} \ldots a_{1_n}$.
Then we start with second column $a_{23}$ and so on. That means we need to create in total exactly
$e = \frac{p^2 - p}{2} + np - p^2$ matrices $\vec{Q}(i, j)$. We can denote this sequence of matrices as $\vec{Q_1},  \vec{Q_2}, \ldots \vec{Q_e}$.
The QR decomposition then looks like

\begin{equation}
    \vec{Q_e}\ldots\vec{Q_2}\vec{Q_1}\vec{A} = \vec{R}
\end{equation}
where $\vec{Q_e}\ldots\vec{Q_2}\vec{Q_1}$ is actually $\vec{Q}^T$. So $\vec{Q}$ is obtained by 

\begin{equation}
 \vec{Q} = \vec{Q_1^T}\vec{Q_2^T}\ldots\vec{Q_e^T}.
\end{equation}

We can see that for each row of the matrix we need one additional matrix $\vec{Q_i}$ and with such matrix, we are multiplying only two rows. Moreover, the rows are getting shorter after each finished column. So the time we see that time complexity is equal to
\todo{todo write the pseudocode}
\begin{equation}
    \sum\limits_{i=1}^n  \sum\limits_{j=i+1}^p 6(n-i+1) \approx 6np^2 - 3np^2 - 3p^3 + 2p^3 = 3np^2 - p^3
\end{equation}
\todo{move all this about rotations somewhere else}

So we are now in a situation when we have QR decomposition of $\vec{X}$, and we need to exchange $i$th row for $j$th row. We can simulate this by first adding $j$th row and consequently removing $i$th row.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%    QR INSERT     %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Updating QR decomposition}
First, let us discuss how to update QR decomposition when a row is added. 
If we add a row to $\vec{A}$ our decomposition looks like
\begin{equation}
    \vec{R^{+}} = \vec{Q_e}\ldots\vec{Q_2}\vec{Q_1}\vec{A_{new}}  = 
    \begin{bmatrix}
        \times & \cdots & \cdots & \cdots & \times \\
        0 &\times & \cdots & \cdots & \times \\
        \vdots& 0&\ddots & \cdots & \vdots \\
        \vdots& \vdots&0 & \ddots &  \vdots \\
        \vdots& \vdots& \vdots& 0& \times \\
        \vdots& \vdots& \vdots& \vdots& 0  \\
        \vdots& \vdots& \vdots& \vdots& \vdots \\
        \times & \cdots & \cdots & \cdots & \times \\
    \end{bmatrix}
\end{equation}

So all we need to do is create matrices $\vec{Q_{e+1}}\ldots\vec{Q_{e+p}}$ to zero newly added row.
So the $R$ is then equal to $\vec{R_{new}} = \vec{R}\vec{Q_{e+1}}\vec{Q_e}\ldots\vec{Q_2}\vec{Q_1}\vec{A}$. \todo{better notation}
$\vec{Q}$ is updated in the same way thus $\vec{Q_{new}} = \vec{Q}\vec{Q_e^T}\vec{Q_{e+1}^T}\ldots\vec{Q_{e+p}^T}$.

\todo{todo pseudokod? moc se mi nechce :-D}
\begin{algorithm}[H]
    \label{addingrowqr}
    % \SetKwInOut{Input}{input}
    % \SetKwInOut{Output}{output}
    \KwIn{$\vec{Q}, \vec{R}, \vec{x_i}$}
        \KwOut{$\vec{Q^{+}}, \vec{R^{+}}$ }
      \caption{QR insert}

    \Return{ $\vec{Q^{+}}$\,, $\vec{R^{+}}$ }\;
\end{algorithm}

Computing $\vec{R^{+}}$ is $\mathcal{O}(p^2)$ and computing $\vec{Q^{+}}$ is $\mathcal{O}(np)$.

\begin{note} \label{qnotrequired}
    For adding rows, the matrix $\vec{Q}$ is not needed so that we can update $\vec{R}$ to $\vec{R^{+}}$. Later we show that this is useful. \todo{say where}
\end{note}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%    QR DELETE     %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

When we are deleting the row $\vec{x_i}$ from matrix $\vec{A}$ we can use following trick 
\cite{hammarling2008updatingqr}. First, we move such row as the first row of the matrix $\vec{A}$ so ve create permutation matrix $\vec{P}$ so that 
\begin{equation}
    \vec{P}\vec{A} = \begin{bmatrix}
        \vec{x_i} \\
        \vec{A(1:i-1 , 1:p)} \\
        \vec{A(1:i+1 , 1:p)} 
    \end{bmatrix}
    = 
    \begin{bmatrix}
        \vec{x_i} \\
        \vec{A^{-}}
    \end{bmatrix}
    = \vec{P}\vec{Q}\vec{R}
\end{equation} 
where $\vec{A(a:b , 1:p)} $ means rows of matrix $\vec{A}$ from $a$ to $b$.
No we can se that we only need to zero first row $\vec{q_1}$ of matrix $\vec{Q}$. We can do this by $n-1$ matrixes matrixes $\vec{Q}(i,j) \in \mathbb{R}^{n \times n}$ so that

\begin{equation}
    \vec{Q}(n-1,n) \ldots \vec{Q}(1,2)\vec{q_1^T} =     \begin{bmatrix}
        1 \\
        0\\
        \vdots \\
    \end{bmatrix}
\end{equation}

To propagate the change into $\vec{R}$ we then consequently need to update $\vec{R}$ so that
\begin{equation}
    \vec{Q}(n-1,n) \ldots \vec{Q}(1,2)\vec{q_1^T} \vec{R} =     \begin{bmatrix}
        \times \\
        \vec{R^{-}}\\
        \vdots \\
    \end{bmatrix}.
\end{equation}
The result is
\begin{equation}
    \vec{P}\vec{A} = (\vec{P}\vec{Q}  \vec{Q^T}(1,2) \ldots   \vec{Q^T}(n-1,n)) (\vec{Q}(n-1,n) \ldots \vec{Q}(1,2)\vec{q_1^T} \vec{R} ) \\
    = 
    \begin{bmatrix}
        1 & 0 \\
        0 & \vec{Q^{-}}
    \end{bmatrix}
    \begin{bmatrix}
        \times \\
        \vec{R^{-}}
    \end{bmatrix},
\end{equation}
so that 
\begin{equation}
    \vec{A^{-}} = \vec{Q^{-}}\vec{R^{-}}
\end{equation}

\todo{napsat pseudokod. opet se mi moc necche :-D }
\begin{algorithm}[H]
    \label{removingrow}
    % \SetKwInOut{Input}{input}
    % \SetKwInOut{Output}{output}
    \KwIn{$\vec{Q}, \vec{R}, i$}
        \KwOut{$\vec{Q^{-}}, \vec{R^{-}}$ }
      \caption{QR delete}

    \Return{ $\vec{Q^{-}}$\,, $\vec{R^{-}}$ }\;
\end{algorithm}

Computing $\vec{R^{-}}$ is $\mathcal{O}(p^2)$ and computing $\vec{Q^{-}}$ is $\mathcal{O}(np)$.












%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%% IMPROVED FSA - QR DECOMPOSITION  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Calculation of improved FSA using QR}
So let's focus on our problem. As we said using decomposition is better than calculating inversion primarily due to higher numerical stability. \todo{nevim jestli zminovat ze muzu delat i SVD dekompozici ktera je nejstabilnejsi ze vsech...}
We'll show that both FSA as well as improved FSA can be calculated using QR decomposition.

Let's start with \ref{agullo:rovnice}. Here inversion is need to calculate 
$\vec{x_i}\vec{Z}^{-1}\vec{x_i^T}$, $\vec{x_j}\vec{Z}^{-1}\vec{x_j^T}$ and $\vec{x_i}\vec{Z}^{-1}\vec{x_j^T}$. This can also be done without inversion, only using the decomposition. We can write this equation
\begin{equation} \label{solveimi}
	\vec{x_i}\vec{Z}^{-1}\vec{x_i^T} = \vec{v}^T\vec{v} 	\iff \vec{x_i}(\vec{R}^T\vec{R})^{-1}\vec{x_i^T} = \vec{v}^T\vec{v}
\end{equation} 
where $\vec{v}$ can be obtained by solving lower triangular system
\begin{equation} \label{vsolution}
	\vec{R}^T\vec{v} = \vec{x_i}^T.
\end{equation} 
The same can be done with  $\vec{x_j}\vec{Z}^{-1}\vec{x_j^T}$. Last but not least we need to solve 
$\vec{x_i}\vec{Z}^{-1}\vec{x_j^T}$. We can write this 
\begin{equation}
	\vec{x_i}\vec{Z}^{-1}\vec{x_j^T} = \vec{x_j}\vec{u} 	\iff \vec{x_i}(\vec{R}^T\vec{R})^{-1}\vec{x_j^T} = \vec{x_j}^T\vec{u}
\end{equation} 
Where column vector $\vec{u}$ is defined by \ref{agullo_u}. We can see that 

\begin{equation}
	\vec{u} = \vec{Z}^{-1}\vec{x_i^T} 	\iff (\vec{R}^T\vec{R})^{-1}\vec{x_i^T} = \vec{u}
\end{equation}
so that $\vec{u}$ can be obtained ba solving upper triangular system 

\begin{equation} \label{solve_u_qr}
	\vec{R}\vec{u} = \vec{v}
\end{equation}
where $\vec{v}$ is solution of \ref{vsolution}. Other quantities of \ref{agullo:rovnice} does not require $\vec{Z}^{-1}$.

When optimal exchange $\vec{z_i}, \vec{z_j}$ is found then we need to update $RSS$ which can be done same way by \ref{updaterss}. Updating $\vec{\hat{w}}$ to  $\vec{\overline{\hat{w}}}$ by \ref{thetaplus} requires
$\vec{u}$  which in this case we calculate by \ref{solve_u_qr} and $b$ which requires  $\vec{x_j}\vec{Z}^{-1}\vec{x_j^T}$ but that we've also already covered by idea \ref{solveimi}. Analogous operations can be used to update $\vec{\overline{\hat{w}}}$ to $\vec{\overline{\overline{\hat{w}}}}$  by means of \ref{thetaminus}. Only thing wee need to realize that we can express \ref{xjoverlinez} as $\vec{x_j}\vec{\overline{{Z}}}^{-1}\vec{x_j^T} = \vec{x_i}\vec{{{Z}}}^{-1}\vec{x_i^T} + b(\vec{u}\vec{x_j}^2)$.

We can't use updating inversion because we don't have one so we need to somehow update our QR decomposition. This can be done by both Householder rotation as well as by Givens rotation, but because our $\vec{R}$ is already sparse matrix, Givens rotations are ideal tool for this. We've already described how this can be done. First we can use \ref{addingrowqr} to add row $\vec{x_i}$ to the decomposition and consequently \ref{removingrow} to remove $\vec{x_j}$  from the decomposition. We can see that this is slower than updating inversion directly. On the other hand this solution is numerically stable and requires less time than computing factorization again from scratch. 

Finally let's talk about matrix $\vec{\tilde{Z}}$ which we used \ref{matrixZ} for derivation of our equations. If we use \ref{qrcholesky} observation then we realize that if we make QR factorization of $\vec{A} = (\vec{X}, \vec{y})$ then

\begin{equation}
	\vec{\tilde{Z}} = 
	\begin{bmatrix}
		\vec{Z} & \vec{X}^T\vec{y} \\
    \vec{y}^T\vec{X} & \vec{y}^T\vec{y}
	\end{bmatrix} 
	= 
	\begin{bmatrix}
		\vec{R^T} & 0 \\
    \vec{\phi^T} & r
	\end{bmatrix} 
	\begin{bmatrix} 
		\vec{R} & \vec{\phi} \\
     0 & r
	\end{bmatrix} 
\end{equation}

where 

\begin{equation}
	\vec{\tilde{R}} = 
	\begin{bmatrix}
		\vec{R} & \vec{\phi} \\
     0 & r
	\end{bmatrix} 
	= \vec{\tilde{Q^T}}\vec{A}
\end{equation}

is matrix from QR factorization of $\vec{A}$. 
Next we can realize that $\vec{R} \in \mathbb{R}^{p \times p}$ is actually matrix $\vec{R}$ which we can obtain by QR factorization of $\vec{X}$. 
Moreover $\vec{\phi} \in \mathbb{R}^{p \times 1}$ is column vector which is actually equal to
\begin{equation}
	\vec{\phi} =  \vec{Q^T}\vec{y}
\end{equation}
where $\vec{Q}$ is matrix $\vec{Q}$ from QR factorization of $\vec{R}$.
Due to this fact $\vec{\hat{w}}$ is solution of upper triangular system 
\begin{equation}
	\vec{R}\vec{\hat{w}} = \vec{\phi}
\end{equation}

Finally $r \in \mathbb{R}$ is scalar such that 

\begin{equation}
	r^2 = \vec{y}^T\vec{y} - \vec{y}^T\vec{X}\vec{\hat{w}} = RSS
\end{equation}

\begin{remark} \label{qnotrequiredspeedupremark}
	We can see that all quantities we use in the algorithm can be obtained from matrix $\vec{\tilde{R}}$ moreover if we didn't need to remove rows from $\vec{X}$ then matrix $\vec{\tilde{Q}}$ would not need \ref{qnotrequired}, thus speedup while inserting and rows would be possible. 
\end{remark}

Now we've described all properties of improved FSA algorithm. We can see that two different methods of computation are possible. One, using inversion $\vec{Z}^{-1}$ is faster, but numerically less stable, second using QR decomposition is slower due to updating matrix $\vec{R}$ and matrix $\vec{Q}$. We've also shown that using matrix $\vec{\tilde{R}}$ is useful because it contains all necessary quantities for the improved FSA algorithm, although it won't improve the speed of the algorithm. Later we'll introduce different algorithm where we won't need to remove rows from $\vec{X}$ so then we'll be ab1`A																		`e to use this observation in our favor.

\todo{time complexity QR qwerty}
\todo{time complexity tildeZQR is the same as QR}






\section{Combined algorithm}
\todo{ todo this section and move it somewhere else qwerty}
Here we describe what we indicated above, and that is a combination of both previous algorithms.
Two options. 
z
\begin{enumerate}
    \item Let fast-lts converge and then run FSA and let it converge
    \item Let fast-lts converge make one step of FSA and try fast-LTS again and iterate between this
\end{enumerate}

The second option would be faster? Let us think about it.



\section{BAB algorithm}  % Also from Agulló, 2000
\todo{zminit podobnost s: Hoffmann, Kontoghiorghes, 2010 (Matrix strategies SUR)}
\section{BSA algorithm}
In this section we'll introduce another exact algorithm. It have a little different approach than previous algorithms. First of all we'll built a theoretical basis for this algorithm.



% \subsubsection{Domain of $\oflts$}
% We've already showed that we can derivate LTS objective function so it's domain is on discrete set  $Q^{(n,h)}$ see \ref{oflts_discrete}. We'll now show that we can transform this objective function to the non-discrete form so that $\oflts (\vec{w}) \vec{w} \in \mathbb{R}^p$ \cite{klouda2015exact}.
% We'll do this by relation $Z \subset \mathbb{R}^p \times Q^{(n,h)}$ so that $(\vec{w}, \vec{m}) \in Z$ only and if only
% \begin{equation}
% 	\sum\limits_{i=1}^h r^{2}_{(i:n)}(\vec{w})  =  \sum\limits_{i=1}^n m_{i}r^{2}_{i}(\vec{w}).
% \end{equation}
% $Z$ is not a mapping. To show this we can take simple example so that $r^{2}_{h} = r^{2}_{h+1}$. Then we have for given $\vec{w}$ two different vectors $\vec{m}$ that are in relation with it.

% If we want to find domain where $Z$ is mapping, we simply define $\mathcal{U} \subset \mathbb{R}^{p}$ as a maximal set where $Z$ is a mapping. Next we define 
% $\mathcal{H} = \mathbb{R}^{p}  \setminus   \mathcal{U}$
% as a complement of $\mathcal{U}$.

% Let's now describe some properties based on which $\vec{w}$ is in $\mathcal{U}$ or $\mathcal{H}$ set.

% \begin{theorem} \label{klouda1}
% 	$\vec{w} \in \mathcal{U}, \vec{w} \in \mathbb{R}^{p}$ if and only if 
% 	$r^{2}_{(h:n)}(\vec{w}) < r^{2}_{(h+1:n)}(\vec{w})$
% \end{theorem}
% \begin{proof}
% 	As shown in example above. If $r^{2}_{i}(\vec{w}) = r^{2}_{(h:n)}(\vec{w}) = r^{2}_{(h+1:n)}(\vec{w}) = r^{2}_{j}(\vec{w}), i,j \in   \{{1,2,\ldots , n\}} $ then $(\vec{w}, \vec{m_1}) \in Z$ and also $(\vec{w}, \vec{m_2}) \in Z$ where $\vec{m_1}$ has ones at indexes of $h$ smallest residuals and $\vec{m_2}$ has ones at the same indexes except swap $i$th one with $j$th zero.
% \end{proof}

% \begin{corollary}
% 	Complement of vectors $\vec{w}$ from theorem \ref{klouda1} are vectors such that 
% 	\begin{equation}
% 		r^{2}_{(h:n)}(\vec{w}) = r^{2}_{(h+1:n)}(\vec{w})
% 	\end{equation}
% 	thus
% 	\begin{equation}
% 		\mathcal{H} = \{{ \vec{w} \in \mathbb{R}^{p} | r^{2}_{(h:n)}(\vec{w}) = r^{2}_{(h+1:n)}(\vec{w}) \}}.
% 	\end{equation}
% 	That means that for each $\vec{w} \in \mathcal{H}$ there are two different $(\vec{x_i}, y_i)$ and  $(\vec{x_j}, y_j)$ so that
% 	\begin{equation}
% 		(y_i - \vec{x_i} \vec{w})^2 = r^{2}_i(\vec{w}) =  r^{2}_{(h:n)}(\vec{w}) = r^{2}_{(h+1:n)}(\vec{w}) =  r^{2}_j(\vec{w}) = (y_j - \vec{x_j} \vec{w})^2.
% 	\end{equation}

% 	We can see that 
% 	\begin{equation}
% 		(y_i - \vec{x_i} \vec{w})^2 =  (y_j - \vec{x_j} \vec{w})^2 \iff	  y_i \pm y_j + (\vec{x_i} \pm \vec{x_j})  \vec{w} = 0
% 	\end{equation}
% \end{corollary}

% Moreover if we assume that for all $i,j, \in \{{1,2 \ldots, n \}}$  if $ i \neq j$ then $\vec{x_i} \neq \pm \vec{x_j}$ and $\norm{\vec{x_i}} \neq 0$ then $ y_i \pm y_j + (\vec{x_i} \pm \vec{x_j})  \vec{w} = 0$ represents a hyperplane. 

% It's easy to show that set $\mathcal{U}$ is open and 	lebesgue measure of $\mathcal{H}$  is $0$ \cite{klouda2015exact}. Moreover $\mathcal{H}$ splits $\mathbb{R}^{p}$ into finite number of $m$ open disjoint subsets of $\mathcal{U}$. 

% \begin{definition}
% 	Let's define set of $m \in \mathbb{N}$ sets $\mathcal{U}^{(set)} = \{{ U_i\}}_{i=1}^{m}$ so that
% 	all $U_i$ are open and connected sets, $U_i$ $U_j$ are disjoined thus $U_i \cap  U_j = \emptyset$, $\cup_{i=1}^{m}	U_i = \mathcal{U}$ and $\cup_{i=1}^{m}	\partial U_i =  \mathcal{H}$.

% 	We define neighbor sets $U_i, U_j, i \neq j$ so that $ U_i \cap U_j \neq \emptyset$. 

% 	We also define $M^{(min)}$ set of $m$ vectors $\vec{m} \in Q^{(n,h)}$ so that 
% 	\begin{equation*}
% 		M^{(min)} = \{{ \vec{m_1}, \ldots, \vec{m_m} | \vec{m_i} = Z(\vec{w}) \vec{w} \in U_i  \}}.
% 	\end{equation*}
% 	where $Z(\vec{w})$ represent unique vector $\vec{m_i}$. That means $Z(\vec{w}) = Z(\vec{w^{\prime}})$ for all $w, w^{\prime} \in U_i$.
% \end{definition}

% For a better understanding definition above see \missingfigure{p=1 n=5 h=3. visualise all $U_i$ and elements of $\mathcal{H}$}

% \begin{theorem}
% 	If the matrix $\vec{X}$ have $h$-full rank then for every local minima at point satisfying weak-necessary condition $\vec{w_{0}}$ of the lts objective function, then there exists a vector $\vec{m} \in Q^{(n,h)}$ such that $(\vec{w_{0}}, \vec{m}) \in Z$.
% \end{theorem}

% Proof can be found in \cite{klouda2015exact}. 

% Given that we can see that 

% \begin{equation}
% 	\minim_{\vec{m} \in Q^{(n,h)}} \of^{(OLS, \vec{M}\vec{X}, \vec{M}\vec{Y})} (\vec{w}) =
% 	\minim_{m \in M^{(min)}} \of^{(OLS, \vec{m^i}\vec{X}, \vec{m^i}\vec{Y})} (\vec{w})
% \end{equation}

% where $\vec{M} = diag(\vec{m})$, $\vec{m^i} = diag(m)$. We can see that set $M^{(min)}$ is very useful because we can minimize only on this set and not on whole $Q^{(n,h)$.

% Then minimizing objective function $\oflts(\vec{m})$ from \ref{oflts_discrete} can be reformulated as 
% \begin{equation}
% 	\minim_{\vec{m} \in Q^{(n,h)}}  \oflts(\vec{m}) = \minim_{m \in M^{(min)}} \oflts(m)
% \end{equation}

% We now have all the theory necessary for describing two dimensional version of the algorithm. We'll now describe this algorithm and next we'll provide some more theory necessary to extend this algorithm to higher dimensions.

% First we'll show how to find fde
