% \listoftodos

\chapter{Least trimmed squares}
In this chapter will be introduced one of the most common regression analysis models which is linear regression model. This model tries to model relationship between one variable which is considered to be dependent and one or more variables which are considered to be explanatory. Relationship is based on model function with parameters which are not known in advance and are estimated from data. We will also introduce one of the most commons methods of finding those parameters in this model, namely ordinary least squares.
\section{Linear regression model}
\begin{definition}\label{definition:lr_model}
    Linear regression model is 
    \[ 
        y_i = \vec{x_i}^T\vec{w} + \varepsilon_i\\, \ \  i = 1,2,\ldots,n \numberthis
    \]
where $y_i \in \mathbb{R}$ is random variable which we denote as \defterm{dependent variable}, vector $\vec{x_i}^T = (x_1, x_2, \ldots, x_p)$ is column vector of \defterm{explanatory variables} and $\varepsilon \in \mathbb{R}$ random variable called \defterm{noise} or \defterm{error}. Vector $\vec{w} = (w_1, w_2, \ldots, w_p)$ is vector of parameters called  \defterm{regression coefficients}. It is common to write whole model in a matrix form. 

\begin{equation}
    \vec{y} = \vec{X}\vec{w} + \vec{\varepsilon}    \numberthis
\end{equation} where

\[ 
\vec{y} = \begin{bmatrix}
    y_{1} \\
    y_{2} \\
    \vdots \\
    y_{n}
  \end{bmatrix}\\,
 \vec{X} = \begin{bmatrix}
    \vec{x_1}^T \\
    \vec{x_2}^T \\
    \vdots \\
    \vec{x_n}^T
\end{bmatrix}
=
\begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1p} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2p} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{n1} & x_{n2} & x_{n3} & \dots  & x_{np}
\end{bmatrix}\\,
\vec{w} = \begin{bmatrix}
    w_{1} \\
    w_{2} \\
    \vdots \\
    w_{p}
  \end{bmatrix}\\,
  \vec{\varepsilon} = \begin{bmatrix}
    \varepsilon_{1} \\
    \varepsilon_{2} \\
    \vdots \\
    \varepsilon_{n}
  \end{bmatrix}
\]

We consider $\vec{x_{i1}} = 1$ to be a constant. Corresponding $w_{1}$ we call \defterm{intercept}. We refer this model as \defterm{model with intercept}. It is possible to have model without intercept but then hyperplane generated by this model goes through origin. That is usually rare case so we always use intercept unless we explicitly mention it. Due to this we can assume that in model with intercept expected value is  $\mathbb{E}(\varepsilon_i) = 0$.
\end{definition}


\subsection{Prediction with linear regression model}
Linear regression model contains vector of real regression coefficients which we don't know and which we need to estimate in order to be able to use model for prediction. So let us assume that we already have estimated regression coefficients which we mark as $\vec{\hat{w}}$. Then we're able to predict value of $y$ by
\begin{equation}
    \hat{y} = \vec{\hat{w}}^T\vec{x} \numberthis
\end{equation}
$\hat{y}$ denotes that it is predicted value. Real value of $y$ is given by 

\begin{equation}
    y = \vec{w}^T\vec{x} + \varepsilon \numberthis
\end{equation}

Because we assume linear dependence between dependent variable $y$ and explanatory variables $\vec{x}$ then what makes $y$ random variable is actually random variable $\varepsilon$. Because we use model with intercept thus with $\mathbb{E}(e) = 0$ we can see that 

\begin{equation}
\mathbb{E}(y) =\mathbb{E}(\vec{x}^T\vec{w}) + \mathbb{E}(\varepsilon) = \mathbb{E}(\vec{x}^T\vec{w})
\numberthis
\end{equation}

so $\hat{y}$ is actually a point estimation of expected value of $y$.


\section{Ordinary least squares}
We want to estimate $\vec{w}$ so that error of the model will be minimal. Measurement of this error is most often done by \defterm{loss function} 

\begin{equation}
L : \mathbb{R}^2 \rightarrow  \mathbb{R} \numberthis
\end{equation}

Which in case of ordinary least squares is quadratic loss function $L(y, \hat{y}) := (y - \hat{y})^2$. We refer to $r(\vec{\hat{w}}) = y - \hat{y} = y - \vec{\hat{w}}^T\vec{x} $ as to residual. 

So the idea of this method lies in fact that we want to minimize error given by sum of squared residuals commonly known as residual sum of squares $RSS$

\begin{equation}
    RSS = \sum\limits_{i=1}^n r_i^2 = (y_i - \vec{w}^T\vec{x_i})^2 \numberthis
\end{equation}

\begin{definition} If we think about RSS as the function of $\vec{w}$ we'll get \defterm{objective function} for ordinary least squares. 
    \begin{equation}
        \of^{(OLS,\vec{X}, \vec{y})}(\vec{w}) = \sum\limits_{i=1}^n (y_i - \vec{w}^T\vec{x_i})^2 \numberthis
    \end{equation}
Where $\vec{w}$ has the meaning of function argument, not the real regression coefficients.
\end{definition}

And minimum of this function we can denote as 

% \begin{equation}
%     \vec{\hat{w}} = \argmin_{\vec{\w} \in \mathbb{R}^p} \of^{(OLS,\vec{X}, \vec{y})}(\vec{w}) \sum\limits_{i=1}^n r_i^2 = \sum\limits_{i=1}^n (y_i - \vec{w}^T\vec{x_i})^2 
% \end{equation}



\begin{equation}
    \vec{\hat{w}} = \argmin_{\vec{w} \in \mathbb{R}^p} \of^{(OLS,\vec{X}, \vec{y})}(\vec{w}) 
    \sum\limits_{i=1}^n r_i^2 = \sum\limits_{i=1}^n (y_i - \vec{w}^T\vec{x_i})^2 
\end{equation}

which is basically a definition of ordinary least squares estimate.

\begin{definition} Estimate of regression parameters using ordinary least squares (OLS) on $n$ observations is defined as
    \begin{equation}
        \vec{\hat{w}^{OLS, n}} =  \sum\limits_{i=1}^n (y_i - \vec{\tilde{w}}^T\vec{x_i})^2 \numberthis
    \end{equation}
\end{definition}

Let's now talk about finding solution for this. If we want to find minimum of this function, first we need to find gradient by calculating all partial derivatives
\begin{equation}
    \frac{\partial {\of^{(OLS,\vec{X}, \vec{y})}} }{\partial w_j} = \sum\limits_{i=1}^n 2(y_i - \vec{w}^T\vec{x_i})(-x_{ij}) \\, \ \ j \in \{{1,2,\ldots,p\}}. \numberthis
\end{equation}
By this we obtain gradient 

\begin{equation}
    \nabla \of^{(OLS,\vec{X}, \vec{y})} = -\sum\limits_{i=1}^n 2(y_i - \vec{w}^T\vec{x_i})\vec{x_i}. \numberthis
\end{equation}

As a \defterm{normal equation} we mark gradient equal to zero.

\begin{equation}
    -\sum\limits_{i=1}^n 2(y_i - \vec{w}^T\vec{x_i})\vec{x_i} = 0 \numberthis
\end{equation}

Which we can write in matrix form as 

\begin{equation}
    \vec{X}^T\vec{y} - \vec{X}^T\vec{X}\vec{w} = 0 \numberthis  \label{abcd}
\end{equation}

\todo{proof it is a local minimum}

Let's now construct hessian matrix using second-order partial derivatives. With

\begin{equation}
    \frac{\partial^2{\of^{(OLS,\vec{X}, \vec{y})}} }{\partial w_h\partial w_j} = \sum\limits_{i=1}^n 2(- x_{ik})(-x_{ij}) \\, \ \ h \in \{{1,2,\ldots,p\}}. \numberthis
\end{equation}
we get 

\begin{equation}
    \vec{H_{\of^{(OLS,\vec{X}, \vec{y})}}} = 2\vec{X}^T\vec{X}. \numberthis 
\end{equation}

We can see that hessian $\vec{H_{\of^{(OLS,\vec{X}, \vec{y})}}}$ is always positive semi-definite because for all $\vec{s} \in \mathbb{R}^p$


\begin{equation}
    \vec{s}^T(2\vec{X}^T\vec{X})\vec{s} = 2(\vec{X}\vec{s})^T(\vec{X}\vec{s}) =  \numberthis
\end{equation}

It's easy to proof that twice differentiable function is convex if and only if the hessian of such function is positive semi-definite. Due to that we can say that solution of \eqref{abcd} gives us not only local minimum but global minimum. 

If we assume that $\vec{X}^T\vec{X}$ is regular matrix, then its inverse exists and solution can be explicitly written as


\begin{equation}
    \vec{\hat{w}}^{(OLS,n)} = (\vec{X}^T\vec{X})^{-1}\vec{X}^T\vec{y}. \numberthis \label{wols}
\end{equation}

where $(OLS,n)$ denotes that $\vec{\hat{W}}$ is estimate of $\vec{w}$ calculated using ordinary least squares on n observations. 

Moreover we can see that if $\vec{X}^T\vec{X}$ is regular matrix, then a hessian $\vec{H_{\of^{(OLS,\vec{X}, \vec{y})}}}$ is positive definite because for all nonzero $\vec{s} \in \mathbb{R}^p$


\begin{equation}
    \vec{s}^T 2\vec{X}^T\vec{X} \vec{s} > 0
\end{equation}

thus $\vec{\hat{w}}^{(OLS,n)}$ in \eqref{wols} is strict global minimum.

% norms can be used to create distance as d(x,y) = || x - y ||
% but not all distances have corresponding norm! trivial example. d(x,x) = 0, d(x,y) = 1

\todo{ols estimate is BLUE if we assume this and that}

We've shown that $\vec{\hat{w}}^{(OLS,n)}$ can be computed directly by multiplying $\vec{A} = \vec{X}^T\vec{X}$ and $\vec{B} = \vec{X}\vec{y}$ and then by finding inversion of $\vec{A}$  and finally multiplying $\vec{A}^{-1}\vec{B}$. This is certainly possible but not the best solution because this method is not numerically stable. We'll describe this in detail in section \ref{ols:computing}

\section{Robust statistics}
To work properly standard statistic methods expects some assumptions and fail if those assumptions are not met. Robust statistic are statistics that produce acceptable results even if data are from some unconventional distributions or if data contains errors which are not normally distributed. We should be highly motivated to use such methods because in the real world we are often forced to work with such data. Classical statistics methods provide poor results with such data. 

Ordinary least squares method has such assumptions about data. This method expects that
\begin{equation}
    \varepsilon_i \sim \mathcal{N}(\mu,\,\sigma^{2})\\, \ \ i \in \{{1,2,\ldots,n\}}. \numberthis
\end{equation}

Condition of expected value to be zero is met, if we use model with intercept. But we can't be sure if errors are normally distributed. Moreover we expect that variance stays the same for all $\varepsilon_i\,, i \in \{{1,2,\ldots,n\}}$. That is what we call \defterm{homoscedasticity}. Beside assumptions about $\varepsilon$ we expect that explanatory variables $x_1,x_2,\ldots n_p$ are not linear dependent, or in terms of statistics, uncorrelated.

Before we explain what happens if those conditions are not met or met only partially let's describe one of the most common reason why assumptions are false.

\subsection{Outliers}
We stated a lot of assumptions that are required for ordinary least squares to give 
good estimate of $\vec{\hat{w}}$. Unfortunately in  real conditions these assumptions are often false so that ordinary least squares don't produce acceptable results. One of the most common reasons of false assumptions are abnormal observations called outliers.

Outliers are very common and they are for instance erroneous measurements such as transmission errors or noise. Other very common reason is that nowadays we are mostly given data which were automatically processed by computers. Sometimes we are also presented data which are heterogenous in the sense  that they contain data from multiple regression models. In certain sense outliers are inevitable. One would say that we would be able to eliminate them by precise examination, repair or removal of such data. That is possible in some cases, but most of the data we are dealing with are simply too big to check and more often we don't know how the data should look. In higher dimensions it is very difficult to find outliers. There are some methods that tries to find outliers but such methods are only partially efficient. Let us note that robust models are sometimes not only useful to create models that are not being unduly affected by presence of outliers but also capable of identifying data which seems to be outliers. 

We have some terminology to describe certain types of outliers. We use terminology from \cite{rouss:1990}. Let's have observation $(y_i, \vec{x_i})$. If observation is not outlying in any direction we call it \defterm{regular observation} . If it is outlying in $\vec{x_i}$ direction we call it \defterm{leverage point}. We have two types of leverage points. If $\vec{x_i}$ is outlying but $(y_i, \vec{x_i})$ follows liner pattern we call it  \defterm{good leverage point}. If it does not follow such a pattern we call it \defterm{bad leverage point}. Finally if $(y_i, \vec{x_i})$ is  outlying only in $y_i$  direction, we call it a \defterm{vertical outliers}.

\subsection{Measuring robustness}
There are couple of tools to measure robustness of statistics. The most popular one is called \defterm{breakdown point}. Then there are \defterm{empirical influence function} and \defterm{influence function and sensitivity curve}. For sake of simplicity we'll talk only about breakdown point right now. 

\begin{definition}
    Let $T$ be a functional, $\vec{x} = (x_1, x_2,\ldots,x_n)\,, x_i \in \mathcal{X}$ be an n-dimensional random sample and $T_n(\vec{x})1$ value of this functional with parameter $\vec{x}$. The breakdown point of $T$ at sample $x$ can be defined using sample $\vec{x_{new}}^(k)$ where we exchange k points from original sample $\vec{x}$ with random values $x_i$. We get $T_n(\vec{x_{new}}^{(k)})$. Then the \defterm{breakdown point} is 


\begin{equation}
    \bdpoint(T,\vec{x_n} ) = \frac{1}{n} \min S_{T, \vec{x_n}}  \numberthis
\end{equation}

where 

\begin{equation}
   S_{T, \vec{x_n}, D} =   \{{k \in \{{1,2, \ldots ,n\}} : \sup_{ \vec{x_{new}}^{(k)} } \norm{T_n(\vec{x}), T_n(\vec{x_{new}}^{(k)})} = \infty   \}}   \numberthis
\end{equation}


\end{definition}

This definition is very general but let's specify it to our linear regression problem. It basically says that breakdown point is proportion of minimal number of observations needed to be switched for some others so then the estimator will give incorrect results. More robust estimators have higher breakdown point. 

It is intuitive that breakdown point cannot be higher than $0.5$ \cite{rouss:1986} because if we exchange more than $50\%$ of the data, we wouldn't be able to distinguish between the original and exchanged data. Moreover the original data would become minority over exchanged.


In context of ordinary least squares estimator (OLS) it's easy to show that one outlier is enough to increase value of $T$ to any desired value thus 
\begin{equation}
    \bdpoint(OLS,\vec{x_n} ) = \frac{1}{n}\ .
\end{equation}
\todo{include some nice image showing 1/n robustness}
For increasing number of data samples $n$ this tends to zero. We can see that ordinary least squares estimator is not resistant to outliers at all. Due to this fact multiple estimators similar to OLS have been proposed. 


\section{Least trimmed squares}
Least trimmed squares (LTS) estimator builds on OLS but is more robust. In this section we'll define LTS estimator and show that it's breakdown point is variable and can go upt to maximum possible value of breakdown point, thus $0.5$.


\begin{definition} Let's have $\vec{X} \in \mathbb{R}^{n,p}$, $\vec{y} \in \mathbb{R}^{n,1}$, 
    $\vec{w} \in \mathbb{R}^p$ and $h$, $ n/2 \leq h \leq n$. Objective function of LTS is then denoted as 
    \begin{equation} 
        \of^{(LTS,\vec{X}, \vec{y})}(\vec{w}) =  \sum\limits_{i=1}^h r_{i:n}^2(\vec{w})  
    \end{equation}
\end{definition}

Even though that objective function of LTS seems similar to the OLS objective function, finding minimum is much more complex because the smallest residuals are dependent on $\vec{w}$. This fact makes from finding LTS estimate non-convex optimization problem and finding global minimum is NP-hard. 
\todo{reference}

From this form of objective function we can hardly see how to optimize it, because we have $h$ smallest residuals $r_i$ which are dependent on vector of regression coefficients $\vec{w}$. 
Let's try to transform this objective function to another form and from which it'll be clear what we need to do in order to minimize this objective function.

% First let's introduce extended version of model of linear regression. We have couple of options. One option is that if we want to take into account that our model include outliers we can assume that errors $\varepsilon$ are not from normal distribution $\mathcal{N}(0,\,\sigma^{2})$ but from some other unknown distribution which is most probably heavy-tailed. But that wouldn't help us much. Other option is thats called \defterm{mean shift outlier model} \cite{hawkins:1994} that extends standard linear regression model so that

% \begin{equation} \label{extendedlrmodel}
%    \vec{y} = \vec{X}\vec{w} + \vec{I}\vec{\lambda} + \vec{\varepsilon}
% \end{equation}
% where  $\vec{\lambda} \in \mathbb{R}^{n\times 1}$ is vector which contains values which represents outlier displacements. We assume that at least half of the values of the vector are zeros. 
% Due to that errors are still normally distributed.
% \todo{or like this ?}

Let's assume for now that we know $\vec{\hat{w}^{LTS, h, n}}$ vector of regression coefficients. With this in mind create permutation of $\hat{n} = \{{1,2,\ldots, n\}}$ such that 
\begin{equation}
    r_{i:n} = r_{\pi(j)}\\, \ \ j \in \hat{n}
\end{equation}
Next let's mark set
 \begin{equation}
   Q^{(n,h)} = \{{\vec{m} \in \mathbb{R}^n \\, m_i \in \{{0, 1\}} \\,i \in \hat{n} \\, \vec{1}\vec{m} = h \}}
\end{equation}
which is simply set of all vectors $\vec{m} \in \mathbb{R}^n$ which contain $h$ ones and $n-h$ zeros. We can now mark $\vec{m}_{LTS} \in Q^{(n,h)}$  such that  $m^{(LTS)}_j = 1$ when $\pi(j) \leq h$ and $m^{(LTS)}_j = 0$ otherwise. Then

\begin{equation} \label{ltshat}
    \vec{\hat{w}^{LTS, h, n}} =  \sum\limits_{i=1}^h r_{i:n}^2(\vec{w}) =  \sum\limits_{i=1}^n 
    m^{(LTS)}_i r_{i}^2(\vec{w})
\end{equation}
What that simply means that if we know vector $\vec{m}_{LTS}$ than we can simply compute LTS estimate as OLS estimate with $\vec{X}$ and $\vec{Y}$ multiplied by $\vec{M}_{LTS} = diag(\vec{m}^T_{LTS})$. Thus 

\begin{equation} 
    \vec{\hat{w}^{LTS, h, n}} = (\vec{X}^T\vec{M}^T_{LTS}\vec{X}\vec{M}_{LTS})^{-1}\vec{X}^T\vec{M}^T_{LTS}\vec{M}_{LTS}\vec{y}
\end{equation}
That means finding minimum of LTS objective function is equal to solving OLS over all vectors 
$\vec{m} \in Q^{(n,h)}$. Thus if we mark $\tilde{X} = \vec{M}\vec{X} $ and $\tilde{y} = \vec{M}\vec{y} $, then

\begin{align*} 
    \vec{\hat{w}^{LTS, h, n}}
     &=  \sum\limits_{i=1}^n 
     m_i r_{i}^2(\vec{w})\\
    &= \minim_{\vec{m} \in Q^{(n,h)}} 
    \Big( \argmin_{\vec{w} \in\mathbb{R}^p} 
    \of^{(OLS,\vec{M}\vec{X},  \vec{M}\vec{y} )} \Big)\\
    &= \minim_{\vec{m} \in Q^{(n,h)}} 
    \Big(\minim_{\vec{w} \in\mathbb{R}^p}  \norm{ \vec{M}\vec{y} -   \vec{M}\vec{X}\vec{w}  }^2 \Big)
\end{align*}

Finally we can minimize inner part by substituting $\vec{w}$ with \ref{ltshat}. By that we get

\begin{align*}
    \vec{\hat{w}^{LTS, h, n}}
    &=  \minim_{\vec{m} \in Q^{(n,h)}} 
    \Big( \norm{ \vec{M}\vec{y} -  \vec{M}\vec{X}(\vec{X}^T\vec{M}^T\vec{M}\vec{X})^{-1}\vec{X}^T\vec{M}^T\vec{M}\vec{y}  }^2 \Big)\\
    &= \minim_{\vec{m} \in Q^{(n,h)}} 
    \Big( \norm{ \vec{M}\vec{y} -  \vec{M}\vec{X}(\vec{X}^T\vec{M}\vec{X})^{-1}\vec{X}^T\vec{M}\vec{y}  }^2 \Big)
\end{align*}

We can easily see, that we've got objective function with argument $\vec{m} \in Q^{(n,h)}$. This objective function we'll mark as 

\begin{equation} 
    \oflts(\vec{m}) =  \norm{ \vec{M}\vec{y} -  \vec{M}\vec{X}(\vec{X}^T\vec{M}\vec{X})^{-1}\vec{X}^T\vec{M}\vec{y}  }^2.
\end{equation}
We can also clearly see that solution to minimizing this OF could by done straightforwardly by iterating over the  $Q^{(n,h)}$ set.

Unfortunately this set is very big namely $\binom{n}{h}$ so this approach will fail if the $n$ will be greater that $30$ or $40$. To overcome this problem multiple algorithms was proposed. Majority of them are probabilistic algorithms but beside that some exact algorithms were proposed. 

Before moving to the next chapter and start describing those algorithms lets point out some fact about the number of residuals $h$ and how it actually makes least trimmed squares robust estimator.
LTS reaches maximum breakdown point when $h = [(n/2] + [(p+1)/2]$ (where $[.]$ denotes largest integer function) and that is  which $0.5$. That means that up to $50\%$ of the data can be outliers. In practice this number is usually lower and if upper bound of percentage of outliers is known, then $h$ should be set to match this percentage.

% \begin{theorem} % rouss leroy 1984 page 132 (145)
%     Breakdown value of LTS estimator is equal to $h/n$.



