\listoftodos

\chapter{Least trimmed squares}
In this chapter will be introduced one of the most common regression analysis models which is linear regression model. This model tries to model relationship between one variable which is considered to be dependent and one or more variables which are considered to be explanatory. Relationship is based on model function with parameters which are not known in advance and are estimated from data. We will also introduce one of the most commons methods of finding those parameters in this model, namely ordinary least squares.
\section{Linear regression model}
\begin{definition}\label{definition:lr_model}
    Linear regression model is 
    \[ 
        y_i = \vec{x_i}^T\vec{w} + \varepsilon_i\\, \ \  i = 1,2,\ldots,n \numberthis
    \]
where $y_i \in \mathbb{R}$ is random variable which we denote as \defterm{dependent variable}, vector $\vec{x_i}^T = (x_1, x_2, \ldots, x_p)$ is column vector of \defterm{explanatory variables} and $\varepsilon \in \mathbb{R}$ random variable called \defterm{noise} or \defterm{error}. Vector $\vec{w} = (w_1, w_2, \ldots, w_p)$ is vector of parameters called  \defterm{regression coefficients}. It is common to write whole model in a matrix form. 

\[ 
    \vec{y} = \vec{X}\vec{w} + \vec{\varepsilon}    \numberthis
\] where

\[ 
\vec{y} = \begin{bmatrix}
    y_{1} \\
    y_{2} \\
    \vdots \\
    y_{n}
  \end{bmatrix}\\,
 \vec{X} = \begin{bmatrix}
    \vec{x_1}^T \\
    \vec{x_2}^T \\
    \vdots \\
    \vec{x_n}^T
\end{bmatrix}
=
\begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1p} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2p} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{n1} & x_{n2} & x_{n3} & \dots  & x_{np}
\end{bmatrix}\\,
\vec{w} = \begin{bmatrix}
    w_{1} \\
    w_{2} \\
    \vdots \\
    w_{p}
  \end{bmatrix}\\,
  \vec{\varepsilon} = \begin{bmatrix}
    \varepsilon_{1} \\
    \varepsilon_{2} \\
    \vdots \\
    \varepsilon_{n}
  \end{bmatrix}
\]
We consider $\vec{x_{i1}} = 1$ to be a constant. Corresponding $w_{1}$ we call \defterm{intercept}. We refer this model as \defterm{model with intercept}. It is possible to have model without intercept but then hyperplane generated by this model goes through origin. That is usually rare case so we always use intercept unless we explicitly mention it. Due to this we can assume that in model with intercept expected value is  $\mathbb{E}(\varepsilon_i) = 0$.
\end{definition}


\subsection{Prediction with linear regression model}
Linear regression model contains vector of real regression coefficients which we don't know and which we need to estimate in order to be able to use model for prediction. So let us assume that we already have estimated regression coefficients which we mark as $\vec{\hat{w}}$. Then we're able to predict value of $y$ by
\[
    \hat{y} = \vec{\hat{w}}^T\vec{x} \numberthis
\]
$\hat{y}$ denotes that it is predicted value. Real value of $y$ is given by 

\[
    y = \vec{w}^T\vec{x} + \varepsilon \numberthis
\]

Because we assume linear dependence between dependent variable $y$ and explanatory variables $\vec{x}$ then what makes $y$ random variable is actually random variable $\varepsilon$. Because we use model with intercept thus with $\mathbb{E}(e) = 0$ we can see that 
\[
\mathbb{E}(y) =\mathbb{E}(\vec{x}^T\vec{w}) + \mathbb{E}(\varepsilon) = \mathbb{E}(\vec{x}^T\vec{w})
\numberthis
\]
so $\hat{y}$ is actually a point estimation of expected value of $y$.


\section{Ordinary least squares}
We want to estimate $\vec{w}$ so that error of the model will be minimal. Measurement of this error is most often done by \defterm{loss function} 
\[ 
L : \mathbb{R}^2 \rightarrow  \mathbb{R} \numberthis
\]
Which in case of ordinary least squares is quadratic loss function $L(y, \hat{y}) := (y - \hat{y})^2$. We refer to $r(\vec{\hat{w}}) = y - \hat{y} = y - \vec{\hat{w}}^T\vec{x} $ as to residual. 

So the idea of this method lies in fact that we want to minimize error given by sum of squared residuals commonly known as residual sum of squares $RSS$
\[
    RSS = \sum\limits_{i=1}^n r_i^2 = (y_i - \vec{w}^T\vec{x_i})^2 \numberthis
\]

So if we think about RSS as about the function of $\vec{\hat{w}}$ we can mark

\[
    \vec{\hat{w}} = \argmin_{\vec{\tilde{w}} \in \mathbb{R}^p} RSS(\vec{\tilde{w}}) \sum\limits_{i=1}^n r_i^2 = 
    \sum\limits_{i=1}^n (y_i - \vec{\tilde{w}}^T\vec{x_i})^2 \numberthis
\]

which is basically a definition of ordinary least squares estimator.

\begin{definition} Estimate of regression parameters using ordinary least squares (OLS) on $n$ observations is defined as
    \begin{equation}
        \vec{\hat{w}^{OLS, n}} =  \sum\limits_{i=1}^n (y_i - \vec{\tilde{w}}^T\vec{x_i})^2 \numberthis
    \end{equation}
\end{definition}

Let's now talk about finding solution for this. If we want to find minimum of this function, first we need to find gradient by calculating all partial derivatives
\begin{equation}
    \frac{\partial {RSS} }{\partial w_j} = \sum\limits_{i=1}^n 2(y_i - \vec{w}^T\vec{x_i})(-x_{ij}) \\, \ \ j \in \{{1,2,\ldots,p\}}. \numberthis
\end{equation}
By this we obtain gradient 

\begin{equation}
    \nabla RSS = -\sum\limits_{i=1}^n 2(y_i - \vec{w}^T\vec{x_i})\vec{x_i}. \numberthis
\end{equation}

As a \defterm{normal equation} we mark gradient equal to zero.

\begin{equation}
    -\sum\limits_{i=1}^n 2(y_i - \vec{w}^T\vec{x_i})\vec{x_i} = 0 \numberthis
\end{equation}

Which we can write in matrix form as 

\begin{equation}
    \vec{X}^T\vec{y} - \vec{X}^T\vec{X}\vec{w} = 0 \numberthis  \label{abcd}
\end{equation}

\todo{proof it is a local minimum}

Let's now construct hessian matrix using second-order partial derivatives. With

\begin{equation}
    \frac{\partial^2{RSS} }{\partial w_h\partial w_j} = \sum\limits_{i=1}^n 2(- x_{ik})(-x_{ij}) \\, \ \ h \in \{{1,2,\ldots,p\}}. \numberthis
\end{equation}
we get 

\begin{equation}
    \vec{H_{RSS}} = 2\vec{X}^T\vec{X}. \numberthis 
\end{equation}

We can see that hessian $\vec{H_{RSS}}$ is always positive semi-definite because for all $\vec{s} \in \mathbb{R}^p$



\begin{equation}
    \vec{s}^T(2\vec{X}^T\vec{X})\vec{s} = 2(\vec{X}\vec{s})^T(\vec{X}\vec{s}) =  \numberthis
\end{equation}

It's easy to proof that twice differentiable function is convex if and only if the hessian of such function is positive semi-definite. Due to that we can say that solution of \eqref{abcd} gives us not only local minimum but global minimum. 

If we assume that $\vec{X}^T\vec{X}$ is regular matrix, then its inverse exists and solution can be explicitly written as


\begin{equation}
    \vec{\hat{w}}^{(OLS,n)} = (\vec{X}^T\vec{X})^{-1}\vec{X}^T\vec{y}. \numberthis \label{wols}
\end{equation}

where $(OLS,n)$ denotes that $\vec{\hat{W}}$ is estimate of $\vec{w}$ calculated using ordinary least squares on n observations. 

Moreover we can see that if $\vec{X}^T\vec{X}$ is regular matrix, then a hessian $\vec{H_{RSS}}$ is positive definite because for all nonzero $\vec{s} \in \mathbb{R}^p$


\begin{equation}
    \vec{s}^T 2\vec{X}^T\vec{X} \vec{s} > 0
\end{equation}

thus $\vec{\hat{w}}^{(OLS,n)}$ in \eqref{wols} is strict global minimum.

% norms can be used to create distance as d(x,y) = || x - y ||
% but not all distances have corresponding norm! trivial example. d(x,x) = 0, d(x,y) = 1

\todo{ols estimate is BLUE if we assume this and that}


\section{Robust statistics}
To work properly standard statistic methods expects some assumptions and fail if those assumptions are not met. Robust statistic are statistics that produce acceptable results even if data are from some unconventional distributions or if data contains errors which are not normally distributed. We should be highly motivated to use such methods because in the real world we are often forced to work with such data. Classical statistics methods provide poor results with such data. 

Ordinary least squares method has such assumptions about data. This method expects that
\begin{equation}
    \varepsilon_i \sim \mathcal{N}(\mu,\,\sigma^{2})\\, \ \ i \in \{{1,2,\ldots,n\}}. \numberthis
\end{equation}

Condition of expected value to be zero is met, if we use model with intercept. But we can't be sure if errors are normally distributed. Moreover we expect that variance stays the same for all $\varepsilon_i\,, i \in \{{1,2,\ldots,n\}}$. That is what we call \defterm{homoscedasticity}. Beside assumptions about $\varepsilon$ we expect that explanatory variables $x_1,x_2,\ldots n_p$ are not linear dependent, or in terms of statistics, uncorrelated.

Before we explain what happens if those conditions are not met or met only partially let's describe one of the most common reason why assumptions are false.

\subsection{Outliers}
We stated a lot of assumptions that are required for ordinary least squares to give 
good estimate of $\vec{\hat{w}}$. Unfortunately in  real conditions these assumptions are often false so that ordinary least squares don't produce acceptable results. One of the most common reasons of false assumptions are abnormal observations called outliers.

Outliers are very common and they are for instance erroneous measurements such as transmission errors or noise. Other very common reason is that nowadays we are mostly given data which were automatically processed by computers. Sometimes we are also presented data which are heterogenous in the sense  that they contain data from multiple regression models. In certain sense outliers are inevitable. One would say that we would be able to eliminate them by precise examination, repair or removal of such data. That is possible in some cases, but most of the data we are dealing with are simply too big to check and more often we don't know how the data should look. In higher dimensions it is very difficult to find outliers. There are some methods that tries to find outliers but such methods are only partially efficient. Let us note that robust models are sometimes not only useful to create models that are not being unduly affected by presence of outliers but also capable of identifying data which seems to be outliers. 

We have some terminology to describe certain types of outliers. We use terminology from \cite{rouss:1990}. Let's have observation $(y_i, \vec{x_i})$. If observation is not outlying in any direction we call it \defterm{regular observation} . If it is outlying in $\vec{x_i}$ direction we call it \defterm{leverage point}. We have two types of leverage points. If $\vec{x_i}$ is outlying but $(y_i, \vec{x_i})$ follows liner pattern we call it  \defterm{good leverage point}. If it does not follow such a pattern we call it \defterm{bad leverage point}. Finally if $(y_i, \vec{x_i})$ is  outlying only in $y_i$  direction, we call it a \defterm{vertical outliers}.

\subsection{Measuring robustness}
There are couple of tools to measure robustness of statistics. The most popular one is called \defterm{breakdown point}. Then there are \defterm{empirical influence function} and \defterm{influence function and sensitivity curve}. For sake of simplicity we'll talk only about breakdown point right now. 

\begin{definition}
    Let $T$ be a functional, $\vec{x} = (x_1, x_2,\ldots,x_n)\,, x_i \in \mathcal{X}$ be an n-dimensional random sample and $T_n(\vec{x})1$ value of this functional with parameter $\vec{x}$. The breakdown point of $T$ at sample $x$ can be defined using sample $\vec{x_{new}}^(k)$ where we exchange k points from original sample $\vec{x}$ with random values $x_i$. We get $T_n(\vec{x_{new}}^{(k)})$. Then the \defterm{breakdown} point is 


\begin{equation}
    \bdpoint(T,\vec{x_n} ) = \frac{1}{n} \min S_{T, \vec{x_n}}  \numberthis
\end{equation}

where 

\begin{equation}
   S_{T, \vec{x_n}, D} =   \{{k \in \{{1,2, \ldots ,n\}} : \sup_{ \vec{x_{new}}^{(k)} } \norm{T_n(\vec{x}), T_n(\vec{x_{new}}^{(k)})} = \infty   \}}   \numberthis
\end{equation}


\end{definition}

This definition is very general but let's specify it to our linear regression problem. It basically says that breakdown point is proportion of minimal number of observations needed to be switched for some others so then the estimator will give incorrect results. More robust estimators have higher breakdown point. 

It is intuitive that breakdown point cannot be higher than $0.5$ \cite{rouss:1986} because if we exchange more than $50\%$ of the data, we wouldn't be able to distinguish between the original and exchanged data. Moreover the original data would become minority over exchanged.


In context of ordinary least squares estimator (OLS) it's easy to show that one outlier is enough to increase value of $T$ to any desired value thus 
\begin{equation}
    \bdpoint(OLS,\vec{x_n} ) = \frac{1}{n}\ .
\end{equation}

For increasing number of data samples $n$ this tends to zero. We can see that ordinary least squares estimator is not resistant to outliers at all. Due to this fact multiple estimators similar to OLS have been proposed. 


\section{Least trimmed squares}
Least trimmed squares (LTS) estimator builds on OLS but is more robust. In this section we'll define LTS estimator and show that it's breakdown point is variable and can go upt to maximum possible value of breakdown point, thus $0.5$.


\begin{definition} Estimate of regression parameters using least trimmed squares (LTS) on $h$ observations out of $n$ is defined as
    \begin{equation}
        \vec{\hat{w}^{LTS, h, n}} =  \sum\limits_{i=1}^h r_{i:n}^2(\vec{w}) 
    \end{equation}

    where $h\\, n/2 \leq h \leq n$ is number of used observations and 
    $r_{1:n}(\vec{w}), r_{2:n}(\vec{w}),\ldots, r_{h:n}(\vec{w})$ are $h$ smallest residuals.
\end{definition}

Even though that estimation using LTS seems similar to the estimation using OLS the computation is much more complex because the smallest residuals are dependent on $\vec{w}$. We'll show that this fact makes from finding LTS estimate non-convex optimization problem and finding global minimum is NP-hard. 