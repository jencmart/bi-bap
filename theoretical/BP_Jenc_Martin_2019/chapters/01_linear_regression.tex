% \listoftodos


% \usepackage[style=numeric]{biblatex}
% \addbibresource{BP_ja_bibliography.bib}
% ...
% \printbibliography
% Hide or report this
\chapter{Least trimmed squares}
In this chapter, we introduce one of the most common regression analysis models which is known as the linear regression model. It aims to model the relationship between one variable which is called \defterm{dependent} and one or more variables which are called \defterm{explanatory}. The relationship is based on a model function with parameters which are not known in advance and are to be estimated from data. We will also describe one of the most common methods for finding those parameters in this model, namely the ordinary least squares method. It is important to note that it is usual to consider vectors as column vectors. On the other hand, we denote a row vector as a transposed vector.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%      SECTION: LINEAR REGRESSION MODEL     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Linear regression model}
\begin{definition}\label{definition:lr_model}
    The \defterm{linear regression model} is 
\begin{equation}
        y = \vec{x}^T\vec{w} + \varepsilon
\end{equation}
where $y \in \mathbb{R}$ is random variable called \defterm{dependent variable}, vector $\vec{x}^T = (x_1, x_2, \ldots, x_p)$ is column vector of \defterm{explanatory variables}. Usually we call $x_i$ a regressor. Finally $\varepsilon \in \mathbb{R}$ is a random variable called \defterm{noise} or \defterm{error}. Vector $\vec{w} = (w_1, w_2, \ldots, w_p)$ is vector of parameters called  \defterm{regression coefficients}. 
\end{definition}
In practice, we usually deal with multiple dependent variables so we define multiple linear regression model.

\begin{definition}\label{definition:lr_model_multiple}
The \defterm{multiple linear regression model} is 
\begin{equation}
        y_i = \vec{x_i}^T\vec{w} + \varepsilon_i,\\ i=1,\ldots , n.
\end{equation}
It is common to write the whole model in a matrix form

\begin{equation}
    \vec{y} = \vec{X}\vec{w} + \vec{\varepsilon}    \numberthis
\end{equation} where

\[ 
\vec{y} = \begin{bmatrix}
    y_{1} \\
    y_{2} \\
    \vdots \\
    y_{n}
  \end{bmatrix}\\,
 \vec{X} = \begin{bmatrix}
    \vec{x_1}^T \\
    \vec{x_2}^T \\
    \vdots \\
    \vec{x_n}^T
\end{bmatrix}
=
\begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1p} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2p} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{n1} & x_{n2} & x_{n3} & \dots  & x_{np}
\end{bmatrix}\\,
\vec{w} = \begin{bmatrix}
    w_{1} \\
    w_{2} \\
    \vdots \\
    w_{p}
  \end{bmatrix}\\,
  \vec{\varepsilon} = \begin{bmatrix}
    \varepsilon_{1} \\
    \varepsilon_{2} \\
    \vdots \\
    \varepsilon_{n}
  \end{bmatrix}
\]
That means that we can think of rows of matrix $\vec{X}$ as columns vectors $x_i$ written into the row. This means that even when we think of all vectors as columns given the matrix $\vec{X}$ we consider $x_i$ as row vectors. 

Usually we assume that errors are independent and identically distributed so that $\vec{\varepsilon} \sim \mathcal{N}(0,\,\sigma^{2})$.
\end{definition}
It this work we will talk about the multiple linear regression model, but for simplicity, we will call him merely the linear regression model.

%%%% OK TO HERE
\subsection{Prediction with the linear regression model}
The linear regression model contains the vector of actual regression coefficients which are unknown and which we need to estimate in order to be able to use the model for predictions. Let us assume that we already have estimated regression coefficients as $\vec{\hat{w}}$. Then the predicted values of $y$ are given by
\begin{equation}
    \hat{y} = \vec{\hat{w}}^T\vec{x}. \numberthis
\end{equation}
True value of $y$ is given by 
\begin{equation}
    y = \vec{w^{*}}^T\vec{x} + \varepsilon
\end{equation}
where $ \vec{w^{*}}$  represents actual regression coefficients which we estimate. 

Because we assume linear dependence between dependent variable $y$ and explanatory variables $\vec{x}$ then what makes $y$ random variable is a random variable $\varepsilon$. Because we assume that $\mathbb{E}(\varepsilon) = 0$ we can see that 

\begin{equation} \label{equation:vary}
\mathbb{E}(y) =\mathbb{E}(\vec{x}^T\vec{w}) + \mathbb{E}(\varepsilon) = \mathbb{E}(\vec{x}^T\vec{w})
\end{equation}

so $\hat{y}$ is a point estimation of the expected value of $y$.

\subsubsection{Intercept}
In real world situations it is not usual that $\mathbb{E}(\varepsilon) = 0$. Consider this trivial example.
\begin{example} \label{example:intercept}
Let us consider that $y$ represents price of the room and $x \in \mathbb{N}$ represents the number of the windows in such a room. If this room does not have windows thus $x = 0$ and $\mathbb{E}(\varepsilon) = 0$ then $ y = \vec{w}^T\vec{x} + \varepsilon$ equals zero. But it is very unlikely that room without windows is free. 
\end{example}

Because of that, it is very common to include one constant regressor $x_0 = 1$ then the corresponding coefficient $w_0$ of $\vec{w}$ is called \defterm{intercept}. We refer this model as a \defterm{model with an intercept}. Intercept then corresponds to expected value of $y$ when all regressors are zero and prevent the problem from example \ref{example:intercept}. Given that $y$ is a random variable due to the $\varepsilon$, intercept actually corresponds to $\mathbb{E}(\varepsilon) = \mu$. With regards to this fact we can still assume that in the model with intercept $\vec{\varepsilon} \sim \mathcal{N}(0,\,\sigma^{2})$.
In this work, we consider the model with the intercept unless otherwise specified. This means that we consider $\vec{x} = (x_1, x_2 \ldots x_p)$ where constant $x_1 = 1$ represents intercept.  
\begin{note}
Sometimes in model with intercept, the explanatory variable $\vec{x}$  is marked as $\vec{x} \in \mathbb{R}^{p+1}, \vec{x} = (x_0, x_1 \ldots x_p)$ 
which means that actual observation $\vec{x} \in \mathbb{R}^p$ and the intercept $x_0 = 1$ is explicitly marked.
\end{note} 

Some other assumptions about  $\vec{\varepsilon}$ are in practice invalid, and we describe this in \ref{section:roboust}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%      SECTION: ORDINARY LEAST SQUARES    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Ordinary least squares}
We want to estimate $\vec{w}$ so that the error of the model will the least possible. Measurement of this error done by a \defterm{loss function} 
$L : \mathbb{R}^2 \rightarrow  \mathbb{R}$,
which in case of ordinary least squares (OLS) is quadratic loss function $L(y, \hat{y}) := (y - \hat{y})^2$. 
So the basic idea is to find $\vec{\hat{w}}$ so that it minimizes the sum of squared \defterm{residuals}
\begin{equation}
	r_i(\vec{w}) =y_i - \hat{y_i} = y_i - \vec{x_i}^T\vec{w} ,\ i = 1,2,\ldots ,n.
\end{equation}
This is commonly know as residual sum of squares $RSS$
\begin{equation}
    RSS(\vec{w}) = \sum\limits_{i=1}^n r_i^2(\vec{w}) = (y_i - \vec{w}^T\vec{x_i})^2.
\end{equation}
\begin{definition} The RSS as the function of $\vec{w}$ is an \defterm{objective function} for OLS denoted as
    \begin{equation}
        \of^{(OLS,\vec{X}, \vec{y})}(\vec{w}) = \sum\limits_{i=1}^n (y_i - \vec{w}^T\vec{x_i})^2 = (\vec{Y} - \vec{X}\vec{w})^T(\vec{Y} - \vec{X}\vec{w})
    \end{equation}
\end{definition}
The point of the minimum of this function 
\begin{equation}
\vec{\hat{w}^{(OLS, n)}}  = \argmin_{\vec{w} \in \mathbb{R}^p} \of^{(OLS,\vec{X}, \vec{y})}(\vec{w})
\end{equation}
is a definition of ordinary least squares estimate of true regression coefficients $\vec{w}^*$.

To find the minimum of this function, first, we need to find the gradient by calculating all partial derivatives
\begin{equation}
    \frac{\partial {\of^{(OLS,\vec{X}, \vec{y})}} }{\partial w_j} = \sum\limits_{i=1}^n 2(y_i - \vec{w}^T\vec{x_i})(-x_{ij}) \\, \ \ j \in \{{1,2,\ldots,p\}}. 
\end{equation}
By this we obtain the gradient 
\begin{equation}
    \nabla \of^{(OLS,\vec{X}, \vec{y})} = -\sum\limits_{i=1}^n 2(y_i - \vec{w}^T\vec{x_i})\vec{x_i}. 
\end{equation}
Putting gradient equal to zero we get the so called \defterm{normal equation}
\begin{equation}
    -\sum\limits_{i=1}^n 2(y_i - \vec{w}^T\vec{x_i})\vec{x_i} = 0 
\end{equation}
that can be rewritten in matrix form as 
\begin{equation}    \label{equation:zerogradient}
    \vec{X}^T\vec{y} - \vec{X}^T\vec{X}\vec{w} = 0.
\end{equation}

Let's now construct the hessian matrix using second-order partial derivatives:
\begin{equation}
    \frac{\partial^2{\of^{(OLS,\vec{X}, \vec{y})}} }{\partial w_h\partial w_j} = \sum\limits_{i=1}^n 2(- x_{ik})(-x_{ij}) \\, \ \ h \in \{{1,2,\ldots,p\}}. 
\end{equation}
We get 
\begin{equation}
    \vec{H_{\of^{(OLS,\vec{X}, \vec{y})}}} = 2\vec{X}^T\vec{X}.
\end{equation}

We can see that hessian $\vec{H_{\of^{(OLS,\vec{X}, \vec{y})}}}$ is always positive semi-definite because for all $\vec{s} \in \mathbb{R}^p$

\begin{equation}
    \vec{s}^T(2\vec{X}^T\vec{X})\vec{s} = 2(\vec{X}\vec{s})^T(\vec{X}\vec{s}) =  2 \norm{\vec{X}\vec{s}}
\end{equation}

\todo{proof it is a local minimum}
It's easy to prove that twice differentiable function is convex if and only if the hessian of such function is positive semi-definite. Moreover any local minimum of the convex function of also the global one. Give that the solution of \ref{equation:zerogradient} gives us the global minimum. 

Assuming that $\vec{X}^T\vec{X}$ is a regular matrix, then its inverse exists, and solution can be explicitly written as

\begin{equation} \label{wols}
    \vec{\hat{w}}^{(OLS,n)} = (\vec{X}^T\vec{X})^{-1}\vec{X}^T\vec{y}, 
\end{equation}

where $(OLS,n)$ denotes that $\vec{\hat{w}}$ is estimate of true regression coefficients $\vec{w}^*$  given by the OLS method for $n$ observations. 

Moreover, we can see that if $\vec{X}^T\vec{X}$ is a regular matrix, then the hessian $\vec{H_{\of^{(OLS,\vec{X}, \vec{y})}}}$ is positive definite so the $\of^{(OLS,\vec{X}, \vec{y})}$ is strictly convex thus $\vec{\hat{w}}^{(OLS,n)}$ is then strict global minimum.

% norms can be used to create distance as d(x,y) = || x - y ||
% but not all distances have corresponding norm! trivial example. d(x,x) = 0, d(x,y) = 1

\subsection{Properties of OLS estimation}
Gauss-Markov theorem tells us that under specific conditions (assumptions about the regression model) is OLS estimation unbiased and efficient. In other words, Gauss-Markov theorem states that OLS is the best linear unbiased estimator (BLUE). Efficient means that any other linear unbiased estimator has the same or higher variance than OLS.  Those conditions are:

\begin{itemize} \label{ols:assumptions}
  \item Expected value of errors $\mathbb{E}(\varepsilon_i) = 0, \ i = 1,2, \ldots , n$.
  \item Errors are independently distributed and uncorrelated thus \\ $cov(\varepsilon_i, varepsilon_j), \ i, j = 1, 2 \ldots , n, \ \ i \neq j$
  \item Regressors $\vec{x_i}, i = 1, 2, \ldots , n$ and corresponding errors 
$\varepsilon_i, \ i = 1,2, \ldots , n$ are uncorrelated.
 \item All errors have same finite variance. This is known as \defterm{homoscedasticity}.
\end{itemize}

\begin{note}
 As described in \ref{example:intercept} the first assumption is usually fulfilled if we use the model with intercept. Rest of the conditions, on the other hand, are rarely met.
\end{note}

There are also other theorems which describe properties of OLS under specific conditions, but they are out if the scope of this work.

\subsection{Computing OLS}
In this section, we describe a few of many methods that can be used to obtain  $\vec{\hat{w}}^{(OLS,n)}$. 

\subsubsection*{Matrix inversion}
We've shown that $\vec{\hat{w}}^{(OLS,n)}$ can be computed directly by multiplying $\vec{A} = \vec{X}^T\vec{X}$ and $\vec{B} = \vec{X}\vec{y}$ and then by finding inversion of $\vec{A}$  and finally multiplying $\vec{A}^{-1}\vec{B}$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%      SECTION: ROBUST STATISTICS      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Robust statistics} \label{section:roboust}
Standard statistic methods expect some assumptions to work properly and fail if those assumptions are not met. A robust statistic is statistics that produce acceptable results even when data are from some unconventional distributions or if data contains errors which are not normally distributed. We should be highly motivated to use such methods because in practice we are often forced to work with such data on which classical statistics methods provide poor results.

Such assumptions about OLS method are described in \ref{ols:assumptions}. Before we explain what happens if those conditions are not met or met only partially let us describe one of the most common reasons why assumptions are false.

\subsection{Outliers}
We stated a lot of assumptions that are required for ordinary least squares to give a good estimate of $\vec{\hat{w}}$. Unfortunately, in real conditions, these assumptions are often false so that ordinary least squares do not produce acceptable results. One of the most common reasons for false assumptions are abnormal observations called outliers.

Outliers are very common, and they are for instance erroneous measurements such as transmission errors or noise. Another common reason is that nowadays we are mostly given data which were automatically processed by computers. Sometimes we are also presented data which are heterogeneous in the sense that they contain data from multiple regression models. In some sense outliers are inevitable. One would say that we should be able to eliminate them by precise examination, repair or removal of such data. That is possible in some cases, but most of the data we are dealing with are too big to check, and even more often we do not know how such should look. 

Moreover, in higher dimensions, it is complicated to find outliers. Some methods try to find outliers, but such methods are only partially efficient. Let us note that robust models are sometimes not only useful to create models that are not being unduly affected by the presence of outliers but also capable of identifying data which seems to be outliers. 

We have some terminology to describe certain types of outliers. We use terminology from \cite{rouss:1990}. Let us have observation $(y_i, \vec{x_i})$. If observation is not outlying in any direction we call it \defterm{regular observation} . If it is outlying in $\vec{x_i}$ direction we call it \defterm{leverage point}. We have two types of leverage points. If $\vec{x_i}$ is outlying but $(y_i, \vec{x_i})$ follows liner pattern we call it  \defterm{good leverage point}. If it does not follow such a pattern we call it \defterm{bad leverage point}. Finally if $(y_i, \vec{x_i})$ is  outlying only in $y_i$  direction, we call it a \defterm{vertical outliers}.

\subsection{Measuring robustness}
There are a couple of tools to measure the robustness of statistics. The most popular one is called \defterm{breakdown point}. Then there are \defterm{empirical influence function} and \defterm{influence function and sensitivity curve}. For the sake of simplicity, we describe only breakdown point right now. 

\begin{definition}
    Let $T$ be a statistics, $\vec{x} = (x_1, x_2,\ldots,x_n)$ be an n-dimensional random sample and $T_n(\vec{x})$ value of this statistics with parameter $\vec{x}$. The breakdown point of $T$ at sample $\vec{x}$ can be defined using sample $\vec{\overline{x}}^{(k)}$ where we exchange $k$ points from original sample $\vec{x}$ with random values $x_i$. We get $T_n(\vec{\overline{x}}^{(k)})$. Then the \defterm{breakdown point} is 

\begin{equation}
    \bdpoint(T,\vec{x_n} ) = \frac{1}{n} \min S_{T, \vec{x_n}} , 
\end{equation}
where 
\begin{equation}
   S_{T, \vec{x_n}, D} = 
  \left\{ k \in \{{1,2, \ldots ,n\}} : \sup_{ \vec{x_{new}}^{(k)} } \norm{T_n(\vec{x}) - T_n(\vec{x_{new}}^{(k)})} = \infty   \right\}.  
\end{equation}
\end{definition}

This definition is very general but let us specify it to our linear regression problem. It says that breakdown point is the function of the minimal number of observations needed to be changed for some others so then the estimator will give incorrect results. More robust estimators have higher breakdown point. 

It is intuitive that reasonable breakdown point should not be higher than $0.5$ \cite{rouss:1986} because if more than $50\%$ of the data is exchanged, then the model of exchanged data overrides the model of the original data. 

In the context of OLS estimator one outlier is enough to increase the value of $T$ to any desired value \cite{rousseeuw1984least} thus 
\begin{equation}
    \bdpoint(OLS,\vec{x_n} ) = \frac{1}{n}.
\end{equation}
Figure [] gives us an example of how one outlier may change the hyperplane given by the OLS estimation of regression coefficients.

\missingfigure{change of regression hyperplane based on one exchanged observation.}

For an increasing number of data samples $n$ this tends to zero. We can see that ordinary least squares estimator is not resistant to outliers at all. Due to this fact, multiple estimators similar to OLS have been proposed.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%      SECTION: LEAST TRIMMED SQUARES       %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Least trimmed squares}
The least trimmed squares (LTS) estimator is a more robust version of the OLS. In this section, we define LTS estimator and show that its breakdown point is variable and can go up to the maximum possible value of breakdown point, thus~$0.5$.

\begin{definition} \label{definition:of:lts:real}
Let us have $\vec{X} \in \mathbb{R}^{n,p}$, $\vec{y} \in \mathbb{R}^{n,1}$, 
    $\vec{w} \in \mathbb{R}^p$ and $h$, $ n/2 \leq h \leq n$. The objective function of LTS is then denoted as 
    \begin{equation}  
        \of^{(LTS,\vec{X}, \vec{y})}(\vec{w}) =  \sum\limits_{i=1}^h r_{i:n}^2(\vec{w})  
    \end{equation}
\end{definition}
where $r_{i:n}^2$ denotes $i$th smallest squared residuum out of $n$ thus $r_{1:n}^2 \leq r_{2:n}^2 \ldots r_{n:n}^2$
Even though that objective function of LTS seems similar to the OLS objective function, finding minimum is far more complex because the order of the least squared residuals is dependent on $\vec{w}$. Moreover  $r_{i:n}^2(\vec{w})$ residuum is not uniquely determined if more squared residuals have same value as $r_{i:n}^2(\vec{w})$. This makes from finding LTS estimate non-convex optimization problem and finding global minimum is NP-hard \cite{bernholt2006robust}. 

\subsection{Discrete objective function}
The LTS objective function \ref{definition:of:lts:real} is non-differentiable and non-convex, so we are unable to use the same approach as with OLS objective function. Let us transform this objective function to a discrete version which can appropriately be used by algorithms to minimize it.

Let's assume for now that we know the $\vec{\hat{w}^{(LTS, h, n)}}$ vector of estimated regression coefficients. $(LTS, h, n)$ denotes that coefficients are estimated using LTS on the $h$ subset of $n$ data samples. 
With this in mind, let $\pi$ be the permutation of $\hat{n} = \{{1,2,\ldots, n\}}$ such that 

\begin{equation}
    r_{i:n} = r_{\pi(j)}\\, \ \ j \in \hat{n}.
\end{equation}
Put

 \begin{equation}
   Q^{(n,h)} = \{{\vec{m} \in \mathbb{R}^n \\, m_i \in \{{0, 1\}} \\,i \in \hat{n} \\,   \sum\limits_{i=1}^n  m_i = h \}},
\end{equation}
which is simply the set of all vectors $\vec{m} \in \mathbb{R}^n$ which contain $h$ ones and $n-h$ zeros. Let $\vec{m}_{LTS} \in Q^{(n,h)}$  such that  $m^{(LTS)}_j = 1$ when $\pi(j) \leq h$ and $m^{(LTS)}_j = 0$ otherwise. Then

\begin{equation} \label{ltshat}
    \vec{\hat{w}^{(LTS, h, n)}} =  
  \argmin_{ \sum\limits_{i=1}^h r_{i:n}^2(\vec{w}) } = 
  \argmin_{ \sum\limits_{i=1}^n m^{(LTS)}_i r_{i}^2(\vec{w}) }. 
\end{equation}
This means that if we know the vector $\vec{m}_{LTS}$ than we can compute the LTS estimate as the OLS estimate with $\vec{X}$ and $\vec{Y}$ multiplied by the diagonal matrix $\vec{M}_{LTS} = diag(\vec{m}^T_{LTS})$:

\begin{equation}  \label{lts:discrete:objective}
    \vec{\hat{w}^{(LTS, h, n)}} = (\vec{X}^T\vec{M}_{LTS}\vec{X})^{-1}\vec{X}^T\vec{M}_{LTS}\vec{y}.
\end{equation}
In other words, finding the minimum of the LTS objective function can be done by finding the OLS estimate \ref{lts:discrete:objective} for all vectors 
$\vec{m} \in Q^{(n,h)}$. 
Thus if we denote $\vec{X_{M}} = \vec{M}\vec{X} $ and $\vec{y_{M}} = \vec{M}\vec{y}$ which corresponds to $h$ subsets  of $\vec{X}$ and $\vec{Y}$, then


% \of^{(LTS,\vec{X_{M}}, \vec{y_{M}} }(\vec{m})
\todo{cite klouda}

% @mastersthesis{kloudaVyzkumnyUkol,
%   type={Bachelor's Thesis},
%   title={Studium senzitivity odhadu metodou nejmensich usekanych cvtercu
%   author={Klouda, Karel},
%   school       = {Technical University of Prague, Faculty of Nuclear Sciences and Physical Engineering, Department of Mathematics},
%   year         = 2006,
%   address      = {Brehova 78/7, 115 19 Stare Mesto},
%   month        = 9
% }


\begin{align*} 
\minim_{\vec{w} \in \mathbb{R}^p} 
	\of^{(LTS,\vec{X}, \vec{y})}(\vec{w})  
&=\minim_{\vec{w} \in \mathbb{R}^p} 
	\sum\limits_{i=1}^h r_{i:n}^2(\vec{w})  \\
&= \minim_{\vec{w} \in \mathbb{R}^p,\vec{m} \in Q^{(n,h)}} 
		\sum\limits_{i=1}^n m_i r_{i}^2(\vec{w})\\
&= \minim_{\vec{m} \in Q^{(n,h)}} 
    		\Big( \minim_{\vec{w} \in\mathbb{R}^p} 
			\of^{(OLS,\vec{M}\vec{X},  \vec{M}\vec{y} )} (\vec{w}) \Big)\\
&= \minim_{\vec{m} \in Q^{(n,h)}} 
    		\Big(\minim_{\vec{w} \in\mathbb{R}^p}  
			\norm{ \vec{M}\vec{y} -   \vec{M}\vec{X}\vec{w}  }^2 \Big).
\end{align*}

Substituting $\vec{w}$ with \ref{ltshat} we get
%% TODO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{align*}
\vec{\hat{w}^{(LTS, h, n)}}
&=  \minim_{\vec{m} \in Q^{(n,h)}} 
    \Big( \norm{ \vec{M}\vec{y} -  \vec{M}\vec{X}(\vec{X}^T\vec{M}\vec{X})^{-1}\vec{X}^T\vec{M}\vec{y}  }^2 \Big)\\
&= \minim_{\vec{m} \in Q^{(n,h)}} 
    \Big( \norm{ \vec{M}\vec{y} -  \vec{M}\vec{X}(\vec{X}^T\vec{M}\vec{X})^{-1}\vec{X}^T\vec{M}\vec{y}  }^2 \Big)
\end{align*}

We can easily see, that have the objective function with argument $\vec{m} \in Q^{(n,h)}$. This objective function we'll mark as

\begin{equation} \label{oflts_discrete}
    \oflts(\vec{m}) =  \norm{ \vec{M}\vec{y} -  \vec{M}\vec{X}(\vec{X}^T\vec{M}\vec{X})^{-1}\vec{X}^T\vec{M}\vec{y}  }^2.
\end{equation}

We can also clearly see that the minimizing this OF could by done straightforwardly by iterating over the $Q^{(n,h)}$ set. That means we've transformed the objective function to discrete space of $Q^{(n,h)}$.

Unfortunately, this set is vast, namely $\binom{n}{h}$, so this approach is infeasible on more massive data sets. Multiple algorithms were proposed to overcome this problem. Majority of them are probabilistic algorithms, but besides those, some exact algorithms were proposed. 

Finally, let us point out some fact about the number $h$ of non-trimmed residuals and how it makes least trimmed squares robust.
The LTS reaches maximum breakdown point $0.5$ at $h = [(n/2] + [(p+1)/2]$ (where $[.]$ denotes largest integer function). 
\todo{citace}
That means that up to $50\%$ of the data can be outliers. In practice, the portion of outliers is usually lower; if an upper bound on the percentage of outliers is known,  $h$ should be set to match this percentage.
