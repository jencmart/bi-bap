\chapter{Linear Regression}
In this chapter will be introduced one of the most common regression analysis models which is linear regression model as well as ordinary least squares method for estimating is variables. This model tries to model relationship between one variable which is considered dependent and one or more variables which are considered to be explanatory. Relationship is based on model function with parameters which are not known in advance and are estimated from data. We will also introduce one of the most commons methods of finding those parameters of this model, namely ordinary least squares.
\section{Linear regression model}
\begin{definition}\label{definition:lr_model}
    Linear regression model is 
    \[ 
        y_i = \vec{x_i}^T\vec{w} + \varepsilon_i\\, \ \  i = 1,2,\ldots,n \numberthis
    \]
where $y_i \in \mathbb{R}$ is random variable which we denote as \textbf{\textit{dependent variable}}, vector $\vec{x_i}^T = (x_1, x_2, \ldots, x_p)$ is column vector of \textbf{\textit{explanatory variables}} and $\varepsilon \in \mathbb{R}$ random variable called \textbf{\textit{noise}} or \textbf{\textit{error}}. Vector $\vec{w} = (w_1, w_2, \ldots, w_p)$ is vector of parameters called  \textbf{\textit{regression coefficients}}. It is common to write whole model in matrix form. 

\[ 
    \vec{y} = \vec{X}\vec{w} + \vec{\varepsilon}    \numberthis
\] where

\[ 
\vec{y} = \begin{bmatrix}
    y_{1} \\
    y_{2} \\
    \vdots \\
    y_{n}
  \end{bmatrix}\\,
 \vec{X} = \begin{bmatrix}
    \vec{x_1}^T \\
    \vec{x_2}^T \\
    \vdots \\
    \vec{x_n}^T
\end{bmatrix}
=
\begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1p} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2p} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{n1} & x_{n2} & x_{n3} & \dots  & x_{np}
\end{bmatrix}\\,
\vec{w} = \begin{bmatrix}
    w_{1} \\
    w_{2} \\
    \vdots \\
    w_{p}
  \end{bmatrix}\\,
  \vec{\varepsilon} = \begin{bmatrix}
    \varepsilon_{1} \\
    \varepsilon_{2} \\
    \vdots \\
    \varepsilon_{n}
  \end{bmatrix}
\]
We consider $\vec{x_{i1}} = 1$ to be a constant. Corresponding $w_{1}$ we call \textbf{\textit{intercept}}. We refer this model as \textbf{\textit{model with intercept}}. It is possible to have model without intercept but then hyperplane generated by this model goes through origin. That is usually rare case so we'll always use intercept unless we'll explicitly mention it. Due to this we can assume that in model with intercept expected value  $\mathbb{E}(\varepsilon_i) = 0$.
\end{definition}


\subsection{Prediction with linear regression model}
Linear regression model contains vector of real regression coefficients which we don't know and which we need to estimate in order to be able to use model for prediction. So let us assume that we already have estimated regression coefficients which we'll mark $\vec{\hat{w}}$. Then we're able to predict value of $y$ by
\[
    \hat{y} = \vec{\hat{w}}^T\vec{x} \numberthis
\]
$\hat{y}$ denotes that it is predicted value. Real value of $y$ is given by 

\[
    y = \vec{w}^T\vec{x} + \varepsilon \numberthis
\]

Because we assume linear dependence between dependent variable $y$ and explanatory variables $\vec{x}$ then what makes $y$ random variable is actually random variable $\varepsilon$. Because we use model with intercept thus with $\mathbb{E}(e) = 0$ we can see that 
\[
\mathbb{E}(y) =\mathbb{E}(\vec{x}^T\vec{w}) + \mathbb{E}(\varepsilon) = \mathbb{E}(\vec{x}^T\vec{w})
\numberthis
\]
so $\hat{y}$ is actually a point estimation of expected value of $y$.


\section{Ordinary least squares}
We want to estimate $\vec{w}$ so that error of the model will be minimal. Measurement of this error is most often done by \textbf{\textit{loss function}} 
\[ 
L : \mathbb{R}^2 \rightarrow  \mathbb{R} \numberthis
\]
Which in case of ordinary least squares is quadratic loss function $L(y, \hat{y}) := (y - \hat{y})^2$. We refer to $r(\vec{\hat{w}}) = y - \hat{y} = y - \vec{\hat{w}}^T\vec{x} $ as to residual. 

So the idea of this method lies in fact that we want to minimize error given by sum of squared residuals commonly known as residual sum of squares $RSS$
\[
    RSS = \sum\limits_{i=1}^n r_i^2 = (y_i - \vec{w}^T\vec{x_i})^2 \numberthis
\]

So the $\vec{\hat{w}}$ we can give by

\[
    \vec{\hat{w}} =  \argmin{a}{b} \sum\limits_{i=1}^n r_i^2 = (y_i - \vec{w}^T\vec{x_i})^2 \numberthis
\]


dsfdsafdsafdsafdsafadfdsa

\subsection{Problems}
Main problem with OLS is that we assume all measurements
\section{Downfalls}


\begin{note}
    When discussing following algorithms, we'll refer to given $\vec{X}$ and $y$ as to \textbf{\textit{data set}} and to $y_i$ with corresponding $\vec{x_i}$ as to \textbf{\textit{data sample}} or \textbf{\textit{observation}}. Sometimes it's also useful to refer to multiple observations as to subset of observations. When we want to mark subset of observations  
    $y_i\\, \vec{x_i}\\, i \in H\,, H \subset \{{1,2,\ldots , n\}}$ we can simply refer to it as to subset of observations $H$. Sometimes it's also useful to mark matrix $\vec{X}$ with only some subset of observations which we'll do by $\vec{X_H}$.  
\end{note}
