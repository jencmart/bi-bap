{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cppimport\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/home/jencmart/fit/bi-bap/implementace/scprits/lts-eigen/src/somecode.cpp'\n",
      "Checksummed file not found while checking cppimport checksum. Rebuilding.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "error: command 'gcc' failed with exit status 1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m error: command 'gcc' failed with exit status 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "eigen_lts = cppimport.imp(\"somecode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastLtsEigenRegressor:\n",
    "    def __init__(self):\n",
    "        # public\n",
    "        self.n_iter_ = None\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.h_subset_ = None\n",
    "        self.rss_ = None\n",
    "        self.time1_ = None\n",
    "        self.time2_ = None\n",
    "        self.time3_ = None\n",
    "        self.time_total_ = None\n",
    "\n",
    "    # currently support for np.ndarray and matrix\n",
    "    def _validate(self, X, y, h_size, num_start_c_steps, num_starts_to_finish, max_c_steps, threshold,\n",
    "                  use_intercept):\n",
    "        if X is None or not isinstance(X, (np.ndarray, np.matrix)):\n",
    "            raise Exception('X must be  type array or np.ndarray or np.matrix')\n",
    "        if y is None or not isinstance(y, (np.ndarray, np.matrix)):\n",
    "            raise Exception('y must be  type array or np.ndarray or np.matrix')\n",
    "\n",
    "        if X.ndim == 1:\n",
    "            X = np.reshape(X, [X.shape[0], 1])\n",
    "        if y.ndim == 1:\n",
    "            y = np.reshape(y, [y.shape[0], 1])\n",
    "\n",
    "        if type(X) is not np.ndarray:\n",
    "            X = np.ndarray(X)\n",
    "        if type(y) is not np.ndarray:\n",
    "            y = np.ndarray(y)\n",
    "\n",
    "        if y.ndim != 1:\n",
    "            if y.ndim != 2 or y.shape[1] != 1:\n",
    "                raise ValueError('y must be 1D array')\n",
    "        if y.shape[0] != X.shape[0]:\n",
    "            raise ValueError('X and y must have same number of samples')\n",
    "\n",
    "        if X.shape[0] < 1:  # expects N >= 1\n",
    "            raise ValueError('You must provide at least one sample')\n",
    "\n",
    "        if X.ndim < 1:  # expects p >=1\n",
    "            raise ValueError('X has zero dimensions')\n",
    "\n",
    "        if h_size != 'default':\n",
    "            if h_size > X.shape[0]:\n",
    "                raise ValueError('H_size must not be > number of samples')\n",
    "            if h_size < 1:\n",
    "                raise ValueError('H_size must be > 0 ; preferably (n + p + 1) / 2 <= h_size <= n ')\n",
    "\n",
    "        if max_c_steps < 1:  # expects max_steps => 1\n",
    "            raise ValueError('max_c_steps must be >= 1')\n",
    "\n",
    "        if num_start_c_steps < 1:  # expects num_start_steps => 1\n",
    "            raise ValueError('num_start_c_steps must be >= 1')\n",
    "\n",
    "        if num_starts_to_finish < 1:  # expects num_starts_to_finish >= 1\n",
    "            raise ValueError('num_starts_to_finish must be >= 1')\n",
    "\n",
    "        if threshold < 0:  # expects threshold >= 0\n",
    "            raise ValueError('threshold must be >= 0')\n",
    "\n",
    "        if use_intercept:\n",
    "            X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def fit(self, X, y,\n",
    "            num_starts: 'number of initial starts (H1)' = 500,\n",
    "            num_start_c_steps: 'number of initial C steps' = 2,\n",
    "            num_starts_to_finish: 'number of H3 which`ll to finish' = 10,\n",
    "            max_c_steps: 'self explanatory' = 50,\n",
    "            h_size: 'default := (n + p + 1) / 2' = 'default',\n",
    "            use_intercept=True,\n",
    "            threshold: 'stopping criterion Qold Qnew' = 1e-6):\n",
    "\n",
    "\n",
    "        X, y = self._validate(X, y, h_size, num_start_c_steps, num_starts_to_finish, max_c_steps, threshold,\n",
    "                              use_intercept)\n",
    "\n",
    "        # todo - include intercept or not? now - p include intercept..\n",
    "        _h_size = math.ceil((X.shape[0] + X.shape[1] +1) / 2) if h_size == 'default' else h_size  # N + p + 1\n",
    "\n",
    "        eigen_result = eigen_lts.fast_lts(X, y, num_starts, num_start_c_steps, num_starts_to_finish, _h_size,\n",
    "                                          max_c_steps, threshold)\n",
    "\n",
    "        # ... Store best result\n",
    "        weights = eigen_result.get_theta()\n",
    "        if use_intercept:\n",
    "            self.intercept_ = weights[-1, 0]  # last row first col\n",
    "            self.coef_ = np.ravel(weights[:-1, 0])  # for all but last column,  only first col\n",
    "        else:\n",
    "            self.intercept_ = 0.0\n",
    "            self.coef_ = np.ravel(weights[:, 0])  # all rows, only first col\n",
    "\n",
    "        self.h_subset_ = eigen_result.get_h_subset()\n",
    "        self.rss_ = eigen_result.get_rss()\n",
    "        self.n_iter_ = eigen_result.get_n_inter()\n",
    "        self.time1_ = eigen_result.get_time_1()\n",
    "        self.time2_ = eigen_result.get_time_2()\n",
    "        self.time3_ = eigen_result.get_time_3()\n",
    "        self.time_total_ = self.time1_ + self.time2_ + self.time3_\n",
    "\n",
    "class FastLtsRegression:\n",
    "    def __init__(self):\n",
    "        # public\n",
    "        self.n_iter_ = None\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.h_subset_ = None\n",
    "        self.rss_ = None\n",
    "        # process time - for benchmark only\n",
    "        self.time1_ = None\n",
    "        self.time2_ = None\n",
    "        self.time3_ = None\n",
    "        self.time_total_ = None\n",
    "\n",
    "    # currently support for np.ndarray and matrix\n",
    "    def _validate(self, X, y, h_size, num_start_c_steps, num_starts_to_finish, max_c_steps, threshold, use_intercept):\n",
    "        if X is None or not isinstance(X, (np.ndarray, np.matrix)):\n",
    "            raise Exception('X must be  type array or np.ndarray or np.matrix')\n",
    "        if y is None or not isinstance(y, (np.ndarray, np.matrix)):\n",
    "            raise Exception('y must be  type array or np.ndarray or np.matrix')\n",
    "\n",
    "        if X.ndim == 1:\n",
    "            X = np.reshape(X, [X.shape[0], 1])\n",
    "        if y.ndim == 1:\n",
    "            y = np.reshape(y, [y.shape[0], 1])\n",
    "\n",
    "        if type(X) is not np.matrix:\n",
    "            X = np.asmatrix(X)\n",
    "        if type(y) is not np.matrix:\n",
    "            y = np.asmatrix(y)\n",
    "\n",
    "        if y.ndim != 1:\n",
    "            if y.ndim != 2 or y.shape[1] != 1:\n",
    "                raise ValueError('y must be 1D array')\n",
    "        if y.shape[0] != X.shape[0]:\n",
    "            raise ValueError('X and y must have same number of samples')\n",
    "\n",
    "        if X.shape[0] < 1:  # expects N >= 1\n",
    "            raise ValueError('You must provide at least one sample')\n",
    "\n",
    "        if X.ndim < 1:  # expects p >=1\n",
    "            raise ValueError('X has zero dimensions')\n",
    "\n",
    "        if h_size != 'default':\n",
    "            if h_size > X.shape[0]:\n",
    "                raise ValueError('H_size must not be > number of samples')\n",
    "            if h_size < 1:\n",
    "                raise ValueError('H_size must be > 0 ; preferably (n + p + 1) / 2 <= h_size <= n ')\n",
    "\n",
    "        if max_c_steps < 1:  # expects max_steps => 1\n",
    "            raise ValueError('max_c_steps must be >= 1')\n",
    "\n",
    "        if num_start_c_steps < 1:  # expects num_start_steps => 1\n",
    "            raise ValueError('num_start_c_steps must be >= 1')\n",
    "\n",
    "        if num_starts_to_finish < 1:  # expects num_starts_to_finish >= 1\n",
    "            raise ValueError('num_starts_to_finish must be >= 1')\n",
    "\n",
    "        if threshold < 0:  # expects threshold >= 0\n",
    "            raise ValueError('threshold must be >= 0')\n",
    "\n",
    "        if use_intercept:\n",
    "            merged = np.concatenate([y, X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        else:\n",
    "            merged = np.concatenate([y, X], axis=1)\n",
    "\n",
    "        return np.asmatrix(merged)\n",
    "\n",
    "\n",
    "    def fit(self, X, y,\n",
    "            num_starts: 'number of initial starts (H1)' = 500,\n",
    "            num_start_c_steps: 'number of initial C steps' = 2,\n",
    "            num_starts_to_finish: 'number of H3 which`ll to finish' = 10,\n",
    "            max_c_steps: 'self explanatory' = 50,\n",
    "            h_size: 'default := (n + p + 1) / 2' = 'default',\n",
    "            use_intercept=True,\n",
    "            threshold: 'stopping criterion for Qold Qnew sum residuals in c-steps' = 1e-6):\n",
    "\n",
    "        # Init some properties\n",
    "        data = self._validate(X, y, h_size, num_start_c_steps, num_starts_to_finish, max_c_steps, threshold, use_intercept)\n",
    "\n",
    "        _h_size = math.ceil((data.shape[0] + data.shape[1] ) / 2) if h_size == 'default' else h_size # N + (p-1) + 1\n",
    "\n",
    "        # IF N > 1500\n",
    "        # 1. CREATE 5 SUBSETS OF THE DATA\n",
    "        # ON EACH SUBSET CREATE SUBSET RESULTS ( NUMSTARTS / 5 )\n",
    "            # WHAT SHOULD BE THE SIZE OF H -- how many vectors to choose from ( for example n+p/2 ... 750 from 300 is not acceptable\n",
    "            # they say : hsub = [nsub(h/n)] nsub = 300 n = 1500\n",
    "            # I SUPPOSE (and it make sense)\n",
    "                # nested extension : DATA = TEN SUBSET ( 300 napriklad..) H_SIZE := subset_size * h/n ???? jo dava smysl ...lece pres 50% opet..\n",
    "\n",
    "        # on each subset carry out few c steps\n",
    "        # and from each subset select 10 best results\n",
    "\n",
    "        # merge all best results together --> 50 results\n",
    "        # carry out 2 c steps\n",
    "        # select 10 best\n",
    "        # iterate till convergence\n",
    "\n",
    "        # Selective iteration := h1 + few c-steps + find few with best rss\n",
    "        # result = eigen_lts.fast_lts(data, num_starts, num_start_c_steps, num_starts_to_finish, max_c_steps, h_size, threshold)\n",
    "        X = data[:, 1:]\n",
    "        y = data[:, :1]\n",
    "\n",
    "        time1 = time.process_time()\n",
    "        subset_results = self.create_all_h1_subsets(num_starts, _h_size, data) # array of 500 Results (h1, thetha, inf)\n",
    "        self.time1_ =  time.process_time() - time1\n",
    "\n",
    "\n",
    "        time2 = time.process_time()\n",
    "        self.iterate_c_steps(data, _h_size, subset_results, num_starts, False, num_start_c_steps, 0) # few c steps on all 500 results, all happens inplace\n",
    "        k_smallest_inplace(subset_results, num_starts_to_finish) # arr results && indexes are sorted (sort first 10 from 500...)\n",
    "        self.time2_ = time.process_time() - time2\n",
    "\n",
    "        # C-steps till convergence\n",
    "        time3 = time.process_time()\n",
    "        self.iterate_c_steps(data, _h_size, subset_results, num_starts_to_finish, True, max_c_steps, threshold)\n",
    "        # select the best one\n",
    "        best_result = subset_results[0]\n",
    "        for i in range(num_starts_to_finish):\n",
    "            best_result = subset_results[i] if subset_results[i].rss < best_result.rss else best_result\n",
    "        self.time3_ = time.process_time() - time3\n",
    "        self.time_total_ = self.time1_ + self.time2_ + self.time3_\n",
    "\n",
    "        # ... Store best result\n",
    "        if use_intercept:\n",
    "            self.intercept_ = best_result.theta[-1,0]  # last row first col\n",
    "            self.coef_ = np.ravel ( best_result.theta[:-1,0] ) # for all but last column,  only first col\n",
    "        else:\n",
    "            self.intercept_ = 0.0\n",
    "            self.coef_ = np.ravel( best_result.theta[:,0] ) # all rows, only first col\n",
    "\n",
    "        self.h_subset_ = best_result.h_subset.astype(int)\n",
    "        self.rss_ = best_result.rss\n",
    "        self.n_iter_ = best_result.n_iter\n",
    "\n",
    "\n",
    "\n",
    "    # Select initial H1\n",
    "    # ONLY ONE H1 ( one array of indexes to data)\n",
    "    def generate_h1_subset(self, _h_size, data):\n",
    "        p = data.shape[1] - 1\n",
    "        N = data.shape[0]\n",
    "\n",
    "        if p >= N:\n",
    "            J = data\n",
    "        else:\n",
    "            # create random permutation\n",
    "            idx_all = np.random.permutation(N)\n",
    "            # cut first p indexes and save the rest\n",
    "            idx_initial = idx_all[:p]\n",
    "            idx_rest = idx_all[p:]\n",
    "\n",
    "            # create initial matrix of shape (p,p)\n",
    "            J = data[idx_initial, :]\n",
    "\n",
    "            # J[:,1:] == only X, without first y column\n",
    "            rank = np.linalg.matrix_rank(J[:, 1:])\n",
    "\n",
    "            while rank < p and J.shape[0] < N:\n",
    "                # get first index from rest of the indexes\n",
    "                current = idx_rest[[0],]\n",
    "                idx_rest = idx_rest[1:, ]\n",
    "\n",
    "                # add row on this index -fixed, ok\n",
    "                J = np.append(J, data[current, :], axis=0)\n",
    "\n",
    "                # and recalculate rank\n",
    "                rank = np.linalg.matrix_rank(J[:, 1:])\n",
    "\n",
    "        # OLS on J\n",
    "        theta_zero_hat = ols(J)\n",
    "\n",
    "        # abs dist on N, and return h smallest\n",
    "        abs_residuals = abs_dist(data, theta_zero_hat)\n",
    "        indexes = k_smallest(abs_residuals, _h_size) # vraci pole indexu, mohlo by vracet o theta\n",
    "        return indexes\n",
    "\n",
    "    class Result:\n",
    "        def __init__(self, h_subset, theta, rss, n_iter):\n",
    "            self.h_subset = h_subset  # array\n",
    "            self.theta = theta # matrix\n",
    "            self.rss = rss # double\n",
    "            self.n_iter = n_iter # integer\n",
    "\n",
    "\n",
    "    def create_all_h1_subsets(self, num_starts, _h_size, data):\n",
    "        arr_results = []\n",
    "        for i in range(num_starts):\n",
    "            init_h1 = self.generate_h1_subset(_h_size, data) # one array of indexes to h1\n",
    "            arr_results.append(self.Result(init_h1, ols(data[init_h1, :]), math.inf, 0))\n",
    "        return arr_results\n",
    "\n",
    "    def iterate_c_steps(self, data, _h_size, results, length, stop_on_rss, cnt_steps, threshold):\n",
    "\n",
    "        for i in range(length): # bude brat v potaz jenom prvnich X\n",
    "            theta, h_subset, rss, n_iter = self._preform_c_steps(results[i].theta, data, stop_on_rss, results[i].rss, _h_size, cnt_steps, threshold)\n",
    "            results[i].theta = theta\n",
    "            results[i].h_subset = h_subset\n",
    "            results[i].rss = rss\n",
    "            results[i].n_iter += n_iter\n",
    "\n",
    "    def _preform_c_steps(self, theta_old, data, use_sum, sum_old, h_size, max_steps, threshold):  # vola se 10x\n",
    "\n",
    "        j = 0\n",
    "        for i in range(max_steps):\n",
    "            # c step\n",
    "            abs_residuals = abs_dist(data, theta_old)  # nested extension : DATA = TEN SUBSET ( 300 napriklad..) H_SIZE := subset_size * h/n ???? jo dava smysl ...lece pres 50% opet..\n",
    "            h_new = k_smallest(abs_residuals, h_size)  #\n",
    "            theta_new = ols(data[h_new, :])\n",
    "            # ! c step\n",
    "\n",
    "            if use_sum:\n",
    "                sum_new = rss(data[h_new, :], theta_new)\n",
    "                if math.isclose(sum_old, sum_new, rel_tol=threshold):\n",
    "                    j = i+1  # include last step\n",
    "                    break\n",
    "                sum_old = sum_new\n",
    "            theta_old = theta_new\n",
    "\n",
    "        if not use_sum:\n",
    "            sum_new = rss(data[h_new, :], theta_new)\n",
    "\n",
    "        if j == 0:\n",
    "            j =  max_steps\n",
    "\n",
    "        return theta_new, h_new, sum_new[0, 0], j\n",
    "\n",
    "\n",
    "##################\n",
    "# MAIN FUNCTIONS #\n",
    "##################\n",
    "def rss(input_data, theta):\n",
    "    y = input_data[:, [0]]\n",
    "    x = input_data[:, 1:]\n",
    "    return (y - x * theta).T * (y - x * theta)\n",
    "\n",
    "def ols(input_data):\n",
    "    # [0] .. diky tomu bude mit spravny shape\n",
    "    y = input_data[:, [0]]\n",
    "    x = input_data[:, 1:]\n",
    "    return (x.T * x).I * x.T * y  # including intercept (last)\n",
    "\n",
    "def abs_dist(data, theta):\n",
    "    # Y (p+,1)\n",
    "    # theta (p+ , 1)\n",
    "    # xx (n, p)\n",
    "    return np.absolute(data[:, [0]] - data[:, 1:] * theta)\n",
    "\n",
    "def k_smallest_inplace(results, kth):\n",
    "\n",
    "    def kth_smallest(arr_results, left, right, k):\n",
    "        # partition\n",
    "        pivot = arr_results[right].rss\n",
    "        pos = left\n",
    "        for j in range(left, right):\n",
    "            if arr_results[j].rss <= pivot:\n",
    "                arr_results[pos], arr_results[j] = arr_results[j], arr_results[pos]  # swap whole results\n",
    "                pos += 1\n",
    "\n",
    "        arr_results[pos], arr_results[right] = arr_results[right], arr_results[pos]\n",
    "\n",
    "        # finish\n",
    "        if pos - left == k - 1:\n",
    "            #return arr_results[:pos + 1], indexes[:pos + 1]  # values, indexes\n",
    "            return\n",
    "        # left part\n",
    "        elif pos - left > k - 1:\n",
    "            return kth_smallest(arr_results, left, pos - 1, k)\n",
    "            # right part\n",
    "        return kth_smallest(arr_results, pos + 1, right, k - pos + left - 1)\n",
    "\n",
    "    kth_smallest(results, 0, len(results) - 1, kth)\n",
    "    return\n",
    "\n",
    "def k_smallest(absolute_dist_in, kth_smallest):\n",
    "    absolute_dist_copy = np.copy(absolute_dist_in)\n",
    "\n",
    "    indexes = np.arange(absolute_dist_copy.shape[0])\n",
    "    absolute_dist = np.ravel(absolute_dist_copy)\n",
    "\n",
    "    def kth_smallest2(arr, idx_arr, left, right, k):\n",
    "        # partition\n",
    "        pivot = arr[right]\n",
    "        pos = left\n",
    "        for j in range(left, right):\n",
    "            if arr[j] <= pivot:\n",
    "                arr[pos], arr[j] = arr[j], arr[pos]  # swap\n",
    "                idx_arr[pos], idx_arr[j] = idx_arr[j], idx_arr[pos]  # swap indexes\n",
    "                pos += 1\n",
    "\n",
    "        arr[pos], arr[right] = arr[right], arr[pos]\n",
    "        idx_arr[pos], idx_arr[right] = idx_arr[right], idx_arr[pos]\n",
    "\n",
    "        # finish\n",
    "        if pos - left == k - 1:\n",
    "            return arr[:pos + 1], idx_arr[:pos + 1]  # values, indexes\n",
    "\n",
    "        # left part\n",
    "        elif pos - left > k - 1:\n",
    "            return kth_smallest2(arr, idx_arr, left, pos - 1, k)\n",
    "            # right part\n",
    "        return kth_smallest2(arr, idx_arr, pos + 1, right, k - pos + left - 1)\n",
    "\n",
    "    result_values, result_indexes = kth_smallest2(absolute_dist, indexes, 0, absolute_dist.shape[0] - 1, kth_smallest)\n",
    "    return result_indexes\n",
    "\n",
    "\n",
    "# Data generator\n",
    "def generate_data_2D_multi_variate(cnt, outlier_percentage=20):\n",
    "    # LINEAR DATA\n",
    "    # data generated same way as in Rousseeuw and Driessen 2000\n",
    "    N_clean = cnt - int(math.floor(cnt/100*outlier_percentage))\n",
    "    N_dirty = int(math.ceil(cnt/100*outlier_percentage))\n",
    "\n",
    "    X_original = np.random.normal(loc=0, scale=10, size=N_clean)  # var = 100\n",
    "    e = np.random.normal(loc=0, scale=1, size=N_clean)  # var = 1\n",
    "    y_original = 1 + X_original + e\n",
    "    # OUTLIERS\n",
    "    # multivariate N(mean = location, covariance)\n",
    "    # diagonalni 25 I\n",
    "    outliers = np.random.multivariate_normal(mean=[50, 0],\n",
    "                                             cov=[[25, 0], [0, 25]],\n",
    "                                             size=N_dirty)\n",
    "\n",
    "    # outliers\n",
    "    # FINAL DATA\n",
    "    X = np.concatenate((X_original, outliers.T[0]), axis=0)\n",
    "    y = np.concatenate((y_original, outliers.T[1]), axis=0)\n",
    "\n",
    "    return X,y\n",
    "\n",
    "def generate_data_ND(cnt, dim, outlier_percentage=20, n_xij= (0,10), ei = (0,1), n_xi1_outlier = (100,10) ):\n",
    "    N_clean = cnt - int(math.floor(cnt / 100 * outlier_percentage))\n",
    "    N_dirty = int(math.ceil(cnt / 100 * outlier_percentage))\n",
    "\n",
    "    # Xij\n",
    "    mu, sigma = n_xij\n",
    "    X_clean = np.random.normal(mu,sigma, (N_clean, dim))\n",
    "    mu, sigma = n_xi1_outlier\n",
    "    X_outliers = np.random.normal(mu,sigma, (N_dirty, dim))\n",
    "\n",
    "    #ei\n",
    "    mu, sigma = ei\n",
    "    e = np.random.normal(mu, sigma, (N_clean, 1))\n",
    "    e_2 = np.random.normal(mu, sigma, (N_dirty, 1))\n",
    "\n",
    "    #Y\n",
    "    y_clean = np.concatenate((X_clean, e), axis=1)\n",
    "    y_clean = np.sum(y_clean, axis=1)\n",
    "\n",
    "    y_outliers = np.concatenate((X_outliers, e_2), axis=1)\n",
    "    y_outliers = np.sum(y_outliers, axis=1)\n",
    "\n",
    "    X = np.concatenate((X_clean, X_outliers), axis=0)\n",
    "    y = np.concatenate((y_clean, y_outliers), axis=0)\n",
    "\n",
    "\n",
    "    y = np.reshape(y, [y.shape[0], 1])\n",
    "    y_clean = np.reshape(y_clean, [y_clean.shape[0], 1])\n",
    "\n",
    "    return X, y, X_clean, y_clean\n",
    "\n",
    "\n",
    "\n",
    "def getRegressor(type):\n",
    "    if type == 'ltsPython':\n",
    "        return FastLtsRegression()\n",
    "    if type == 'ltsCpp':\n",
    "        return \\\n",
    "            FastLtsEigenRegressor()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    types = ['ltsPython', 'ltsCpp' ]\n",
    "\n",
    "    # same as in FAST-LTS paper -- it is a benchamrk\n",
    "    experiments = [(100,2), (100,3), (100,5), (500,2), (500,3), (500,5), (1000,2),\n",
    "                   (1000,5), (1000,10), (10000,2), (10000,5), (10000,10), (50000,2), (50000,5)]\n",
    "\n",
    "    # experiments = [(100,2), (100,3)]\n",
    "\n",
    "    for experiment in experiments:\n",
    "        n, p = experiment\n",
    "        X, y, X_clean, y_clean = generate_data_ND(n, p)\n",
    "        for kind in types:\n",
    "            lts = getRegressor(kind)\n",
    "\n",
    "\n",
    "            # lts\n",
    "            lts.fit(X, y, use_intercept=True)\n",
    "            weights_correct = lts.coef_\n",
    "\n",
    "            # print data\n",
    "            print('{} ({},{})'.format(kind, n, p))\n",
    "            print('rss: ', lts.rss_)\n",
    "            print('itr: ', lts.n_iter_)\n",
    "            print('tim: ', lts.time_total_)\n",
    "\n",
    "            #ols\n",
    "            lts.fit(X_clean, y_clean, use_intercept=True, h_size=X_clean.shape[0])\n",
    "            weights_expected = lts.coef_\n",
    "            # print('rsO: ', lts.rss_)\n",
    "            # cos similarity\n",
    "            result = 1 - spatial.distance.cosine(weights_correct, weights_expected)\n",
    "            print('cos: ', result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
